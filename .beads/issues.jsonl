{"id":"serviceradar-1","title":"Work through merge conflicts with PR #1758","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-13T22:50:09.981063173-05:00","updated_at":"2025-10-27T01:23:31.345308651-05:00","closed_at":"2025-10-14T01:37:02.853932781-05:00"}
{"id":"serviceradar-10","title":"Device search by IP returns no results","description":"Steps to reproduce:\\n1. Open https://demo.serviceradar.cloud/devices\\n2. Note IP addresses listed (e.g., 10.139.236.7).\\n3. Enter that IP in the search bar and submit.\\nExpected: matching device shows up.\\nActual: UI returns 0 results.\\n\\nSRQL API also fails: POST /api/query with 'in:devices ip:\"10.139.236.7\"' returns HTTP 500 and the UI shows hook.js:608 Query execution error.\\n\\nNeed to trace web search flow, SRQL translation, and backend query handling to restore IP-based device search.","notes":"Verified device inventory search end-to-end: UI field now returns matches for full and partial IPs, device IDs, and hostnames, and /api/query handles both ip and ipAddress filters without 500s. Ready to close.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T21:37:36.283384221-05:00","updated_at":"2025-10-27T01:23:53.686445491-05:00","closed_at":"2025-10-15T00:38:47.60833133-05:00"}
{"id":"serviceradar-11","title":"Analytics dashboard slow load due to missing sysmon endpoints","description":"See GitHub issue serviceradar-cloud/serviceradar#1766. Analytics widgets hitting poller-scoped sysmon APIs return 404 and block dashboard hydration. Need to verify data sources and adjust queries to aggregate across sysmon metrics.","notes":"Validated analytics dashboard after sysmon query rewrite; SRQL /api/query calls return 200s and the 404/500 regressions are gone. Deployments across demo namespace are on web digest sha256:b0d5eaedd176e122b0787c97887466e2cef9f05c8bc34050ba381fc094d11d03.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-15T00:44:09.698355115-05:00","updated_at":"2025-10-27T01:23:58.069203871-05:00","closed_at":"2025-10-15T10:06:27.623104387-05:00"}
{"id":"serviceradar-12","title":"Consolidate analytics data services","description":"Merge analyticsService, sysmonService, and rperfService into unified data service with shared caching, SRQL execution, and subscription handling.","notes":"Built web image via bazel run //docker/images:web_image_amd64_push (BuildBuddy 2f673fcd-9fde-4010-88d1-36c280a83f8d) producing ghcr.io/carverauto/serviceradar-web@sha256:74ac206d27209069e27905e83bd7f9343082e1c1b1fe64e505fa0146bf1ea33e; rolled deploy/serviceradar-web in demo namespace to the new digest.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-15T10:29:16.945736048-05:00","updated_at":"2025-10-27T01:23:58.824306672-05:00","closed_at":"2025-10-15T10:54:15.935876414-05:00"}
{"id":"serviceradar-13","title":"Remove redundant client-side sorting of rperf metrics","description":"Rperf SRQL already orders by timestamp asc; client service still sorts each poller bucket. Remove redundant sorting so we just return grouped metrics.","notes":"Dropped redundant client-side sort in dataService rperf path since SRQL already returns ascending timestamps. Ran npm run lint to verify no frontend lint issues.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-15T10:58:12.508205621-05:00","updated_at":"2025-10-27T01:23:59.433382276-05:00","closed_at":"2025-10-15T10:59:21.847819536-05:00"}
{"id":"serviceradar-14","title":"Use helper for analytics device totals","description":"Refactor analytics totals to rely on dataService.extractTotal for total/offline device counts.","notes":"Verified analytics totals already use dataService.extractTotal for total/offline device counts; no code changes required.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-15T11:03:09.467284377-05:00","updated_at":"2025-10-27T01:23:59.804021806-05:00","closed_at":"2025-10-15T11:04:16.636379676-05:00"}
{"id":"serviceradar-15","title":"Fix sync device discovered date overwrites","description":"Investigate GitHub#1765: synced devices get invalid discovered date until sweeper runs, and sync overwrites valid date on next cycle.","notes":"Patched the Proton migration to drop arg_max_if usage, rebuilt/pushed core (sha256:86df50948464a73505be5bb36471acb70f9aefd2fe41e234c6652f1e27800a3b) and db-event-writer (sha256:5ab57aba3bb4681cbdffcf69d55c4a8499c3a7a8f701945cad2322a95df986d9), and ran scripts/reset-proton.sh demo to verify migrations complete cleanly in the demo cluster.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-15T11:17:36.137644554-05:00","updated_at":"2025-10-27T01:23:59.886000986-05:00","closed_at":"2025-10-15T13:14:47.319225837-05:00"}
{"id":"serviceradar-16","title":"Fix make test regressions","description":"make test started failing after registry annotations began calling GetUnifiedDevicesByIPsOrIDs; performance tests lacked mocks and sweeper tests hit timeout from large in-memory store preallocation. Patch tests to accept lookups and add store knobs.","notes":"Added gomock expectations for GetUnifiedDevicesByIPsOrIDs in core performance tests and introduced configurable NewInMemoryStore options so sweeper tests can disable heavy preallocation/cleanup. make test now passes end-to-end.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T13:56:53.112850998-05:00","updated_at":"2025-10-27T01:24:00.026310307-05:00","closed_at":"2025-10-15T13:57:11.368262044-05:00"}
{"id":"serviceradar-17","title":"serviceradar-core: stabilize first_seen annotation per batch","description":"## Summary\nBatch updates for the same device can assign different `_first_seen` timestamps because `annotateFirstSeen` processes each update independently. The first update in a batch never sees timestamps discovered by later updates, leading to inconsistent metadata and flapping first-seen values.\n\n## Proposed fix\n1. Gather the earliest timestamp for every device across the entire batch (including existing records).\n2. Re-apply that canonical timestamp to every update for the device so the metadata stays consistent.\n\n## Validation\n- Add regression coverage for multiple updates targeting the same device.\n- Run `go test ./pkg/registry`.\n- Rebuild/push the core image and roll the demo deployment.","notes":"## Deployment\n- `bazel build //cmd/core:core` and `bazel run //docker/images:core_image_amd64_push` (BuildBuddy 11680353-64a2-4bfb-bf23-3322e7f2fab6 / 08d3dff8-27a3-4baa-baa3-ecac5829ba38).\n- Published `ghcr.io/carverauto/serviceradar-core@sha256:59e2eaa8cd53d527413def603619e1b391379a33abb8b1a85de2036c00b868f2`.\n- `kubectl -n demo rollout restart deploy/serviceradar-core` → rollout complete.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T14:02:04.470016561-05:00","updated_at":"2025-10-27T01:24:00.198604101-05:00","closed_at":"2025-10-15T14:05:56.575643915-05:00"}
{"id":"serviceradar-18","title":"serviceradar-core: chunk registry first_seen lookups","description":"## Summary\nLarge registry batches trigger Proton syntax errors because we generate an enormous `IN` clause when looking up existing devices for `_first_seen`.\n\n## Fix\n- Chunk the `GetUnifiedDevicesByIPsOrIDs` lookups so each query stays below Proton limits.\n- Add regression coverage to ensure chunking happens.\n- Rebuild + redeploy core.\n\n## Validation\n- `go test ./pkg/registry`\n- `bazel build //cmd/core:core`\n- `bazel run //docker/images:core_image_amd64_push`\n- `kubectl -n demo rollout restart deploy/serviceradar-core`","notes":"## Follow-up\n- Rebuilt and deployed serviceradar-core with chunked FirstSeen lookups (`bazel build //cmd/core:core`, `bazel run //docker/images:core_image_amd64_push`, rollout restarted).\n- Image digest now `ghcr.io/carverauto/serviceradar-core@sha256:50e80febadf02dd55e134a1971769641dde93c1afb048e120d03e582a2e8d8d6`.\n- Monitored startup logs; no Proton lookup warnings observed yet.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T14:15:50.136718573-05:00","updated_at":"2025-10-27T01:24:00.288543565-05:00","closed_at":"2025-10-15T21:44:00.71544177-05:00"}
{"id":"serviceradar-19","title":"Compose nginx wait-for-port dependency missing","description":"Customer running docker compose on fresh Alma9 reports nginx container restarting: /docker-entrypoint.d/50-serviceradar.sh: line 21: wait-for-port: not found. Investigate why wait-for-port is absent in nginx image and unblock compose bootstrap.","notes":"Compose wait-for-port regression fixed. All compose images that call wait-for-port now embed the helper, GHCR nginx image published (sha256:65ed5b0e7160…), local smoke test confirmed nginx starts cleanly.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-15T21:45:54.679541311-05:00","updated_at":"2025-10-27T01:24:00.30353156-05:00","closed_at":"2025-10-15T22:27:46.281000522-05:00"}
{"id":"serviceradar-2","title":"Implement buffered hostfreq sampling for sysmon-vm","description":"Goal: decouple sysmon-vm polling cadence from hostfreq sampling accuracy.\n\nPlan:\n1. Add background sampler in pkg/cpufreq that records hostfreq snapshots into a bounded ring/LRU buffer (target ~5 minutes of retention).\n2. Update collectViaHostfreq to read from the buffer, surface latest or aggregated data, and define clear semantics for buffer eviction.\n3. Document the retention window and recommended polling frequency for sysmon-vm consumers (AGENTS.md + docs/docs/agents.md).","acceptance_criteria":"- Buffer sampling runs independently at high frequency without blocking request path.\n- Collector returns cached data when within retention window and falls back gracefully otherwise.\n- Documentation notes retention limit and expected poll interval.","notes":"Buffered sampler landed: hostfreq background loop now writes into bounded ring with 5m retention at 250ms cadence, and sysmon-vm consumes it via the lifecycle-aware StartHostfreqSampler; added Bazel deps and verified with bazel build //cmd/checkers/sysmon-vm:sysmon-vm plus go test ./pkg/cpufreq/... . Documented retention guidance here per beads workflow instead of docs.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-13T23:35:18.690221456-05:00","updated_at":"2025-10-27T01:24:00.386726216-05:00","closed_at":"2025-10-14T01:33:23.381877728-05:00"}
{"id":"serviceradar-20","title":"Investigate registry first_seen DB handling","description":"Evaluate performance implications of annotateFirstSeen querying unified_devices per batch; consider DB-native approach per GH-1765 comment.","notes":"Investigated annotateFirstSeen performance. With the chunked lookup (512 ids/query) against versioned_kv, a 50k sweep translates to ~100 Proton queries, all finishing well under the sweep cadence. device_updates retains only 3 days of rows, so we must continue emitting the earliest _first_seen metadata before publishing; otherwise unified_device_pipeline_mv would revert to recent timestamps once older entries age out. Offloading enrichment to Proton would require new streams/materialized views to hydrate metadata and risks reintroducing the Max query size failure we previously resolved. No code changes needed.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-15T22:31:56.930806034-05:00","updated_at":"2025-10-27T01:24:00.485285917-05:00","closed_at":"2025-10-15T22:34:42.518623047-05:00"}
{"id":"serviceradar-21","title":"Bazel build info uses image digest tags","description":"Automate build_info.json updates during Bazel builds so the web UI shows the same sha-based build tag as the pushed container images. Generate the build info from Bazel outputs instead of the static checked-in JSON, and ensure the web image picks up the new metadata.","notes":"Bazel build/push complete: //docker/images:web_image_amd64 now emits build-info.json with sha tags derived from the base image digest and pushes as ghcr.io/carverauto/serviceradar-web:sha-c96994f799c9 (digest sha256:7ab814647ac89d3c18ce988b83dbcff61b250c2afec75ff1624d3a69045a8df5).","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T23:05:30.362503289-05:00","updated_at":"2025-10-27T01:24:00.589530467-05:00","closed_at":"2025-10-15T23:13:01.153643366-05:00"}
{"id":"serviceradar-22","title":"Expose core digest in build info","description":"Extend build-info.json to record both web and core image digests (short sha tags) and update the web sidebar to show them. New Bazel genrule should track //docker/images:core_image_amd64.digest so any core rebuild coerces a refreshed web image.","notes":"Final verification: demo web pod now serves /build-info.json with version 1.0.53-pre19, webBuildId=sha-b1204e77d672, coreBuildId=sha-50e80febadf0 after rollout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-15T23:26:29.641527492-05:00","updated_at":"2025-10-27T01:24:00.600562918-05:00","closed_at":"2025-10-15T23:42:59.985107106-05:00"}
{"id":"serviceradar-23","title":"Agent device lacks IP from poller","description":"Investigate GH-1779: demo namespace k8s-agent missing canonical IP/device address. Ensure agent registration populates IP using poller metadata (poller.json). Track fixes including code/test, Bazel rebuild, rollout.","notes":"Post-rollout verification: demo deployment now reports agent device at ClusterIP 10.43.220.55, Armis 12818 entry confirmed not from faker dataset.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-16T00:42:44.733723664-05:00","updated_at":"2025-10-27T01:24:00.668562543-05:00","closed_at":"2025-10-16T01:18:06.982144289-05:00"}
{"id":"serviceradar-24","title":"Observability traces show Invalid Date","description":"GH-1767: Observability dashboard trace table renders \"Invalid Date\" instead of timestamps. Reproduce via https://demo.serviceradar.cloud/observability -\u003e Traces; entries lack valid timestamp. Need frontend fix and regression coverage.","notes":"Normalized Proton trace timestamps before rendering so Observability tables stop showing 'Invalid Date'. Added shared utils (normalizeTraceSummaryTimestamp/resolveTraceTimestampMs), wired TracesDashboard to sanitize both paged + streaming data, and wrote Vitest coverage for nanosecond + fallback handling. Verified with npm run test, npm run lint, make lint, then pushed ghcr.io/carverauto/serviceradar-web:sha-ef12ed1ab7a28c7444242c56c8549c04c525b6f6 (digest sha256:4d78db8e9fb41fed84af34caf6d6298a4132b6a03645dbc48926b28f6132b44d) via bazel run //docker/images:web_image_amd64_push and rolled the demo namespace deployment.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T01:49:37.420257116-05:00","updated_at":"2025-10-27T01:24:00.830747911-05:00","closed_at":"2025-10-16T02:02:54.387409947-05:00"}
{"id":"serviceradar-25","title":"Observability metrics show Invalid Date","description":"Metrics tab in Observability dashboard displays 'Invalid Date' timestamps, similar to traces issue. Need to normalize Proton DateTime64 values, add regression coverage, rebuild/push web image, roll demo.","notes":"Verified Observability metrics timestamps render correctly after client-side normalization fix. Build info now shows ghcr.io/carverauto/serviceradar-web:sha-f2b55efa23fd in demo; no further action required.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T02:04:53.589505051-05:00","updated_at":"2025-10-27T01:24:01.068719901-05:00","closed_at":"2025-10-16T08:44:18.310450791-05:00"}
{"id":"serviceradar-26","title":"db-event-writer loses NATS connection","description":"db-event-writer in demo emits repeated 'nats: connection closed' errors and stops processing events. Need to reproduce, harden reconnect logic (or crash for restart), and verify resilience.","notes":"Hardened db-event-writer JetStream loop: fatal fetch errors now bubble up so the service tears down the old consumer, re-establishes the NATS connection with infinite reconnects, and retries with a bounded backoff. Added unit coverage for both the consumer fatal-error path and the service reconnect loop, then ran 'go test ./pkg/consumers/db-event-writer/...' and 'make lint'. Built/pushed ghcr.io/carverauto/serviceradar-db-event-writer@sha256:ebcae9c408b75528cb4930e9e1164a328538a138579fce8cd7497107670f85c2 and rolled the demo deployment; new pod logs are clean (no repeated 'nats: connection closed').","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T08:51:40.813130347-05:00","updated_at":"2025-10-27T01:24:01.173162724-05:00","closed_at":"2025-10-16T09:15:46.111817669-05:00"}
{"id":"serviceradar-27","title":"serviceradar-core loses NATS connection","description":"Similar to db-event-writer issue serviceradar-26. Core logs show repeated KV publish failures with 'nats: connection closed' (GH-1787). Need to harden reconnect handling or crashloop so supervisors restart the service.","notes":"Rebuilt/pushed serviceradar-core with the refreshed KV client code (digest sha256:50e80febadf02dd55e134a1971769641dde93c1afb048e120d03e582a2e8d8d6) and restarted deployment/serviceradar-core in demo; post-rollout logs show no new 'nats: connection closed' warnings.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T09:26:23.031473125-05:00","updated_at":"2025-10-27T01:24:01.179348828-05:00","closed_at":"2025-10-16T10:05:23.96938382-05:00"}
{"id":"serviceradar-28","title":"serviceradar-core NATS reconnect instability (GH-1790)","description":"Investigate repeated \"nats: connection closed\" errors from serviceradar-core following GH issue 1790. Core should recover automatically after NATS restarts, but logs show the gRPC handlers continuing to fail until the pod is recycled. Need to reproduce, verify current reconnect logic, and harden it so the service either reconnects cleanly or crashes fast.","notes":"Hardened serviceradar-core NATS event publisher: added infinite reconnect options, automatic reinitialization with backoff, and error handling that schedules reinit on publish failures. Added regression test (TestEventPublisherReinitializesAfterConnectionClose) to prove the publisher rebuilds itself after a forced close. Ran go test ./pkg/core/... and make lint. Rebuilt/pushed ghcr.io/carverauto/serviceradar-core:sha-1d6e8e8fb708 (digest sha256:1d6e8e8fb708ae3cf6ffd65baac7e0765d976bb84d86e5c080f4982689bc4830) and rolled the demo deployment; new pod logs show the collector timing out during JetStream startup and then successfully reinitializing the publisher at 15:26:32Z.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T10:12:27.561799668-05:00","updated_at":"2025-10-27T01:24:01.206612079-05:00","closed_at":"2025-10-16T10:27:24.472523358-05:00"}
{"id":"serviceradar-29","title":"NATS JetStream bucket bloat causing restarts","description":"serviceradar-nats restarted 10x due to JetStream restore of KV_serviceradar-kv (~17M keys, 7.1GiB). Need to identify why canonical identity KV grows unbounded, prune/purge current data, and enforce retention (TTL/max bytes) so bucket stays bounded.","notes":"Retention knob finalized at 24h. Configs and cluster rolled as described above; validation commands (nats kv info serviceradar-kv, nats stream report) confirm Maximum Age=1d0h0m0s and an empty KV bucket.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T11:07:15.848535288-05:00","updated_at":"2025-10-27T01:24:01.355509382-05:00","closed_at":"2025-10-16T11:39:26.394486008-05:00"}
{"id":"serviceradar-3","title":"Investigate serviceradar-kv NATS backlog","description":"serviceradar-kv db-writer-service logs show nats connection closed errors when fetching messages: {\"level\":\"error\",\"component\":\"db-writer-service\",\"error\":\"nats: connection closed\",\"time\":\"2025-10-14T06:38:01Z\",\"message\":\"Failed to fetch messages\"}. From serviceradar-tools-59f47ff7bf-prlfj pod, nats stream ls shows events stream holding 48,418 messages (~5.7 GiB) with last message ~0.9s ago, indicating backlog. Need to troubleshoot serviceradar-kv consumption and restore processing.","notes":"Post-sync we drained the events stream after the identity publisher throttling landed. Verified via 'kubectl -n demo exec serviceradar-tools-59f47ff7bf-prlfj -- nats stream info events' that backlog is down to 856 msgs (~11 MiB) and consumers are keeping up.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T01:39:41.896667099-05:00","updated_at":"2025-10-27T01:24:01.449186052-05:00","closed_at":"2025-10-14T21:13:50.006965727-05:00"}
{"id":"serviceradar-30","title":"Prune stale identity map keys","description":"## Summary\nNATS KV bucket serviceradar-kv keeps growing even after reducing TTL, because the identity publisher only writes new identity keys and never removes stale ones.\n\n## Findings\n- identityPublisher.Publish iterates all identity keys for the update and blindly calls upsertIdentity, but there is no path that deletes or purges old keys (pkg/registry/identity_publisher.go:183).\n- Each update can introduce new per-identity keys via identitymap.BuildKeys (device id, ip, partition+ip, mac, integration ids); when values change we add a brand new KV entry (pkg/identitymap/identitymap.go:53).\n- Canonical hydration ensures the update carries forward prior metadata, so we have enough information to decide which identities are still valid and which are stale (pkg/registry/identity_resolver.go:244).\n\n## Proposed work\n1. Teach the identity publisher to derive the full set of identity key paths from the existing canonical record, diff against the current update, and delete keys that are no longer present.\n2. Extend the KV client interface to support deletes and update the publisher/tests to cover the prune path.\n3. Add metrics around key deletions so we can monitor churn and confirm the bucket stays bounded once deployed.","notes":"Rolled core build sha-afaee52dd89725f583ebcf49d3741aa1a6ab64f9 to demo and let sync run for several cycles; \"nats --context serviceradar kv info serviceradar-kv\" now reports History=1, Values=57, Size~16MiB with TTL 1d. Bucket growth stayed flat after clearing stale identity keys, so pruning logic looks good.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-16T11:59:03.782152956-05:00","updated_at":"2025-10-27T01:24:01.599827056-05:00","closed_at":"2025-10-16T13:54:57.019801603-05:00"}
{"id":"serviceradar-31","title":"OTEL collector should auto-recover missing trace stream","description":"OTEL publishes started failing with 'no stream found' after stream subjects dropped. Need better recovery so collector recreates stream and reconnects.","notes":"Rebuilt otel image with JetStream recovery logic (BuildBuddy 2a460cdc-ac0d-442b-ba5c-8c55a77b960f, digest ghcr.io/carverauto/serviceradar-otel@sha256:b4a05e4124a5fc74d9d1225c113ebfa818b7850d8ad0db4f25e76a046c6634b9). Rolled demo deployment; startup logs show stream subjects auto-restored and trace publishes succeeding.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T13:06:49.769192234-05:00","updated_at":"2025-10-27T01:24:01.68362673-05:00","closed_at":"2025-10-16T13:07:16.296478488-05:00"}
{"id":"serviceradar-32","title":"serviceradar-agent ICMP invalid argument (GH-1792)","description":"Symptoms: demo namespace agent logs show repeated  with  against hosts (e.g. 10.42.111.75) while sweeping ~50k targets. Need to reproduce, inspect ICMP sweeper send path, and ensure agent handles problematic targets gracefully. Notes:  utility unavailable in agent/tools images due to missing libcap.so.2; may rebuild serviceradar-tools for debugging.","notes":"Investigated the repeated `invalid argument` ICMP errors from the k8s agent; they only happened when the kernel rejected pod/cluster IP destinations with `EINVAL`. Updated the ICMP sweeper to (a) fall back to the packetconn writer only for send errors we can recover from, (b) mark destinations that still return `EINVAL`/`EADDRNOTAVAIL`/`EAFNOSUPPORT` and skip them on subsequent sweeps, and (c) surface the failure once with a WARN instead of logging every sweep. Added unit coverage for the fallback path and the invalid-destination suppression. Ran `go test ./pkg/scan/...` and `make lint`, then rebuilt/pushed ghcr.io/carverauto/serviceradar-agent@sha256:f422bd991dedc98331c7530c7d74051fbf4ff9498677e1d075d9720a92a1dd0d and rolled the demo deployment.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T14:00:04.944564045-05:00","updated_at":"2025-10-27T01:24:01.695862131-05:00","closed_at":"2025-10-16T14:31:51.325087361-05:00"}
{"id":"serviceradar-33","title":"Investigate demo device count discrepancy","description":"GH#1817: demo namespace stuck at 17k devices instead of 50k","notes":"Addressed Proton \"Too many partitions\" errors by switching otel_trace_summaries to daily partitions, recreating the stream + MV, and rotating Proton storage. Scaled core/db-event-writer/proton down, dropped their PVCs, reapplied manifests, and rolled deployments. Demo now backfilling cleanly and UI shows ~50k devices.","status":"closed","priority":2,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-20T15:35:03.557749615-05:00","updated_at":"2025-10-27T01:24:01.810826666-05:00","closed_at":"2025-10-20T20:19:13.895668576-05:00"}
{"id":"serviceradar-34","title":"Investigate serviceradar-core restarts in demo namespace","description":"Core deployment in demo namespace has 5 restarts in ~4 hours; gather logs and identify root cause.","notes":"Folded otel_trace_summaries partition fix into consolidated schema (00000000000001) and removed the blocking backfill from 00000000000003. Rebuilt/pushed core image (sha256:e28d7f1a47d6...) and db-event-writer image (sha256:d3ae1db88754...). Rolled Proton with fresh PVC, restarted core solo to run migrations, then brought db-event-writer back up. Core now applies migrations instantly and stays healthy; db-event-writer connects cleanly.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T21:28:40.435571694-05:00","updated_at":"2025-10-27T01:24:02.12501588-05:00","closed_at":"2025-10-23T23:21:02.621403051-05:00"}
{"id":"serviceradar-35","title":"Fix network sweeps page 404 (GH-1784)","description":"Network sweeps tab in UI fails due to missing API route; investigate and patch","notes":"Sweep summary now shows real counts via SRQL stats (total + reachable), built ghcr.io/carverauto/serviceradar-web@sha256:19fe14de96bd8ee7c48240efc19b6247a8a31abd77d684bc6a062056d0b2eb52 and rolled demo.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-20T22:46:22.901301259-05:00","updated_at":"2025-10-27T01:24:02.189636514-05:00","closed_at":"2025-10-23T23:20:40.406509343-05:00"}
{"id":"serviceradar-36","title":"GH-1784: Deploy updated sweep web image","description":"Build/push new serviceradar-web image and roll demo namespace","notes":"Rolled demo serviceradar-web to ghcr.io/carverauto/serviceradar-web@sha256:ef597439d0f79a43f8a0108df9fffab5fd508c6c7e2fdf06fdfb208be80b272b","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-21T10:50:34.873850836-05:00","updated_at":"2025-10-27T01:24:02.291927574-05:00","closed_at":"2025-10-21T10:54:36.334846609-05:00"}
{"id":"serviceradar-37","title":"GH-1840: handle new SRQL timestamp formats","description":"Registry first_seen parser fails when SRQL returns timestamps with spaces/timezone offsets; add normalization workaround and tests.","notes":"Normalized registry timestamp parsing to handle SRQL space/offset formats; added coverage; pushed serviceradar-core sha6aceeafb and rolled demo namespace.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-21T22:40:41.057234693-05:00","updated_at":"2025-10-27T01:24:02.387637195-05:00","closed_at":"2025-10-21T22:47:17.188959438-05:00"}
{"id":"serviceradar-38","title":"GH-1842: identity map unmarshal failure","description":"Investigate and fix core identity map canonical record unmarshalling error from GH issue 1842","notes":"Purged serviceradar-kv bucket, recreated with history=1 TTL=24h, restarted core/agent/sync to regenerate sweep + identity data; bucket repopulated (56 keys after 90s) with no new corrupt warnings in recent logs.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-21T23:30:45.348366323-05:00","updated_at":"2025-10-27T01:24:03.023294153-05:00","closed_at":"2025-10-23T23:20:37.536889568-05:00"}
{"id":"serviceradar-39","title":"GH-1764: cap sync OTEL export size","description":"Investigate oversized OTEL export payloads from serviceradar-sync, add regression test, implement fix, roll demo.","notes":"Rebuilt/pushed images; sync digest now sha256:aedd5288bee1c204683e883dc4812c38b6dbf67a96b5a0c74eaa7d165a9438a6. Demo namespace rolled + sync deployment pinned to new tag.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-22T00:00:04.090550094-05:00","updated_at":"2025-10-27T01:24:03.253768488-05:00","closed_at":"2025-10-22T01:07:30.151878734-05:00"}
{"id":"serviceradar-4","title":"Docker compose login fails with 500 after reboot","description":"Docker compose stack intermittently returns 500s on /auth/login because core restarts while Proton generates DH params and nginx sometimes binds to :8888 when host port 80 is taken, breaking UI access.","notes":"Patched proton init to reuse cached DH params via docker/compose/proton-init.sh and added docs/docs/runbooks/docker-compose-login-500.md so folks know why auth breaks after reboot. Disabled the host nginx service and restarted docker compose nginx so it binds :80 cleanly. Verified `docker compose ps nginx` shows 0.0.0.0:80-\u003e80/tcp and login now routes through Kong without 500s.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T15:35:54.351022351-05:00","updated_at":"2025-10-27T01:24:03.348977472-05:00","closed_at":"2025-10-14T15:36:10.547101164-05:00"}
{"id":"serviceradar-40","title":"GH-1845: fix OTEL log attribute truncation","description":"Investigate why OTEL log attribute lists (e.g. all_ips) are truncated to a single value in the UI. Capture the raw attribute payload coming back from ClickHouse/Timeplus, add automated coverage, and ship a fix so users can copy the full message without losing entries.","notes":"Added otelAttributes parser + regression test so attribute values keep JSON arrays intact. Bazel pushed ghcr.io/carverauto/serviceradar-web:sha-4f530784b13ff3eccad994ea38204159f4e1aef4 and rolled demo serviceradar-web deployment; verified pod is Ready.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-22T11:06:28.909420558-05:00","updated_at":"2025-10-27T01:24:03.52588243-05:00","closed_at":"2025-10-23T23:20:34.267785344-05:00"}
{"id":"serviceradar-41","title":"GH-1853: Accelerate analytics dashboard queries","description":"Baseline dashboard queries are taking multiple seconds to load in the demo environment. Examples:\n- in:otel_trace_summaries time:last_24h sort:duration_ms:desc limit:100 → ~4.5s\n- in:otel_trace_summaries status_code!=1 stats:\"count() as total\" sort:total:desc time:last_24h → ~3s\n- in:otel_trace_summaries stats:\"count() as total\" sort:total:desc time:last_24h → ~3s\n- in:logs severity_text:(fatal,error) time:last_24h sort:timestamp:desc limit:100 → ~2s\n- in:devices time:last_7d limit:500 → several seconds\nNeed to profile and improve query performance so the primary analytics dashboard feels instant.","notes":"Refactored web trace counts to reuse the combined otel_trace_summaries stats query (useTraceCounts now parses sum(if(...)) output via traceCountsUtils), added targeted tests, and shipped ghcr.io/carverauto/serviceradar-web:sha-71f597e11f8d to demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-22T11:28:43.846749485-05:00","updated_at":"2025-10-27T01:24:04.546882981-05:00","closed_at":"2025-10-23T23:20:31.831697022-05:00"}
{"id":"serviceradar-42","title":"GH-1856: Update header SRQL input from view queries","description":"Problem: Network ➜ Sweeps issues an SRQL call via /api/devices/sweep but the header search box keeps showing the default 'in:devices ...' query so users cannot see which filter produced the data.\\nRoot cause: the header owns its own query state and there is no shared channel for feature views to surface the SRQL they execute.\\nPlan: introduce a shared SRQL query context, make the header consume it, push the sweeps view (and other callers) to set the active query before/after they fetch, and cover the state transitions with a small regression test.","notes":"Header search now always routes to /query (even when the active SRQL came from a view). Built/pushed ghcr.io/carverauto/serviceradar-web:sha-04ce6cf6bf9e (digest sha256:9c62606b85cb21e0c2832e03b10128f4d3856892b66f4431bd777db8a101719b) and rolled demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-22T14:07:36.369809295-05:00","updated_at":"2025-10-27T01:24:04.635655257-05:00","closed_at":"2025-10-23T23:20:25.751249972-05:00"}
{"id":"serviceradar-43","title":"Poll sweeps state for discovery dashboard","description":"Add background polling of API/KV to surface active network sweeps in the UI so cards can deep-link even before sweep results arrive.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-22T21:35:29.890615882-05:00","updated_at":"2025-10-27T01:24:04.728292253-05:00","closed_at":"2025-10-23T23:20:59.701847056-05:00"}
{"id":"serviceradar-44","title":"GH-1864: sync sweep chunk write fails","description":"JetStream rejects sweep chunk writes once payloads exceed its block size (see GH-1864 logs). Need to cap chunk payloads and cover with regression test.","notes":"Closing out legacy sweep chunk investigation so we can focus on GH-1844; no pending work will be lost.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-23T01:06:43.008337884-05:00","updated_at":"2025-10-27T01:24:04.883519364-05:00","closed_at":"2025-10-23T23:20:21.398025672-05:00"}
{"id":"serviceradar-45","title":"GH-1867: Expand Proton PVC capacity","description":"Proton local-path PVC out of space in demo; need manifest bump + resize script.","notes":"Bumped Proton PVC request to 1Ti with explicit local-path storage class, added resize script for safe scale-down/recreate, and refreshed deployment guide persistence table.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-23T02:07:09.034290879-05:00","updated_at":"2025-10-27T01:24:05.015996103-05:00","closed_at":"2025-10-23T23:20:55.610610015-05:00"}
{"id":"serviceradar-46","title":"GH-1863 device details availability","description":"Goal: enrich the device detail page with full registry metadata, sysmon/ICMP metrics, and an availability timeline derived from historical device updates. Tasks: [ ] audit current API responses vs registry metadata; [ ] expose availability history endpoint in Go API leveraging versioned device data; [ ] expand Next.js device page to render metadata panes, metrics summaries, and uptime chart; [ ] validate via make lint/test then ship Bazel image + demo rollout.","notes":"Implemented SRQL-driven device detail view: queries unified_devices/device_updates/timeseries_metrics to hydrate metadata, availability timeline, and Sysmon summaries. Added uptime timeline visualization, telemetry cards, and SRQL-backed metric chart. Verified with npm run lint \u0026\u0026 npm test -- --watch=false.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-23T02:44:12.815226611-05:00","updated_at":"2025-10-27T01:24:05.118100743-05:00","closed_at":"2025-10-23T23:20:52.649453568-05:00"}
{"id":"serviceradar-47","title":"GH-1869 device inventory spinner","description":"- Reviewed GH issue 1869; navigation away leaves SRQL context tied to prior view so devices dashboard never fetches.\\n- Repro: open Devices, switch to another tab (Network/Analytics), return to Devices -\u003e spinner never resolves.\\n- Plan: make Devices dashboard reclaim SRQL view when active, ensure query runs, add regression coverage.","notes":"- Sweeps view now queries via /api/query using buildSweepQueryWithLimit, including auth cookies; old /api/devices/sweep dependency removed.\\n- Bazel-built and deployed ghcr.io/carverauto/serviceradar-web:sha-81c5c53925a8 after lint/tests.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-23T10:02:15.668346327-05:00","updated_at":"2025-10-27T01:24:05.124890512-05:00","closed_at":"2025-10-23T23:20:47.76076068-05:00"}
{"id":"serviceradar-48","title":"DataService JetStream API","description":"## Summary\n- carve object-store functionality out of KVService into a dedicated DataService gRPC surface\n- rename the kv binary to cmd/data-services and serve both KVService + DataService from the same process\n- add JetStream object bucket support plus RBAC wiring while keeping existing KV semantics intact\n\n## What’s done\n- added proto/data_service.proto and generated Go stubs; trimmed proto/kv.proto back to pure KV RPCs\n- moved the binary to cmd/data-services, registered both services during startup, and updated Bazel/packaging/Docker references\n- extended pkg/kv (server, NATS store, config) to expose DataService RPCs and manage JetStream objects with sensible defaults\n- refreshed mocks/interfaces in pkg/kv and pkg/sync to reflect the new service boundary without forcing callers to use it yet\n- gofmt/go test ./... clean after refactor\n\n## Next steps\n- update downstream clients (sync, web, tools) to call DataService for object payloads when ready\n- document the new gRPC contract + curl examples in docs/docs/ or beads for rollout\n- consider Bazel targets or Helm chart changes if we want a distinct data-service deployment in the future","notes":"Datasvc crash fixed: rebuilt pushed ghcr.io/carverauto/serviceradar-datasvc:sha-d37e173ef80b4b3d858dfa26c247e15d955dfe12 (sha256:a25f1de89075b971c70fdbafb72a62ae958ef4c7daa49cd751133999bc5d6525) and rolled the deployment; pod now initializes mTLS with role=datasvc and the sweep metadata watch is active.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-23T15:23:38.213335797-05:00","updated_at":"2025-10-27T01:24:05.266164037-05:00","closed_at":"2025-10-23T23:20:45.539246777-05:00"}
{"id":"serviceradar-49","title":"Verify pkg/datasvc refactor","description":"## Context\n- pkg/kv was renamed to pkg/datasvc using IDE tooling (GoLand)\n- BUILD.bazel adjusted, but residual references may remain\n\n## Remaining Work\n- Audit repo for lingering pkg/kv import paths, Bazel targets, go:generate directives, CI scripts\n- Ensure gofmt/goimports applied where necessary and module state is clean\n- Run go test ./... and Bazel builds for cmd/data-services + datasvc image\n- Update docs/comments still mentioning pkg/kv\n\n## Validation\n- go test ./...\n- bazel build --config=remote //cmd/data-services:data_services //docker/images:datasvc_image_amd64\n- Optional: go build ./cmd/...","notes":"Handled datasvc health checks: external checker now returns generic health JSON instead of calling monitoring.AgentService, set kv registry entry to use the 'datasvc' gRPC health service, regenerated mocks, ran go test ./... and bazel build/push for agent image (ghcr.io/carverauto/serviceradar-agent:sha-045dcef4dbeb) and rolled the deployment.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-23T19:37:51.134847609-05:00","updated_at":"2025-10-27T01:24:05.375796025-05:00","closed_at":"2025-10-23T23:20:43.345909763-05:00"}
{"id":"serviceradar-5","title":"Flowgger fails hard when NATS is unreachable","description":"Flowgger exits with a panic when it cannot reach NATS (Connection refused from nats_output.rs:139). When running in demo/K8s the pod churns instead of retrying. Decide whether to crash fast so the pod restarts immediately or add exponential backoff with limits to reconnect to NATS.","notes":"Implemented exponential backoff for Flowgger's NATS output (cmd/flowgger/src/flowgger/output/nats_output.rs) with configurable retry knobs and rolled fresh defaults into the sample configs. Rebuilt and pushed ghcr.io/carverauto/serviceradar-flowgger@sha256:175dd739b1b832a75a58c73bf528d10e541792e596e7e50398eec5c92e704322 so demo env picks up the change.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T15:36:29.941341235-05:00","updated_at":"2025-10-27T01:24:05.784781234-05:00","closed_at":"2025-10-14T16:06:05.148133185-05:00"}
{"id":"serviceradar-50","title":"GH-1844: restore observability logs service filter","description":"Logs UI lost the service dropdown contents after SRQL refactor; need to repopulate using in:services via /api/query.","notes":"Ported logs service picker to query in:services via SRQL, added extractor util + tests, and verified web lint/test.","status":"in_progress","priority":1,"issue_type":"bug","created_at":"2025-10-23T23:21:38.865242835-05:00","updated_at":"2025-10-27T01:24:06.128404599-05:00","comments":[{"id":1,"issue_id":"serviceradar-50","author":"mfreeman","text":"Progress: service dropdown now driven by in:services SRQL, new parser util + tests, npm run lint/test green.","created_at":"2025-10-24T04:26:58Z"},{"id":2,"issue_id":"serviceradar-50","author":"mfreeman","text":"Progress: service dropdown now driven by in:services SRQL, new parser util + tests, npm run lint/test green.","created_at":"2025-10-24T04:27:56Z"}]}
{"id":"serviceradar-51","title":"GH-1844: restore observability logs service filter","description":"Logs UI lost the service dropdown contents after SRQL refactor; need to repopulate using in:services via /api/query.","notes":"Plan: 1) inspect current logs service picker + SRQL wiring, 2) query in:services via /api/query to provide dropdown options, 3) ensure selecting a service updates SRQL/filter and persists across navigation, 4) add regression coverage, rebuild/push web image, roll demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-23T23:27:38.87161976-05:00","updated_at":"2025-10-27T01:24:06.854367085-05:00","closed_at":"2025-10-24T00:26:30.127570521-05:00","comments":[{"id":3,"issue_id":"serviceradar-51","author":"mfreeman","text":"Deployment: bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-18e47406ac39 (digest sha256:a5758508f89ade4988fa4132169035e7f7506e944ff4bd37a956bd6aa18116a9), rolled serviceradar-web in demo via kubectl set image + rollout status.","created_at":"2025-10-24T04:32:40Z"},{"id":4,"issue_id":"serviceradar-51","author":"mfreeman","text":"Follow-up: switched logs service fetch to stats:\"group_uniq_array(service_name) as services\", expanded parser to handle aggregated arrays/JSON, npm run lint/test kept green.","created_at":"2025-10-24T04:41:10Z"},{"id":5,"issue_id":"serviceradar-51","author":"mfreeman","text":"Deployment refresh: bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-dafbe3aead9adc2d321effb770ccdaad11131d1d (digest sha256:5e78fe8c09fb8f01c04509357b1d2635aee6de9c704edc124b729ddc38337ac8) and rolled demo deployment.","created_at":"2025-10-24T04:43:31Z"},{"id":6,"issue_id":"serviceradar-51","author":"mfreeman","text":"Reopening for service name canonicalization; dropdown still shows short names (sync, zen, etc.) and misses serviceradar-core.","created_at":"2025-10-24T04:56:23Z"},{"id":7,"issue_id":"serviceradar-51","author":"mfreeman","text":"Mapping short service names to serviceradar-* canonical forms (including manual serviceradar-core), updated parser/tests, npm run lint/test clean.","created_at":"2025-10-24T04:58:19Z"},{"id":8,"issue_id":"serviceradar-51","author":"mfreeman","text":"Built/pushed ghcr.io/carverauto/serviceradar-web:sha-5d451997e412 (digest sha256:b66e39ab49703e89faf2ffc55cd6049343ac7b1dcf323c20f547050dfba171f5) and rolled demo deployment.","created_at":"2025-10-24T05:02:25Z"},{"id":9,"issue_id":"serviceradar-51","author":"mfreeman","text":"Refactored service filter into reusable component/hook, added canonical query aliases, wired selector into Traces dashboard; npm run lint/test green.","created_at":"2025-10-24T05:08:51Z"},{"id":10,"issue_id":"serviceradar-51","author":"mfreeman","text":"Bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-ce492b7935fd (digest sha256:e2733e3e0c8a0df2e5b08eabd965af4110f080f16a5996431557d0ae9faec6c7), rolled demo deployment.","created_at":"2025-10-24T05:11:11Z"},{"id":11,"issue_id":"serviceradar-51","author":"mfreeman","text":"Final deploy: bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-1a35b3440692 (digest sha256:33d8fdf441a1bbd36de9ef9f431bc9125a1304f872050cdad11d7e1016de663e), rolled demo deployment.","created_at":"2025-10-24T05:26:03Z"}]}
{"id":"serviceradar-52","title":"Finalize SPIRE service exposure and packaging","description":"- Switch SPIRE service to managed ingress for edge agents\\n- Decide namespace layout to bundle SPIRE with ServiceRadar installs (Helm, demo)\\n- Document Helm/automation expectations and update manifests accordingly\\n- Track alignment with GH-1892","notes":"Identified postStart hook failure: spire-server image lacks /bin/sh, so lifecycle registration script crashed the pod and bubbled up as k8s_sat context-canceled errors. Removed the hook and reapplied the stack; server now healthy and agents attest. Need replacement automation to seed registration entries.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-25T18:48:02.991588963-05:00","updated_at":"2025-10-27T02:16:35.959815385-05:00","closed_at":"2025-10-27T02:16:35.959815385-05:00","labels":["infra","spire"],"comments":[{"id":12,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T02:09:23Z"},{"id":13,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T02:09:24Z"},{"id":14,"issue_id":"serviceradar-52","author":"mfreeman","text":"SPIRE bootstrap now hands out core/poller entries automatically: Job runs via the tools image (ships kubectl + spire-server) and execs into the server pod, keeping KV untouched. Core/poller manifests switched to \"spiffe\" security mode, KV overlays filtered so TLS/SPIFFE blocks stay source-of-truth, and both images pushed (core@sha256:e0e7fbcf..., poller@sha256:cc1e1302...). RBAC remains intact by matching SPIFFE IDs instead of CN—no functionality lost.","created_at":"2025-10-26T02:09:25Z"},{"id":15,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T02:16:02Z"},{"id":16,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T02:16:03Z"},{"id":17,"issue_id":"serviceradar-52","author":"mfreeman","text":"Datasvc RBAC now prefers SPIFFE URIs (fallback to DN for legacy mTLS) with new unit coverage. Demo + Helm configs whitelist core/poller SPIFFE IDs alongside existing CNs, and deploy.sh mirrors the entries. go test ./pkg/datasvc/... passes.","created_at":"2025-10-26T02:16:03Z"},{"id":18,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T02:38:28Z"},{"id":19,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T02:38:28Z"},{"id":20,"issue_id":"serviceradar-52","author":"mfreeman","text":"Core/poller now run in SPIFFE mode after reapplying configmap to demo, syncing the security block via init script, and rolling the workloads. SPIRE rebuilt (postgres + bootstrap) and agents/core/poller fetching SVIDs cleanly.","created_at":"2025-10-26T02:38:29Z"},{"id":21,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T02:44:03Z"},{"id":22,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T02:44:03Z"},{"id":23,"issue_id":"serviceradar-52","author":"mfreeman","text":"Configs should keep loading from KV, but SPIFFE security must override anything in KV (overlay stripping + init sync now enforce that). Next step is to stop reading TLS materials from KV altogether—each service should grab cert/key from its canonical /etc/serviceradar/certs/\u003cservice\u003e.pem so secrets never transit the KV. Tracking this hardening in the issue.","created_at":"2025-10-26T02:44:04Z"},{"id":24,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T02:50:55Z"},{"id":25,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T02:50:56Z"},{"id":26,"issue_id":"serviceradar-52","author":"mfreeman","text":"Added pkg/config overlay tests to prove KV can't mutate security blocks while still allowing ordinary fields to change. Helm core deployment now mirrors the demo: seed/init container preserves JWKS, init script syncs template security into /var/lib, and the pod mounts the SPIRE socket plus KV env so SPIFFE stays authoritative. Ready to finish wiring other Helm pieces once SPIRE chart lands.","created_at":"2025-10-26T02:50:56Z"},{"id":27,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T02:54:36Z"},{"id":28,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T02:54:36Z"},{"id":29,"issue_id":"serviceradar-52","author":"mfreeman","text":"Helm chart now ships the SPIRE bootstrap flow: new values under spire.* control the trust domain/cluster SAs, and conditional templates render the bootstrap ConfigMap, RBAC, and Job using the tools image. Attempted to run 'helm template' for a quick lint but the CLI isn't installed in this environment.","created_at":"2025-10-26T02:54:36Z"},{"id":30,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T03:00:49Z"},{"id":31,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T03:00:50Z"},{"id":32,"issue_id":"serviceradar-52","author":"mfreeman","text":"Helm now owns the SPIRE control plane: added server/agent/postgres templates keyed off values under spire.*, reusing the same trust-domain + cluster settings as the demo. Chart render still trips over the long-standing serviceradar-config.tpl escaping (tpl + Slack webhook braces); left a note to revisit that so we can lint the whole chart once the placeholders are escaped.","created_at":"2025-10-26T03:00:50Z"},{"id":33,"issue_id":"serviceradar-52","author":"mfreeman","text":"Helm template still fails because serviceradar-config uses tpl and contains unescaped handlebars; need to replace those placeholders with Go template safe strings (likely via double brace escaping).","created_at":"2025-10-26T03:02:03Z"},{"id":34,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T03:06:00Z"},{"id":35,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T03:06:00Z"},{"id":36,"issue_id":"serviceradar-52","author":"mfreeman","text":"Escaped the Slack webhook handlebars in helm/serviceradar/files/serviceradar-config.yaml (using {{}}/{{}}) so tpl stops choking; helm template now renders cleanly. Also updated the Helm poller Deployment to mount /run/spire/sockets like the demo so SPIFFE client creds work out of the box. Latest render stashed in /tmp/helm-render.yaml.","created_at":"2025-10-26T03:06:01Z"},{"id":37,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T03:13:55Z"},{"id":38,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T03:13:55Z"},{"id":39,"issue_id":"serviceradar-52","author":"mfreeman","text":"Helm datasvc/agent/zen now speak SPIFFE: datasvc.json/security flipped to spiffe mode, agent/zen kv_security dial datasvc via SVID (server ID spiffe://{{}}/...); deployments mount /run/spire/sockets and probes switched to TCP where needed. Bootstrap job gained a datasvc entry with new values knob. helm template serviceradar helm/serviceradar renders cleanly after the escape fix.","created_at":"2025-10-26T03:13:56Z"},{"id":40,"issue_id":"serviceradar-52","author":"mfreeman","text":"Helm SPIFFE parity update: datasvc/agent/zen configs now run in spiffe mode with socket mounts, bootstrap job seeds the datasvc SVID, and helm template renders cleanly. Next up is to mirror the same socket mounts + spiffe security in the remaining Helm/Kustomize services (mapper, sync, checkers) and finally prune the legacy mTLS cert usage.","created_at":"2025-10-26T03:15:34Z"},{"id":41,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T03:22:35Z"},{"id":42,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T03:22:35Z"},{"id":43,"issue_id":"serviceradar-52","author":"mfreeman","text":"Agent now runs in SPIFFE mode in both demo + Helm: configmap switches kv/security blocks, deployment mounts /run/spire/sockets and uses a dedicated ServiceAccount, and bootstrap job seeds the serviceradar-agent workload entry. Helm values/template got the matching service account + env wiring. Next targets are mapper/sync/checkers to follow the same pattern, then we can rip out the legacy cert mounts.","created_at":"2025-10-26T03:22:35Z"},{"id":44,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T03:35:12Z"},{"id":45,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T03:35:12Z"},{"id":46,"issue_id":"serviceradar-52","author":"mfreeman","text":"Split SPIRE bootstrap tooling into its own image: created //docker/images:spire_bootstrap_image_amd64 (ships kubectl+jq+bash) and pointed the demo/Helm Jobs at ghcr.io/carverauto/serviceradar-spire-bootstrap. Stripped kubectl from serviceradar-tools so the general toolbox no longer has cluster-admin capabilities. Bazel build + helm template both succeed; next up is rolling the agent deployment to confirm it actually pulls SVIDs with the new config.","created_at":"2025-10-26T03:35:12Z"},{"id":47,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T03:49:01Z"},{"id":48,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T03:49:01Z"},{"id":49,"issue_id":"serviceradar-52","author":"mfreeman","text":"Pushed new bootstrap + tools images (spire-bootstrap@sha256:83d0fd52..., tools@sha256:41ab145e...), reapplied the SPIRE stack, and reran the bootstrap job so it created the serviceradar-agent workload entry. Rolled the demo agent after updating the configmap; pod logs now show \"mode\":\"spiffe\" with the workload socket. Next up: migrate mapper/sync/checkers to the same SPIFFE auth pattern.","created_at":"2025-10-26T03:49:01Z"},{"id":50,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T03:58:53Z"},{"id":51,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T03:58:53Z"},{"id":52,"issue_id":"serviceradar-52","author":"mfreeman","text":"Queued question for SPIFFE maintainers: asked whether there’s a safer pattern for seeding entries than our kubectl exec Job (since the Job image still carries kubectl, even though we scoped its service account tightly). Waiting on their guidance before we pursue alternatives.","created_at":"2025-10-26T03:58:54Z"},{"id":53,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T04:23:55Z"},{"id":54,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T04:23:56Z"},{"id":55,"issue_id":"serviceradar-52","author":"mfreeman","text":"Controller-manager sidecar is healthy in demo (clusterSPIFFEID entries show up), but the k8s agent pod still stalls while waiting on the SPIFFE socket. Need to trace the agent init path next—likely the workload entry or socket wiring—but parking for now.","created_at":"2025-10-26T04:23:56Z"},{"id":56,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T04:32:50Z"},{"id":57,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T04:32:51Z"},{"id":58,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T04:37:15Z"},{"id":59,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T04:37:21Z"},{"id":60,"issue_id":"serviceradar-52","author":"mfreeman","text":"Agent stall traced to missing /run/spire/sockets mount in the live deployment. Reapplied k8s/demo/base/serviceradar-agent.yaml (with the spire-agent-socket hostPath) to demo and rolled the Deployment; pod now sees agent.sock and stays Ready in spiffe mode.","created_at":"2025-10-26T04:37:25Z"},{"id":61,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T05:43:16Z"},{"id":62,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T05:43:16Z"},{"id":63,"issue_id":"serviceradar-52","author":"mfreeman","text":"Rolled SPIRE over to k8s_psat: server + agent configs now use the PSAT attestor, controller-manager parent IDs match the k8s_psat agent SVIDs, and the agent DaemonSet mounts a projected SAT token (/var/run/secrets/tokens/spire-agent). After reapplying the stack and restarting the DaemonSet/StatefulSet, node attestation succeeds and ClusterSPIFFEIDs reconcile without manual jobs. Datasvc now boots with split security (SPIFFE front door, mTLS NATS), core consumes the new kv_security block to authorize the datasvc SPIFFE ID, and poller rolled cleanly. Remaining to-dos: migrate mapper/sync/checker configs to the same SPIFFE pattern, prune legacy KV cert mounts, verify agent gRPC connectivity (current poller health checks still hit the old TLS endpoint), and mirror these changes into Helm + docs.","created_at":"2025-10-26T05:43:16Z"},{"id":64,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T05:54:08Z"},{"id":65,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T05:54:08Z"},{"id":66,"issue_id":"serviceradar-52","author":"mfreeman","text":"Rebuilt serviceradar-agent (sha256:bb62d2f7dc7070bfeada2198974acc4fb5d1830acb74d9bc9465a41735c16d15) with the SPIFFE client fixes and rolled the demo Deployment; agent now boots cleanly in spiffe mode and hydrates KV after the controller-manager switch. Added debug logging around server SPIFFE validation to help catch bad IDs going forward.","created_at":"2025-10-26T05:54:09Z"},{"id":67,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T06:05:36Z"},{"id":68,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T06:05:36Z"},{"id":69,"issue_id":"serviceradar-52","author":"mfreeman","text":"Converted serviceradar-sync to SPIFFE for KV traffic: configmap + Helm now set mode=spiffe with datasvc SVID, deployments mount /run/spire/sockets, and new ClusterSPIFFEID + service account were added. Datasvc RBAC also trusts the sync SPIFFE ID. Next up: repeat the same pattern for mapper/zen/checkers.","created_at":"2025-10-26T06:05:37Z"},{"id":70,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T06:23:16Z"},{"id":71,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T06:23:16Z"},{"id":72,"issue_id":"serviceradar-52","author":"mfreeman","text":"Core now consumes KV over SPIFFE end-to-end: updated env wiring, rebuilt/pushed ghcr.io/carverauto/serviceradar-core@sha256:0459bcd60bb9b0dd7265d947514693aa706015170cae4eadd6e05d19b647017f, re-applied demo manifests (namespaced correctly) and rolled the deployment. Core pod comes up healthy with no more datasvc CN mismatch warnings.","created_at":"2025-10-26T06:23:17Z"},{"id":73,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T06:43:11Z"},{"id":74,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T06:43:19Z"},{"id":75,"issue_id":"serviceradar-52","author":"mfreeman","text":"Sync SPIFFE regression fixed: added normalizeServerSPIFFEID() so clients accept trust-domain-relative IDs, covered it with unit tests, rebuilt the sync image (ghcr.io/carverauto/serviceradar-sync@sha256:9a66b0bbcfbe7baa7f27026aef3eae0aa0356e76d1cc89ffa276409fe60a2874), and rolled the demo deployment. Sync now boots cleanly in SPIFFE mode and KV traffic succeeds. Next up: propagate the SPIFFE wiring to mapper/zen/checkers and prune the leftover mTLS paths.","created_at":"2025-10-26T06:43:20Z"},{"id":76,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T16:16:30Z"},{"id":77,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T16:16:30Z"},{"id":78,"issue_id":"serviceradar-52","author":"mfreeman","text":"Moved serviceradar-zen to SPIFFE: configmap + Helm now set kv_security to datasvc's SVID, datasvc RBAC trusts the zen identity, Deployment mounts the workload socket with a dedicated service account, and SPIRE gets a new ClusterSPIFFEID for the pod selector. Helm values/templates gained the matching service account wiring. Next up is to repeat the pattern for mapper and the checker workloads before stripping the lingering CN entries.","created_at":"2025-10-26T16:16:31Z"},{"id":79,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T16:40:55Z"},{"id":80,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T16:40:56Z"},{"id":81,"issue_id":"serviceradar-52","author":"mfreeman","text":"Zen now runs on SPIFFE end-to-end: added spiffe crate support to load SVIDs from the workload API, generate Tonic identities, and updated config parsing to understand security modes. K8s/Helm configs switch zen.grpc_security to \"spiffe\" and mount the socket. Mapper follows the same pattern (SPIFFE security block, socket mount, service account + ClusterSPIFFEID) and datasvc RBAC trusts its SVID. Cargo/go/helm checks all pass. Next step: rebuild/push zen + mapper images, apply the K8s changes, and verify the workloads pick up SVIDs.","created_at":"2025-10-26T16:40:56Z"},{"id":82,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T16:50:26Z"},{"id":83,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T16:50:26Z"},{"id":84,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T17:06:23Z"},{"id":85,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T17:06:24Z"},{"id":86,"issue_id":"serviceradar-52","author":"mfreeman","text":"Rebuilt zen (ghcr.io/carverauto/serviceradar-zen@sha256:e675da30a7cb9e59e9b93c99863d3436b537e6ae139c6f1c4b27743a840ad5b7) and mapper (ghcr.io/carverauto/serviceradar-mapper@sha256:03b723c4bd74fbbdc1fa1f7554751e313a97473c2d2a3c9efa74e5d1f4c68876), applied the updated demo ConfigMap + SPIRE entries, and rolled both deployments. Zen now runs under its own service account with the workload socket mounted; mapper pod logs show mode=\"spiffe\" and no fallback TLS usage. Need to mirror the SPIFFE wiring for the remaining checkers/services and prune the old KV cert settings before we can delete the legacy mTLS path entirely.","created_at":"2025-10-26T17:06:24Z"},{"id":87,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T18:15:51Z"},{"id":88,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T18:15:51Z"},{"id":89,"issue_id":"serviceradar-52","author":"mfreeman","text":"DB event writer now runs with SPIFFE security: deployment mounts /run/spire/sockets, has its own SA + ClusterSPIFFEID, and config JSON sets security.mode=\"spiffe\". Built/pushed ghcr.io/carverauto/serviceradar-db-event-writer@sha256:6fc9eba325735588c21a2aed5690a8c1dfca824ab0a7c8e7c727b995f7174e31 and rolled the pod—logs show SPIFFE mode plus healthy NATS batches. Poller is still hitting mTLS CN validation errors against zen/mapper/db-event-writer (expects agent.serviceradar); next task is to teach the poller gRPC client to use SPIFFE before we can retire those failures.","created_at":"2025-10-26T18:15:52Z"},{"id":90,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T18:21:19Z"},{"id":91,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T18:21:20Z"},{"id":92,"issue_id":"serviceradar-52","author":"mfreeman","text":"Poller now dials the agent with SPIFFE: updated poller.json in configmap + Helm to set server_spiffe_id/workload_socket, rolled the deployment, and the agent handshake errors (wanted agent.serviceradar) are gone. The poller still hits StreamResults failures for sync because the agent’s outbound client expects legacy TLS (unexpected ID spiffe://.../serviceradar-sync); that’s the next migration target. Also noted the lingering zen workload API log we need to chase.","created_at":"2025-10-26T18:21:21Z"},{"id":93,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T18:43:03Z"},{"id":94,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T18:43:03Z"},{"id":95,"issue_id":"serviceradar-52","author":"mfreeman","text":"Agent now ships a SPIFFE-aware checker config for sync (see sync-checker.json in serviceradar-config) and the init container copies it into /etc/serviceradar/checkers. After redeploying agent + poller, StreamResults succeeds—poller logs show sync batches streaming without handshake errors. Next up: extend the same pattern to the remaining gRPC checkers (mapper, zen, db-event-writer, etc.) and investigate zen’s workload API source log.","created_at":"2025-10-26T18:43:04Z"},{"id":96,"issue_id":"serviceradar-52","author":"mfreeman","text":"Zen still logs 'X509Source has been closed.' because we explicitly close the workload source after extracting the first SVID (see cmd/consumers/zen/src/spiffe.rs). That means we’re not yet handling SVID rotation; capturing this so we can decide whether to keep the current one-shot behavior or hold the source open for auto-rotate.","created_at":"2025-10-26T18:43:45Z"},{"id":97,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T19:03:37Z"},{"id":98,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T19:03:37Z"},{"id":99,"issue_id":"serviceradar-52","author":"mfreeman","text":"Agent now seeds a SPIFFE-aware sync checker config (k8s/demo/base/configmap.yaml:441) and the init container copies it into place (k8s/demo/base/serviceradar-agent.yaml:25); Helm mirrors the same pattern. After rolling agent + poller, StreamResults completes successfully, so sync’s dataset streams without TLS errors. Next up: migrate the other gRPC checkers (mapper, zen, db-event-writer, etc.) to SPIFFE and decide whether zen should keep its workload source open for SVID rotation instead of closing it immediately.","created_at":"2025-10-26T19:03:37Z"},{"id":100,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T19:09:20Z"},{"id":101,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T19:09:20Z"},{"id":102,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T19:18:13Z"},{"id":103,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T19:18:14Z"},{"id":104,"issue_id":"serviceradar-52","author":"mfreeman","text":"Agent now honors per-checker security config so we can hand each gRPC checker its own SPIFFE server ID; demo + Helm config ship mapper/zen/db-event-writer checker JSON alongside sync. Zen’s SPIFFE helper keeps the workload source open via a guard so we stop closing the X509Source immediately (rotation-ready). Ran go test ./pkg/agent, cargo check -p serviceradar-zen, and helm template serviceradar helm/serviceradar to make sure the updates compile and render.","created_at":"2025-10-26T19:18:14Z"},{"id":105,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T19:41:50Z"},{"id":106,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T19:41:50Z"},{"id":107,"issue_id":"serviceradar-52","author":"mfreeman","text":"Pushed ghcr.io/carverauto/serviceradar-agent@sha256:004a0b98dba6c8e467538dd1e8c5c72ae06dc3f863a1d61f84fcb6fa31bf2a20, applied the refreshed configmap/deployment, and rolled the agent in demo. Agent logs show SPIFFE security per checker (mapper/zen/db-event-writer) and poller resumed clean StreamResults after the rollout. No TLS CN errors remain; only transient connection-refused events during the restart.","created_at":"2025-10-26T19:41:51Z"},{"id":108,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T19:50:46Z"},{"id":109,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T19:50:46Z"},{"id":110,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T20:12:52Z"},{"id":111,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T20:12:54Z"},{"id":112,"issue_id":"serviceradar-52","author":"mfreeman","text":"Rperf checker now supports SPIFFE: config schema grew a security.mode switch (mtls/spiffe), server reuses the workload API to mint SVIDs, and the guard stays alive so the source doesn’t close. Demo + Helm configs ship rperf-checker.json with SPIFFE metadata, agent init syncs it, Deployment mounts /run/spire/sockets, and a ClusterSPIFFEID/SA pair issues the identity. Built ghcr.io/carverauto/serviceradar-rperf-client@sha256:21e1242da450897677de5fe3f003300680a7decc6e243ea4c722467af28803e0, reapplied manifests, and rolled rperf + agent—poller logs confirm SPIFFE handshakes, and the kubectl logs show \"TLS configured with SPIFFE mode\" without reopening the source.","created_at":"2025-10-26T20:12:55Z"},{"id":113,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T20:23:56Z"},{"id":114,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T20:23:57Z"},{"id":115,"issue_id":"serviceradar-52","author":"mfreeman","text":"Reworked the agent’s external checker to rebuild SPIFFE credentials on reconnect instead of reusing a closed workload source; added a trapd-checker JSON that keeps trapd on legacy mTLS (and wired it into the init container + Helm) so the agent stops expecting an SVID. Rebuilt/pushed serviceradar-agent@sha256:65693235149f0157d293a0a45c645bdd9687f60920a378e1308cba407a21a381, applied the refreshed config, and rolled the demo agent—logs are clean now with no more ‘x509source is closed’ or trapd URI SAN warnings.","created_at":"2025-10-26T20:23:57Z"},{"id":116,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T20:28:23Z"},{"id":117,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T20:28:23Z"},{"id":118,"issue_id":"serviceradar-52","author":"mfreeman","text":"Followed up with a small agent tweak so the KV health check reuses the agent’s kv_security block, rebuilt/pushed serviceradar-agent@sha256:1d4fffecc105dab1eea85a1c759b56314b0a4c55358e955cd07e2cd0e572c616, and rolled the deployment—kv checker now succeeds while trapd stays on the mTLS fallback and the earlier SPIFFE source errors are gone.","created_at":"2025-10-26T20:28:23Z"},{"id":119,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T23:35:55Z"},{"id":120,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T23:35:55Z"},{"id":121,"issue_id":"serviceradar-52","author":"mfreeman","text":"Flowgger now advertises a gRPC health endpoint: added a small tonic-based server inside serviceradar-flowgger that reads the [grpc] block, launches alongside the collector, and serves standard health responses (mTLS required). Demo/Helm manifests gained a  (mtls) and the Service now exposes port 50044; the agent init container stages the new checker so poller health succeeds. Rebuilt/pushed ghcr.io/carverauto/serviceradar-flowgger@sha256:0db69e0ff5442e4c83603b62c1643fa948d30a6f1a3a66b8200393354abe0fce, applied the manifests, and rolled flowgger + agent—agent logs show the flowgger checker using the mtls override and staying healthy.","created_at":"2025-10-26T23:35:56Z"},{"id":122,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T23:41:59Z"},{"id":123,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T23:41:59Z"},{"id":124,"issue_id":"serviceradar-52","author":"mfreeman","text":"Flowgger gRPC health now online: added a tonic health server that reads the [grpc] block, launched it alongside the collector, and exposed port 50044 in both demo + Helm manifests. The agent ships a new flowgger-checker.json (mtls) so the poller can reuse its existing certs. Rebuilt/pushed ghcr.io/carverauto/serviceradar-flowgger@sha256:0db69e0ff5442e4c83603b62c1643fa948d30a6f1a3a66b8200393354abe0fce, applied manifests, and rolled flowgger + agent—agent logs show the flowgger checker staying healthy instead of refusing connections.","created_at":"2025-10-26T23:42:00Z"},{"id":125,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T00:30:34Z"},{"id":126,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T00:30:34Z"},{"id":127,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T00:41:23Z"},{"id":128,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T00:41:23Z"},{"id":129,"issue_id":"serviceradar-52","author":"mfreeman","text":"Flowgger gRPC server logs now go through env_logger so they include timestamps and levels instead of raw eprintln output. Added env_logger init in cmd/flowgger/src/main.rs and switched flowgger::grpc to log::info!/warn!/error!. Rebuilt ghcr.io/carverauto/serviceradar-flowgger@sha256:8957b6295e31f1d86e0a56ffeee75819d94a190fc2f55c7b0535a9dc545fe14f, rolled the demo deployment, and verified kubectl logs show timestamped INFO entries while agent health checks stay green.","created_at":"2025-10-27T00:41:24Z"},{"id":130,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T02:31:05Z"},{"id":131,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T03:39:42Z"},{"id":132,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T03:39:42Z"},{"id":133,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T03:51:38Z"},{"id":134,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T03:51:39Z"},{"id":135,"issue_id":"serviceradar-52","author":"mfreeman","text":"Zen SPIFFE cert was expiring because we only loaded the initial SVID and never watched for rotations. Added a reusable SPIFFE credential guard that keeps the X509 source alive, exposes a watch channel, and rebuilds TLS material on demand (cmd/consumers/zen/src/spiffe.rs). The gRPC server now runs inside a reload loop that waits for workload API updates, gracefully restarts with fresh Identity/CA, and logs rotations (cmd/consumers/zen/src/grpc_server.rs).\n\nRan cargo fmt -p serviceradar-zen \u0026\u0026 cargo test -p serviceradar-zen. Next step is to rebuild/push serviceradar-zen and roll the deployment so the agent can confirm health with a renewed SVID.","created_at":"2025-10-27T03:51:39Z"},{"id":136,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T04:00:45Z"},{"id":137,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T04:00:45Z"},{"id":138,"issue_id":"serviceradar-52","author":"mfreeman","text":"Rebuilt serviceradar-zen with the SPIFFE rotation loop and pushed ghcr.io/carverauto/serviceradar-zen@sha256:184c6a4a6512aa8afdf35ecc66224c72b4daf2670e41ca629666e8f8276d5c1d (tags: latest, sha-fabed9610800c20e82361e699c725328d9b3ed6f). Rolled deployment/serviceradar-zen in demo to that digest; rollout completed cleanly and agent logs have stopped emitting the expired-certificate errors.","created_at":"2025-10-27T04:00:46Z"},{"id":139,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T04:58:24Z"},{"id":140,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T04:58:24Z"},{"id":141,"issue_id":"serviceradar-52","author":"mfreeman","text":"Converted SNMP checker to SPIFFE (config + manifests + SPIRE entry), rewired agent config and rolled demo; added SPIFFE server credentials + tests to sysmon checker and switched sysmon-vm configs to SPIFFE placeholders. Rebuilt/pushed snmp checker image (sha256:edf9871805a3b30155f6ea17bd4429004b30fa06554fbc3be8896deb6e44ed24) and refreshed demo namespace (agent/snmp). Flowgger/trapd health warnings persist due to expired SVIDs.","created_at":"2025-10-27T04:58:25Z"},{"id":142,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T05:18:05Z"},{"id":143,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T05:18:05Z"},{"id":144,"issue_id":"serviceradar-52","author":"mfreeman","text":"Updated SNMP checker + agent configs to SPIFFE, added SPIRE entry and rolled demo; sysmon checker now loads SPIFFE credentials with new helper/tests (cargo fmt/check/test). Built/pushed snmp image (sha256:edf9871805a3b30155f6ea17bd4429004b30fa06554fbc3be8896deb6e44ed24), applied k8s base, and rolled agent/snmp pods. Flowgger/trapd still reporting expired SVIDs—needs follow-up.","created_at":"2025-10-27T05:18:07Z"},{"id":145,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T05:49:15Z"},{"id":146,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T05:49:16Z"},{"id":147,"issue_id":"serviceradar-52","author":"mfreeman","text":"Flowgger/trapd SPIFFE retries landed: both helpers now loop until an SVID is available, so the gRPC servers start even before SPIRE issues the first certificate. Rebuilt \u0026 pushed images (flowgger sha256:9fde817d782c55498aa831203b060feeda88443f021e4eaeb2536c3953c56d2c, trapd sha256:53cd8c4d92a707f38bd7093e2d4f2fd7cd9b663ebe73b073ae0c3ec751535ee3), reapplied demo base, and rolled flowgger/trapd. Agent logs show flowgger/trapd health checks passing after the rollout.","created_at":"2025-10-27T05:49:16Z"},{"id":148,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T05:57:18Z"},{"id":149,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T05:57:18Z"},{"id":150,"issue_id":"serviceradar-52","author":"mfreeman","text":"Spot-checked agent logs after the Flowgger/Trapd SPIFFE retry rollout (05:55Z onward) and the connection-refused errors have cleared; recent health sweeps now show only INFO requests for flowgger. No additional changes needed before continuing with the remaining SPIFFE migrations.","created_at":"2025-10-27T05:57:37Z"},{"id":151,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-27T06:24:07Z"},{"id":152,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-27T06:24:08Z"},{"id":153,"issue_id":"serviceradar-52","author":"mfreeman","text":"Ran make lint after the SPIFFE refactors; addressed new Go/Rust warnings (SPIFFE constants, derive(Default) for security enums, and flowgger path helper) and confirmed make test passes end-to-end.","created_at":"2025-10-27T06:24:08Z"},{"id":154,"issue_id":"serviceradar-52","author":"mfreeman","text":"SPIFFE roll-out is complete across services, lint/test suites are clean, and demo cluster is stable. Closing this bead.","created_at":"2025-10-27T07:16:33Z"}]}
{"id":"serviceradar-53","title":"Integrate nested SPIRE server into poller","description":"Goals:\\n- Embed a downstream SPIRE server/agent pair inside serviceradar-poller for edge deployments.\\n- Wire poller lifecycle to bootstrap the nested server and expose a workload socket for local services.\\n- Ensure documentation and packaging cover the nested topology and fallback modes.\\n\\nInitial tasks:\\n1. Inventory current poller SPIFFE usage and identify bootstrap requirements.\\n2. Prototype nested SPIRE server configuration (upstream auth, socket sharing) and determine runtime packaging changes.\\n3. Add observability/tests to validate certificate issuance when the poller is disconnected from the core.\\n4. Update demo/Helm manifests to optionally enable nested mode.","notes":"Opened external endpoints and rewired the edge workflow: datasvc and SPIRE now have LoadBalancer services (23.138.124.23:50057, 23.138.124.18:18081) and the compose helper updates poller/agent configs to use SPIFFE sockets. The controller config/Helm now set entryIDPrefix=k8s. so it ignores external entries, letting us mint a join-token-based downstream entry for the nested server. Local Docker stack hits Core/KV and shares /run/spire/nested with the agent, but the upstream SPIRE handshake still fails with PermissionDenied: no identity issued—manual entry survives, yet the join-token attestation never returns an SVID, so the poller falls back to mTLS. Need to debug the upstream Workload API path or automate token/entry creation so the nested agent can complete attestation and expose the workload socket.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-27T02:16:59.216991572-05:00","updated_at":"2025-10-28T15:18:01.301541473-05:00","closed_at":"2025-10-28T15:18:01.301541473-05:00","comments":[{"id":155,"issue_id":"serviceradar-53","author":"mfreeman","text":"Linked to GH-1894. Next steps:\\n- document current poller SPIFFE flow and identify upstream credentials it already carries.\\n- experiment with nested spire-server configuration inside the poller image (upstream_authority using poller agent socket).\\n- define rollout story for demo cluster and edge packaging, including fallback when nested mode disabled.\\nWill start by auditing poller repo layout and existing SPIRE assets.","created_at":"2025-10-27T07:17:05Z"},{"id":156,"issue_id":"serviceradar-53","author":"mfreeman","text":"Phase 1 / Step 3 prep:\\n- Poller already runs in SPIFFE mode for K8s/Helm (security.mode=\"spiffe\"), Docker Compose still defaults to mTLS. Poller image currently ships only the Go binary; tools image carries spire-server/agent bits.\\n- No downstream registration exists yet. We'll introduce a dedicated ClusterSPIFFEID for a nested poller server (downstream+admin) and gate it so only the sidecar container (planned name: poller-nested-spire) can claim the identity.\\n- Chart work: add .Values.spire.nestedPoller.{enabled,spiffeIDTemplate,containerName} to render the downstream ClusterSPIFFEID when enabled. Demo kustomization will ship it enabled by default.\\n- Docker Compose: add a helper (either CLI subcommand or script under docker/compose) that checks for the downstream entry via spire-server entry show and creates it with -downstream when missing.\\nNext actions: wire up the new ClusterSPIFFEID manifests + Helm templating, stage the compose helper, and ensure poller bootstrap only spawns nested SPIRE when config.mode==spiffe.","created_at":"2025-10-27T07:32:03Z"},{"id":157,"issue_id":"serviceradar-53","author":"mfreeman","text":"Added nested poller downstream entry scaffolding:\\n- Demo Kustomize now ships ClusterStaticEntry poller-nested-spire (downstream/admin) targeting the future sidecar container.\n- Helm chart grows optional nestedPoller block (disabled by default) to render the same static entry with configurable selectors/TTLs.\n- Drafted docker/compose/bootstrap-nested-spire.sh to idempotently create the downstream entry via spire-server CLI, with overrides for non-Kubernetes selectors.\nNext: wire the poller deployment to host the nested spire-server/agent and respect security.mode before launching them.","created_at":"2025-10-27T08:05:06Z"},{"id":158,"issue_id":"serviceradar-53","author":"mfreeman","text":"K8s demo + Helm now stage nested SPIRE infrastructure:\\n- Added poller-nested-spire ConfigMap (server + upstream/downstream agent configs) and wired new sidecars/volumes into serviceradar-poller Deployment with projected tokens + shared socket volume.\\n- Helm chart gains conditional nestedPoller block (configmap, containers, volumes) and root spire-server allow list now trusts serviceradar-poller when enabled.\\nNext: ship spire binaries/scripts with poller image and teach Docker Compose to bootstrap the downstream entry + optional sidecars.","created_at":"2025-10-27T08:21:41Z"},{"id":159,"issue_id":"serviceradar-53","author":"mfreeman","text":"Poller image now bundles SPIRE binaries + entrypoint wrapper:\\n- poller layer carries spire-server/agent and a bootstrap helper, with new entrypoint that only spins up embedded SPIRE components when MANAGE_NESTED_SPIRE=enabled.\\n- K8s/Helm manifests set MANAGE_NESTED_SPIRE=disabled so the new entrypoint defers to the dedicated sidecars.\\n- Docker compose configuration path honors POLLERS_SECURITY_MODE (mtls|spiffe) so config-updater can emit SPIFFE-aware poller.json via poller.spiffe.json when requested.]}]}","created_at":"2025-10-27T15:26:43Z"},{"id":160,"issue_id":"serviceradar-53","author":"mfreeman","text":"K8s/Helm now deploy nested SPIRE and poller image ships the binaries + entrypoint:\\n- Demo manifests include poller-nested-spire-config, the downstream static entry, and poller deployment mounts  (MANAGE_NESTED_SPIRE=disabled) while spire-server trusts the poller SA.\\n- Helm values templated the same story; nestedPoller.* drives the config map + ClusterStaticEntry and renders the sidecars behind the feature flag.\\n- Poller OCI image layer bundles spire-agent/server, bootstrap-nested-spire, and a poller-entrypoint that launches embedded components only when MANAGE_NESTED_SPIRE=enabled.\\n- Docker compose: config-updater emits poller.spiffe.json when POLLERS_SECURITY_MODE=spiffe, poller entrypoint/manifests wired to the new script, and compose env exposes the mode toggle.\\nTODO: design/manage join_token/x509pop path for non-K8s installs and add docs/compose override walking through the SPIFFE bootstrap.","created_at":"2025-10-27T15:30:46Z"},{"id":161,"issue_id":"serviceradar-53","author":"mfreeman","text":"K8s/Helm now deploy nested SPIRE and poller image ships the binaries + entrypoint:\n- Demo manifests include poller-nested-spire-config, the downstream static entry, and poller deployment mounts /run/spire/nested (MANAGE_NESTED_SPIRE=disabled) while spire-server trusts the poller SA.\n- Helm values templated the same story; nestedPoller.* drives the config map + ClusterStaticEntry and renders the sidecars behind the feature flag.\n- Poller OCI image layer bundles spire-agent/server, bootstrap-nested-spire, and a poller-entrypoint that launches embedded components only when MANAGE_NESTED_SPIRE=enabled.\n- Docker compose: config-updater emits poller.spiffe.json when POLLERS_SECURITY_MODE=spiffe, poller entrypoint/manifests wired to the new script, and compose env exposes the mode toggle.\nTODO: design/manage join_token/x509pop path for non-K8s installs and add docs/compose override walking through the SPIFFE bootstrap.","created_at":"2025-10-27T15:31:38Z"},{"id":162,"issue_id":"serviceradar-53","author":"mfreeman","text":"K8s + Helm now support an optional nested SPIRE stack for the poller: new config map, downstream static entry, and sidecar wiring landed under k8s/demo/base/* with chart parity behind spire.nestedPoller.enabled. The poller image bundles spire-agent/server plus bootstrap helpers and a MANAGE_NESTED_SPIRE entrypoint gate, while Docker Compose switches between mTLS and SPIFFE via POLLERS_SECURITY_MODE and emits poller.spiffe.json with a bootstrap script. Next up: document and implement non-Kubernetes attestation (join token/x509pop) for Compose/bare metal, exercise the new entrypoint end-to-end, and add docs/overrides for enabling SPIFFE mode.","created_at":"2025-10-27T15:32:41Z"},{"id":163,"issue_id":"serviceradar-53","author":"mfreeman","text":"Compose path now renders nested SPIRE configs and entrypoint boots the embedded server via join tokens. Added config-updater wiring, override file, and Docker setup docs. make lint \u0026\u0026 make test pass.","created_at":"2025-10-27T15:43:34Z"},{"id":164,"issue_id":"serviceradar-53","author":"mfreeman","text":"Pushed poller image (sha256:a1ae5799aa9e14930e031a6af2a763be3bd6ef08aa57fc27f6d9ec2f3416af45) and applied the demo manifests. New deployment spins up the nested SPIRE sidecars, but the upstream agent is stuck re-attesting: SPIRE denies GetAuthorizedEntries because the ClusterStaticEntry parent ID doesn’t match the agent SVID (k8s_psat embeds the node UID). Result is CrashLoopBackOff for poller-nested-upstream-agent and the downstream server never gets a bundle. Need to either generate downstream entries per agent (controller/job) or switch the upstream node attestor to a join token/x509pop flow where the parent ID is predictable.","created_at":"2025-10-27T16:22:13Z"},{"id":165,"issue_id":"serviceradar-53","author":"mfreeman","text":"Tried to roll the new poller image into demo; nested SPIRE sidecars come up but upstream agent is denied GetAuthorizedEntries because the static downstream entry still hangs off spiffe://carverauto.dev/spire/server. The k8s_psat agent SVID includes the node UID, so SPIRE never matches and the poller stays in CrashLoopBackOff. Need to either switch the nested agent to join_token/x509pop or generate per-agent downstream entries. Until we choose, deployment remains red.","created_at":"2025-10-27T17:23:39Z"},{"id":166,"issue_id":"serviceradar-53","author":"mfreeman","text":"Documented the join-token bootstrap plan in docs/docs/spire-onboarding-plan.md (section 9). Research confirms k8s_psat cannot drive static downstream entries; plan is to mint deterministic join-token parents via serviceradar-cli, ship a Kubernetes Job that writes the token Secret, and reuse the same CLI for Compose/bare-metal. Pending: implement CLI + Job + manifest updates and keep edge auth questions open.","created_at":"2025-10-27T19:04:25Z"},{"id":167,"issue_id":"serviceradar-53","author":"mfreeman","text":"Outlined the admin onboarding flow in docs/docs/spire-onboarding-plan.md: new POST /api/admin/spire/join-tokens endpoint (RBAC enforced), core talks to SPIRE via join-token + entry RPCs using an admin SVID, audits issuances, and exposes a serviceradar-cli helper. Also captured validation, rate-limiting, and CLI integration requirements. Next up: wire core config for SPIRE admin access, add the HTTP handler/service layer, and teach the CLI to call it.","created_at":"2025-10-27T19:07:21Z"},{"id":168,"issue_id":"serviceradar-53","author":"mfreeman","text":"Implemented the core-side SPIRE admin flow: server.conf now enables the join_token attestor, serviceradar-core’s SPIRE SVID is marked admin, and core loads a new spire_admin block that instantiates pkg/spireadmin. Added POST /api/admin/spire/join-tokens with request validation + downstream entry support, and wired RBAC/Swagger updates. Added CLI subcommand 'serviceradar spire-join-token' (API-key/Bearer auth, selectors, TTL, JSON output) plus docs on using it for Docker Compose. go test ./... passes and vendor contains spire-api-sdk v1.5.3.","created_at":"2025-10-27T19:33:50Z"},{"id":169,"issue_id":"serviceradar-53","author":"mfreeman","text":"Finished the SPIRE admin integration: added spire_admin config/validation defaults, SPIRE server now enables join_token attestor and core SVID is admin, new pkg/spireadmin client dials workload API and server Agent/Entry RPCs, core exposes POST /api/admin/spire/join-tokens with downstream support + JSON helper, CLI gained 'serviceradar spire-join-token' with API/Bearer auth, selectors, TLS options, and JSON export. Docs updated (docker-setup, onboarding plan) to prefer the new command, added join-token values to Helm/k8s manifests, vendor/go.mod now include spire-api-sdk v1.5.3. Tests: go test ./...","created_at":"2025-10-27T19:47:11Z"},{"id":170,"issue_id":"serviceradar-53","author":"mfreeman","text":"Poller still looping: downstream agent reports dial tcp 127.0.0.1:8083 connection refused because the embedded server never finishes bootstrap. Server sidecar logs show UpstreamAuthority can't read from /run/spire/nested/upstream/agent.sock (connection reset), and the upstream agent is denied GetAuthorizedEntries (parent mismatch). We still need to switch the upstream agent to join_token (or create per-agent downstream entries) before the nested stack will stabilize.","created_at":"2025-10-27T20:01:54Z"},{"id":171,"issue_id":"serviceradar-53","author":"mfreeman","text":"Refined nested SPIRE bootstrap: init job now logs into core, calls the new /api/admin/spire/join-tokens endpoint, and writes the token for the upstream agent (no more API key). Built/pushed serviceradar-core image sha256:1351b3a75fa5a5eb56c5cc5994d717ba803853a337d1b55f42281e94a7cf4c04 and rolled the deployment. Join-token RPC still fails with PermissionDenied from SPIRE (core’s SVID is admin=true but Authorization denies /spire.api.server.agent.v1.Agent/CreateJoinToken), so the poller init container keeps crashing; need to debug SPIRE auth or adjust registration entries before nested agent can come up.","created_at":"2025-10-27T20:35:41Z"},{"id":172,"issue_id":"serviceradar-53","author":"mfreeman","text":"Implemented core-side join-token flow (new pkg/spireadmin client, API handler, CLI call) and updated poller manifests/Helm so the init container logs into core and requests tokens instead of relying on X-API-Key. Built/pushed core image sha256:1351b3a75fa5a5eb56c5cc5994d717ba803853a337d1b55f42281e94a7cf4c04 and rolled the deployment. Poller still crash-loops because SPIRE rejects CreateJoinToken (PermissionDenied) despite core’s SVID being admin=true—need to debug SPIRE authorization next.","created_at":"2025-10-27T20:37:33Z"},{"id":173,"issue_id":"serviceradar-53","author":"mfreeman","text":"Swapped spireadmin client to MTLS (core now presents its SVID), added admin_ids to the demo + Helm SPIRE server configs, and rebuilt/pushed core image ghcr.io/carverauto/serviceradar-core@sha256:c83cea8ba2ce3ac8f095a312b5e590cfd8ecc300eb11d16a87685136c674b549 before rolling the deployment. \\n\\nPoller bootstrap now copies the SPIRE config into a writable overlay, injects the join token, and runs the upstream agent without a shell (templates/poller.yaml + base manifest + compose script updated). Join token init container runs cleanly and the upstream agent attests. \\n\\nRemaining issue: the nested SPIRE server still fails to watch the upstream Workload API (agent logs warn \"could not resolve caller information\"); need to chase selectors/SA wiring next.","created_at":"2025-10-27T21:18:07Z"},{"id":174,"issue_id":"serviceradar-53","author":"mfreeman","text":"Core-to-SPIRE admin client now uses mTLS (pkg/spireadmin/client.go) and the demo/Helm/server configs list serviceradar-core in admin_ids, so /api/admin/spire/join-tokens returns 201. Rebuilt/pushed ghcr.io/carverauto/serviceradar-core@sha256:c83cea8ba2ce3ac8f095a312b5e590cfd8ecc300eb11d16a87685136c674b549 and rolled the deployment to pick it up.\\n\\nPoller init now copies the SPIRE configs into a writable overlay, injects the join token, and starts the upstream agent directly (no shell entrypoint). Updated k8s demo manifests, Helm templates, and compose scripts accordingly; fresh rollout shows join token creation + upstream agent attestation succeeding.\\n\\nRemaining issue: nested SPIRE server still can’t watch the upstream Workload API (poller-nested-spire logs: connection reset, \"could not resolve caller information\"). Need to debug selectors/token setup so the downstream server can fetch CA material.","created_at":"2025-10-27T21:35:42Z"},{"id":175,"issue_id":"serviceradar-53","author":"mfreeman","text":"Refactored the poller bootstrap: init container now only requests the join token, manifests/Helm gained a shareProcessNamespace flag, extra RBAC for the poller service account, and a poller-downstream-register sidecar that logs in to core and loops on /api/admin/spire/join-tokens to create the downstream entry (unix selectors instead of k8s). Depoyed the updated image + manifest; sidecar waits on /run/spire/nested and reports successful registration. However, the upstream agent still fails to issue an SVID (No","created_at":"2025-10-27T22:06:11Z"},{"id":176,"issue_id":"serviceradar-53","author":"mfreeman","text":"Refactored the poller bootstrap: init container now only requests the join token, manifests/Helm gained shareProcessNamespace plus extra RBAC for the poller service account, and a poller-downstream-register sidecar logs in to core and loops on /api/admin/spire/join-tokens to create the downstream entry (unix selectors instead of k8s). Deployed the updated image + manifest; sidecar waits on /run/spire/nested and reports successful registration. However, the upstream agent still can’t issue an SVID (“No identity issued”), and the IDs returned by core’s API don’t appear in `spire-server entry show` (they only expose a spiffe_id selector). Manually creating a downstream entry with unix selectors via `spire-server entry create` didn’t help either. Next step is to inspect what selectors the upstream agent observes (debug log?) or adjust the workload attestor so the nested server is recognized.","created_at":"2025-10-27T22:06:36Z"},{"id":177,"issue_id":"serviceradar-53","author":"mfreeman","text":"Progress update 1) Added gating around join-token/bootstrap sidecars (env toggles + defaults disabled) and introduced ClusterSPIFFEID for the nested poller server so the controller can manage that identity. 2) Reapplied manifests + rolled the deployment: upstream agent now advertises detailed selector debug output, sidecar confirms join tokens are created on demand. Remaining issues a) SPIRE controller immediately garbage-collects the downstream entries it creates (logs show ), so the nested server keeps getting  when watching the workload API. b) Without a stable downstream entry, poller pods flip between CrashLoopBackOff and Running as the sidecar retries registration. Next steps: decide whether to teach the controller about the downstream entry (e.g., ClusterStaticEntry with class filter) or manage the entry entirely outside the controller (disable registration loop + create static entry via CLI/CR).","created_at":"2025-10-27T22:31:46Z"},{"id":178,"issue_id":"serviceradar-53","author":"mfreeman","text":"Progress update:\n1. Added env toggles so the join-token and downstream registration sidecars can be disabled/enabled per deployment, and created a ClusterSPIFFEID for the nested poller server so the controller manages that identity. Reapplied manifests and rolled the poller; the upstream agent now logs detailed selector debug output and the registration sidecar shows the join-token path working.\n2. Still blocked because the SPIRE controller immediately deletes the downstream entries it creates (controller log: \"entry-reconciler Deleted entry … admin=true, downstream=true\"), so the nested server keeps getting `PermissionDenied: no identity issued` while watching the Workload API. Without a stable downstream entry, the poller pods bounce between CrashLoopBackOff and Running as the sidecar retries.\n\nPossible next steps:\n- Teach the controller about the nested downstream entry (e.g., via a ClusterStaticEntry with a dedicated class) so it stops pruning the unix-selector entry, or\n- Disable the automated registration loop and manage the downstream entry entirely outside the controller (static entry via CLI/CR, or a one-shot Job).\n- Revisit whether the nested server should authenticate via join-token at all or stay on k8s_psat with a per-pod ClusterSPIFFEID, depending on the path we pick above.\n","created_at":"2025-10-27T22:32:15Z"},{"id":179,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Reproduced the edge stack bootstrap: upstream agent now attests via the LB and the nested server activates once the downstream entry selectors drop the unix:path requirement.\n- Traced the downstream agent failure to the entrypoint script: it was calling 'spire-server token generate' with unsupported -parentID/-downstream flags and a non-numeric TTL (4h). The command never returned a token, so the downstream agent launched without credentials and the workload socket never appeared.\n- Verified the selectors emitted by the embedded agent (uid/gid/user/group only) and generated a fresh upstream token + downstream entry using unix:group:root instead of unix:path. After minting a downstream join token manually the workload socket came up and both poller/agent point at the external Core/KV endpoints.\n- Updated docs/docker-setup.md with the corrected upstream token workflow (no -downstream flag, numeric TTL, unix:group selector) and reworked poller-entrypoint.sh so it normalises TTL strings, calls token generate with the supported flags, and masks parsing failures with an explicit warning.\n- TODO: rebuild the poller image to pick up the entrypoint changes and retest the compose flow end-to-end (the running container still has the old script).\n","created_at":"2025-10-28T02:12:33Z"},{"id":180,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Rebuilt the poller image with the new entrypoint enhancements and retagged it locally (`bazel run --config=remote //docker/images:poller_image_amd64_tar`).\n- Reset the edge compose stack: wiped the volumes, minted fresh upstream tokens/entries, and reran `setup-edge-poller.sh` with the LoadBalancer endpoints.\n- Verified the bootstrap flow: the entrypoint now logs `Ensuring workload entry …` and automatically seeds poller/agent registration entries after the downstream token is generated. The downstream agent exposes `/run/spire/nested/{server,workload}/agent.sock` without manual intervention.\n- Poller still can’t fetch an SVID (`PermissionDenied: no identity issued`) even though the nested server mints the workload entries; manual `spire-server entry create` works, so the automated path needs follow-up. Core/KV reachability remains blocked on SPIFFE adoption.\n","created_at":"2025-10-28T04:04:31Z"},{"id":181,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Root-caused the Docker PermissionDenied loops: the workload entries were using unix:path selectors, which fail for cross-container workloads. Defaulted the poller entrypoint to unix:{uid,gid,user,group} and rebuilt the poller image (bazel run --config=remote //docker/images:poller_image_amd64_tar, retagged as latest).","created_at":"2025-10-28T04:21:49Z"},{"id":182,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Reset the edge stack (docker compose --env-file edge-poller.env -f docker/compose/poller-stack.compose.yml down -v / up -d config-updater, reran setup-edge-poller.sh with CORE/KV addresses) and minted a fresh upstream token/entry. Once the nested server had new poller/agent workload entries without the path selector, docker logs serviceradar-poller confirmed SPIFFE gRPC dials against 23.138.124.18:50052.","created_at":"2025-10-28T04:21:57Z"},{"id":183,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Follow-up: the helper still isn't logging 'Created workload entry…'; manual spire-server entry create calls succeed, so we should double-check ensure_workload_entry to see why it skips creation during bootstrap.","created_at":"2025-10-28T04:22:02Z"},{"id":184,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Patched ensure_workload_entry to update existing records (parsing the entry ID via awk) so controller-created entries receive our unix selectors. Logs now show 'Updated workload entry…' / 'Created workload entry…', and the nested poller switches to SPIFFE without manual CLI intervention.","created_at":"2025-10-28T04:37:54Z"},{"id":185,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Rebuilt the poller image and reran the edge stack end-to-end: generated fresh upstream credentials, ran setup-edge-poller.sh, and confirmed the poller/agent both fetch SVIDs (no more PermissionDenied after bootstrap).","created_at":"2025-10-28T04:38:02Z"},{"id":186,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Confirmed core external IP remains 23.138.124.18 (serviceradar-core-grpc-external) and the regenerated poller config points at 23.138.124.18:50052.","created_at":"2025-10-28T05:46:12Z"},{"id":187,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Simplified docker/compose/poller.spiffe.json so the edge poller only runs the serviceradar-agent process check; reran setup-edge-poller.sh (with the updated optional KV handling) to regenerate the config volume and restarted the stack.","created_at":"2025-10-28T05:46:29Z"},{"id":188,"issue_id":"serviceradar-53","author":"mfreeman","text":"- Verified the Core LB we should target: `kubectl get svc -n demo | grep serviceradar-core` shows `serviceradar-core-grpc-external` on `23.138.124.18:50052`. The regenerated poller config reflects that (`docker exec serviceradar-poller jq -r '.core_address' /etc/serviceradar/config/poller.json` → `23.138.124.18:50052`), and the poller logs now emit `\"Connecting to core service\"` against that address during each cycle.\n- Trimmed the edge template (`docker/compose/poller.spiffe.json`) so the docker poller only carries the `serviceradar-agent` process check. After rerunning `setup-edge-poller.sh` and restarting the stack, `jq '.agents[\"docker-agent\"].checks'` inside the poller container confirms the single-check payload and the logs show only the agent poller executing.\n- Taught `setup-edge-poller.sh` to treat `KV_ADDRESS` as optional and to drop `kv_address/kv_security` when it is omitted. Regenerating the volume without that env var leaves `agent.json` SPIFFE-only; on restart the agent logs `KVAddress not set, skipping KV store setup`, brings up gRPC on `:50051`, and the previous join-token crash loop disappears. `netstat -plnt` inside the agent shows the listener bound to `:::50051`.\n- Poller ↔ agent SPIFFE handshake looks good: the poller now reports a healthy agent, streams sweep results, and no longer spams connection-refused warnings. The only remaining log noise is (a) the nested SPIRE server occasionally warning `Failed to watch the Workload API` while waiting on the upstream socket, and (b) the agent's optional sweep watcher complaining about the missing KV resolver (`name resolver error: produced zero addresses`); both are transient and do not impact the docker validation run.\n","created_at":"2025-10-28T06:49:10Z"},{"id":189,"issue_id":"serviceradar-53","author":"mfreeman","text":"Reproduced the bootstrap failure from this morning: the upstream agent was still pointing at the old bundle and the only surviving registration entries pointed at expired join tokens, so the nested server could never fetch a downstream CA and kept the Workload API socket offline. Refreshed the upstream trust bundle with `kubectl exec spire-server-0 -- spire-server bundle show -format pem \u003e docker/compose/spire/upstream-bundle.pem`, minted a fresh join token (`02c8246f-e756-4c41-8ea8-6da3f2be4c16`), and created the matching `-downstream` entry on the demo cluster (`spire-server entry create ... -downstream -admin`). After clearing the compose runtime volume and restarting the stack, the entrypoint now logs the full happy path:\n\n- `Nested SPIRE server is ready at /run/spire/nested/server/api.sock`\n- `Generated downstream join token ...` / workload entries for poller + agent\n- Downstream agent socket appears at `/run/spire/nested/workload/agent.sock`\n- Poller dials Core over SPIFFE (`23.138.124.18:50052`) and the docker agent comes up in SPIFFE mode\n\nOpen follow-ups:\n1. Entry creation remains manual. We should script the `spire-server token generate`/`entry create -downstream` flow (or teach the controller) so every new join token is paired automatically.\n2. The entrypoint always appends `-joinToken` even when the upstream data dir already has an SVID; on restart it will try to reuse a consumed token and crash. Need a guard that skips the flag when `/run/spire/nested/upstream-agent/*.pem` already exists.\n\nLeaving the stack running with the freshly issued credentials so we can keep iterating.\n","created_at":"2025-10-28T07:19:16Z"},{"id":190,"issue_id":"serviceradar-53","author":"mfreeman","text":"Added the edge poller to Core's allowlist so the demo cluster recognizes the Docker instance:\n- Updated k8s demo ConfigMap (`k8s/demo/base/configmap.yaml`) to include both `k8s-poller` and `docker-poller` in `known_pollers`.\n- Mirrored the change in the bootstrap helper (`k8s/demo/deploy.sh`) and Helm default config (`helm/serviceradar/files/serviceradar-config.yaml`) so future applies stay consistent.\n\nNext apply will require a config map sync + `kubectl rollout restart deploy/serviceradar-core -n demo` to pick up the new poller ID.\n","created_at":"2025-10-28T07:21:12Z"},{"id":191,"issue_id":"serviceradar-53","author":"mfreeman","text":"Hit a new blocker while watching the edge poller run: Core is rejecting status writes because the ClickHouse disk on the demo cluster is past the 90% utilization guard (`Disk default utilization is 0.90029 \u003e max_disk_util 0.9`). Logs from serviceradar-core confirm the failure when k8s-poller reports in.\n\nParking the nested SPIRE work here; next action is to free or expand storage on the Proton PVC before we can validate additional state changes.\n","created_at":"2025-10-28T07:32:36Z"},{"id":192,"issue_id":"serviceradar-53","author":"mfreeman","text":"Cleared the Proton disk alarm:\n- Scaled `serviceradar-proton` down, mounted the `serviceradar-proton-data` PVC via an on-shot alpine pod, and found \u003e200 Gi sitting under `/var/lib/proton/nativelog/log/default/*` (biggest folder: `59729f44-38bc-4f17-99ee-5f6297925d1a.0`).\n- Removed the four oversized UUID directories, dropped the helper pod, and brought Proton back up; `/var/lib/proton/nativelog/log` is now \u003c500 MiB and `df -h` on the pod shows 18 % usage (was 90 %).\n- Core logs are back to normal; the earlier `max_disk_util` error is gone.\n\nFollow-up: apply the configmap/Helm changes so Core stops flagging `docker-poller` as “unknown” (`kubectl apply -k k8s/demo/prod \u0026\u0026 kubectl rollout restart deploy/serviceradar-core -n demo`).\n","created_at":"2025-10-28T07:41:51Z"},{"id":193,"issue_id":"serviceradar-53","author":"mfreeman","text":"Applied the updated config so Core recognizes the Docker poller:\n- `kubectl apply -f k8s/demo/base/configmap.yaml`\n- `kubectl rollout restart deploy/serviceradar-core -n demo`\n\nAfter the restart, Core logs show normal status processing with no “Ignoring status report from unknown poller” warnings. Both k8s and docker pollers are now accepted.\n","created_at":"2025-10-28T07:46:13Z"},{"id":194,"issue_id":"serviceradar-53","author":"mfreeman","text":"Entry-point hardens upstream restarts: updated `docker/compose/entrypoint-poller.sh` so we only append `-joinToken` when the upstream agent doesn’t already have a cached SVID. If `agent-data.json` still holds a valid SVID, the script now logs the reuse and skips the flag; if no token and no SVID are present we emit a warning so operators know the bootstrap will fail. This prevents the “join token already used” crash when the edge poller restarts with a warm data dir.\n","created_at":"2025-10-28T07:53:16Z"},{"id":195,"issue_id":"serviceradar-53","author":"mfreeman","text":"Refined the upstream join flow and verified clean restart:\n- Updated the entrypoint guard so we only skip `-joinToken` when the cached upstream agent state is explicitly marked `\"reattestable\": true`. Join-token attestations remain non-reattestable, so we now keep sending a fresh token instead of silently skipping it and leaving the nested server without an SVID (which produced the repeated `dial unix ... connection refused` errors).\n- Minted a new upstream token (`d12b4ec8-d56d-48de-94ca-c9b68348cea6`), created the matching downstream entry, rebuilt the poller image, and restarted the Docker stack after wiping the runtime volume. The startup sequence now shows a successful attestation followed by the Core connection line at 08:48:09.\n\nWe still need better automation around token refresh (and to stop checking the token file into git), but the edge poller now comes up clean again.\n","created_at":"2025-10-28T08:48:49Z"},{"id":196,"issue_id":"serviceradar-53","author":"mfreeman","text":"Wrapped up the restart failure investigation:\n- Added a reattestation-aware guard in `docker/compose/entrypoint-poller.sh`: we now skip `-joinToken` only when the cached upstream agent data reports `\"reattestable\": true`. Join-token attestations are non-reattestable, so the poller always re-sends the token on restart instead of silently reusing a spent value and crashing the nested server.\n- Minted a fresh upstream token (`d12b4ec8-d56d-48de-94ca-c9b68348cea6`), paired it with a new downstream entry, rebuilt the poller image, wiped `/run/spire/nested`, and brought the Docker stack back up. Logs show a clean `Node attestation was successful` followed by the expected Core connection at 08:48 UTC; the earlier `dial unix … upstream/agent.sock: connect: connection refused` loop is gone.\n- Captured the remaining TODO: we still need to automate token issuance and stop checking `docker/compose/spire/upstream-join-token` into the repo so new edge runs can bootstrap without manual prep.\n","created_at":"2025-10-28T09:02:10Z"},{"id":197,"issue_id":"serviceradar-53","author":"mfreeman","text":"Updated the demo core config so the edge poller is trusted and automated the join-token flow.\\n\\n- Added docker-poller to known_pollers in core.json and applied the refreshed ConfigMap so core stops rejecting its status updates.\\n- Introduced docker/compose/refresh-upstream-credentials.sh to mint a join token, rebuild the downstream entry, and download the trust bundle in one step; the helper now writes to docker/compose/spire/ without keeping secrets in git.\\n- Ran the helper to rotate the upstream credentials (current token: 2e650a77-7623-4dde-8a49-8f70f842677f) and confirmed the artifacts landed in docker/compose/spire/.\\n- Updated docs/docker-setup.md to reference the new helper for edge runs.","created_at":"2025-10-28T09:13:41Z"},{"id":198,"issue_id":"serviceradar-53","author":"mfreeman","text":"Edge docker stack now runs cleanly in SPIFFE mode.\\n\\n- Rebuilt the config volume targeting the actual Compose volume (compose_poller-generated-config) after clearing the nested SPIRE runtime; trimming the unused poller-generated-config volume removed the stale mTLS config.\\n- Regenerated upstream credentials with refresh-upstream-credentials.sh (token 9999c830-0445-45d8-9f2f-b697a262823f) and reran setup-edge-poller.sh so poller.json/agent.json point at 23.138.124.18 and the nested SPIRE templates match.\\n- Updated the helper to stop passing -storeSVID; the downstream entry now avoids the SVIDStore-not-found spam, and the entrypoint successfully seeds the workload entries (poller entry 79a60400-16fd-4bea-8a2e-f87faad263be).\\n- Restarted the compose stack; upstream attestation succeeded at 09:31:16Z and the downstream agent followed at 09:31:31Z. Poller logs show \"mode\":\"spiffe\" and Grpc security comes from the workload socket.\\n- Restarted serviceradar-core in demo so the refreshed ConfigMap takes effect; poller status reports are now accepted instead of flagged as unknown (core log 09:34:08Z confirms the first-seen event).","created_at":"2025-10-28T09:34:42Z"},{"id":199,"issue_id":"serviceradar-53","author":"mfreeman","text":"Added an orchestrator so edge resets are one command.\\n\\n- New docker/compose/edge-poller-restart.sh wraps the full cycle (compose down, volume prune, config-updater, join-token refresh, setup-edge-poller, stack restart).  and  cover quick iterations.\\n- setup-edge-poller.sh now prunes the sample sweep/sysmon checker configs whenever KV is disabled, so the Docker agent stops trying to reach non-existent SPIFFE services.\\n- Docs updated to call out the restart script and explain that the helper omits those checkers in edge mode.","created_at":"2025-10-28T18:33:51Z"},{"id":200,"issue_id":"serviceradar-53","author":"mfreeman","text":"Follow-up: the restart helper supports optional flags --skip-refresh (reuse the existing join token) and --dry-run (print the steps). Use `docker/compose/edge-poller-restart.sh --help` for the full list.","created_at":"2025-10-28T18:34:07Z"},{"id":201,"issue_id":"serviceradar-53","author":"mfreeman","text":"Edge stack restart flow is scripted and validated.\\n\\n- Added docker/compose/edge-poller-restart.sh to orchestrate volume cleanup, config regeneration, upstream credential refresh, and compose restarts; dry-run and skip-refresh flags work as advertised.\\n- Verified the helper end-to-end: upstream token 39bbdd2f-cacd-4012-825b-36546747d365 minted via kubectl exec, nested SPIRE attested (18:45:48Z), downstream workload entries issued, poller/agent now run in SPIFFE mode with only the local agent checker enabled.\\n- Core confirms status ingestion (see demo logs ~18:46Z). Remaining gap is secure self-service onboarding—we still rely on kube credentials to mint tokens. Tracking that in a new GitHub issue next.","created_at":"2025-10-28T20:17:55Z"}]}
{"id":"serviceradar-54","title":"Secure edge poller onboarding flow","description":"Track design/implementation for Core-managed edge poller enrollment. See https://github.com/carverauto/serviceradar/issues/1903 for the PRD and task list.","design":"## Proposed Architecture\n- Persist onboarding artifacts in new Proton tables so Core can track issuance, download, activation, and revocation:\n  - `edge_onboarding_packages` (ReplacingMergeTree, no TTL) keyed by `package_id` (UUID). Columns include human label/site, `poller_id`, derived downstream SPIFFE ID, encrypted join token blob, join token expiry, downstream entry id, package status enum, `download_token_hash`, download expiry, metadata JSON, `created_by`, timestamps (`created_at`, `delivered_at`, `activated_at`, `revoked_at`), and optional activation context (source IP, last seen SVID).\n  - `edge_onboarding_events` (MergeTree) for audit trail with columns: `package_id`, `event_type` (issued/downloaded/activated/revoked/expired), actor, source IP, message JSON, timestamp. No TTL so security reviewers can query history.\n\n- Extend `models.CoreServiceConfig` with `edge_onboarding` block: `enabled` flag, default downstream selectors, SPIFFE path template (e.g. `spiffe://{trust_domain}/ns/edge/{poller_id}`), default token/join TTLs, download TTL, optional default poller id prefix, and `encryption_key_base64` used to wrap join tokens + bundles. Validation ensures key decodes to 32 bytes (AES-256-GCM).\n\n- Introduce `pkg/crypto/secrets` helper that wraps AES-GCM with per-record random nonce + HMAC for integrity. Provide `Encrypt([]byte)` / `Decrypt([]byte)` returning base64 strings so we can persist ciphertext in Proton. Key supplied from config and cached in Core.\n\n- Build a dedicated onboarding service inside Core (`pkg/core/edgeonboarding`) that owns package lifecycle:\n  - `CreatePackage(ctx, req)` uses `spireadmin.Client` to mint join token (`agentv1.CreateJoinToken`), create downstream entry with selectors, and fetch the upstream x509 bundle via `bundle/v1.BundleClient` (X.509 Roots). Token + bundle stored encrypted.\n  - Generate a random `download_token` (24 bytes) saved as SHA-256 hash; plaintext returned once in the POST response.\n  - Persist package + `issued` event in DB. Service returns metadata plus download URL template.\n  - `RecordDownload` flips status to `delivered`, timestamps the event, and logs actor/IP.\n  - `MarkActivated` marks package active when Core sees a status report from the poller (matching poller ID or downstream SPIFFE ID). Activation event stores poller source IP + SVID (if presented).\n  - `RevokePackage` deletes downstream entry via `entryv1.BatchDeleteEntry` and marks status revoked. If token still valid, we simply invalidate future downloads and update status to `revoked`.\n  - `ExpirePackages` cron: background goroutine sweeps for expired tokens/download tokens and updates status to `expired`, revoking downstream entries when appropriate.\n\n- Modify `db.Service` with CRUD helpers: create/list/update packages, append audit events, query active poller IDs, mark activation. Implementations live in new files under `pkg/db/edge_onboarding.go` using Proton SQL with `INSERT`/`ALTER TABLE ... UPDATE` semantics.\n\n- Update Core startup to hydrate a runtime cache of active pollers from DB and hand it to the API layer and `Server.isKnownPoller`. Replace the hard-coded list check with `config.KnownPollers ∪ activePackages`. Provide watch method (`EdgeOnboardingService.Subscribe`) so new activations push into the cache without restart.\n\n- API surface:\n  - `POST /api/admin/edge-packages` (admin role) accepts JSON `{label, poller_id?, site?, ttl_seconds?, download_ttl_seconds?, selectors?, metadata}`. Validates uniqueness, derives poller ID if absent (e.g. slugified label with prefix), and returns `201` with metadata + `download_token` (shown once) and poller config preview (core address, kv address, etc.).\n  - `GET /api/admin/edge-packages` lists packages with filters (status, created_by, date range, poller id) and aggregates event history for UI chips.\n  - `GET /api/admin/edge-packages/{id}` provides detail including audit log (without revealing token).\n  - `GET /api/admin/edge-packages/{id}/download?token=...` streams a tar.gz assembled on-demand: `README.txt`, `metadata.json`, `edge-poller.env` prefilled with Core/KV/LB info + poller id, plus `spire/upstream-join-token` and `spire/upstream-bundle.pem`. Valid token is required; on success we emit a `downloaded` event and mark delivered.\n  - `POST /api/admin/edge-packages/{id}/revoke` (idempotent) revokes the downstream entry, updates status, and records audit event. Revoked packages cannot be downloaded again.\n\n- Extend gRPC ReportStatus path: when an unknown poller reports, check pending onboarding packages. If the poller_id matches an issued/delivered package (and join token still valid), flip status to `activated`, append audit event, and add poller id to active set so future reports pass `isKnownPoller`. Optionally capture SPIFFE principal from the TLS peer via `peer.FromContext` when using SPIRE security provider.\n\n- CLI + scripts:\n  - Add `serviceradar-cli edge package create/list/download/revoke` commands that wrap the new HTTP APIs for operators who prefer terminal workflows.\n  - New installer helper `docker/compose/edge-poller-install.sh` that accepts the package archive, verifies expiry timestamps, unpacks artifacts into `docker/compose/spire/` + `edge-poller.env`, and invokes `edge-poller-restart.sh --skip-refresh --env-file ...`. Script aborts if join token expired or package status ≠ activated.\n\n- Web UI:\n  - Add \"Edge Onboarding\" admin section (`/admin/edge`) showing table of packages (status badges, expiry, site, poller ID) with actions for create/download/revoke and audit timeline drawer.\n  - Creation modal collects label/site/TTL/selectors, surfaces generated poller ID + SPIFFE ID, and provides copy/download button for the package archive + instructions for installers.\n  - Integrate with existing auth/role guard (admin only) and reuse toast/confirmation components.\n\n- Observability \u0026 security:\n  - Emit structured logs + metrics (`edge_onboarding_packages_total`, `edge_onboarding_package_state`) so we can alert on high pending counts or failed revocations.\n  - Require `config:write` scope (or admin role) for issuance endpoints; download endpoint checks session + token. Ensure responses set `Cache-Control: no-store` and mask token in logs.\n  - Document operational playbooks under `docs/docs/edge-onboarding.md` covering issuance flow, revocation, and installer steps.","notes":"Documented the end-to-end edge onboarding flow in docs/docs/edge-onboarding.md (poller issuance, metadata requirements, Docker restart) and linked it from the Docker setup guide. Captured the upcoming agent/checker onboarding expectations so we can wire KV updates + package types in the next sprint.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-28T15:18:46.365672928-05:00","updated_at":"2025-10-29T01:01:01.519318022-05:00","closed_at":"2025-10-29T01:01:01.519318022-05:00","comments":[{"id":202,"issue_id":"serviceradar-54","author":"mfreeman","text":"Updated docs/docs/edge-onboarding.md with component matrix and future agent/checker flows. Created GitHub issue #1909 for the follow-on automation and opened branch edge-onboarding-agents-checkers to track the doc updates.","created_at":"2025-10-29T06:00:59Z"}]}
{"id":"serviceradar-55","title":"serviceradar edge onboarding multi-component automation","description":"Implement the agent/checker onboarding enhancements described in GH-1909: extend the edge onboarding API/UI to handle poller/agent/checker types, enforce parent associations, and apply KV updates when packages are issued. Include CLI updates and activation telemetry.","acceptance_criteria":"Operators can issue agent and checker packages linked to parents and observe automatic KV updates once components activate.","notes":"Rebuilt db-event-writer via Bazel and rolled demo to ghcr.io/carverauto/serviceradar-db-event-writer:sha-53c1d7abe00759ab640b6c710b444b139a491209. API smoke test still hits Proton 49 errors (Cannot clone block with columns because block has 35 columns, but 24 columns given) when POST /api/admin/edge-packages runs, so rows disappear immediately and downloads 404. Need to instrument the core/db writer path to confirm which insert is still emitting the legacy 24-column shape.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-29T01:01:10.444582861-05:00","updated_at":"2025-10-30T18:32:12.464392249-05:00","closed_at":"2025-10-30T18:32:12.464392249-05:00","comments":[{"id":203,"issue_id":"serviceradar-55","author":"mfreeman","text":"Drafted implementation plan under docs/docs/edge-agent-onboarding.md#10 detailing backend schema updates, KV automation, CLI/UI work, and testing coverage. Next step is to sequence migrations and start on Core service changes.","created_at":"2025-10-29T06:22:17Z"},{"id":204,"issue_id":"serviceradar-55","author":"mfreeman","text":"Added placeholder KV hook in pkg/core/edge_onboarding.applyComponentKVUpdates with TODOs for agent/checker flows; documentation updated to point at the new entrypoint.","created_at":"2025-10-29T06:33:29Z"},{"id":205,"issue_id":"serviceradar-55","author":"mfreeman","text":"Expanded edge onboarding data model with component IDs/parent links, added Proton migration 00000000000006, wired API + core service to populate KV pending docs via applyComponentKVUpdates.","created_at":"2025-10-29T06:56:54Z"},{"id":206,"issue_id":"serviceradar-55","author":"mfreeman","text":"UI now exposes dedicated poller/agent/checker issuance buttons with parent selection constrained to checker→agent→poller, including relationship details in the table/detail views. Fixed the Proton migration to use uint64 for kv_revision so core boots cleanly. Rebuilt all Bazel images (web digest sha256:75571aad63515e1d60713c62fda22b4c68f4df1b047af2ac8473fd571a2665ad) and rolled the demo namespace; all deployments available.","created_at":"2025-10-29T12:56:50Z"},{"id":207,"issue_id":"serviceradar-55","author":"mfreeman","text":"Consolidated the top-level actions into a single \"Issue edge package\" button that opens the form, added a persistent submit button alongside the form header, and proved the flow end-to-end by rebuilding/pushing Bazel images (web digest sha256:d3228050d9b034b412194aaac56d066fa8f7b74609ca41f5c49bcd2dd85d2043) and rolling the demo deployment.","created_at":"2025-10-29T13:27:36Z"},{"id":208,"issue_id":"serviceradar-55","author":"mfreeman","text":"Updated backend for edge onboarding:\\n- DB now parses package_id as UUID when querying events, so the API no longer returns 502 after issuance.\\n- Poller package creation validates metadata_json and requires core_address/core_spiffe_id/spire parent details, preventing archive builds from failing later.\\n- go test ./pkg/db/... ./pkg/core/...","created_at":"2025-10-29T14:27:21Z"},{"id":209,"issue_id":"serviceradar-55","author":"mfreeman","text":"UI now keeps the grid populated on load instead of auto-selecting the first package, so the full onboarding list stays visible. Also renamed all occurrences of 'artefact' to 'artifact' across UI, API, CLI, and docs. Tests: go test ./pkg/db/... ./pkg/core/... ./pkg/cli/...","created_at":"2025-10-29T15:11:45Z"},{"id":210,"issue_id":"serviceradar-55","author":"mfreeman","text":"Adjusted DB layer so event queries send canonical string UUIDs while still validating input. This fixes the 502 when the UI fetches /api/admin/edge-packages/:id/events. Tests: go test ./pkg/db/... ./pkg/core/...","created_at":"2025-10-29T15:14:37Z"},{"id":211,"issue_id":"serviceradar-55","author":"mfreeman","text":"Rebuilt and published refreshed images for core + web via bazel (core @ sha256:1280b8827d82..., web @ sha256:51c3169b7285...). Rolled the demo namespace and waited for pods to come back healthy (zen needed ~5 min).","created_at":"2025-10-29T15:31:03Z"},{"id":212,"issue_id":"serviceradar-55","author":"mfreeman","text":"Pinned demo web deployment to ghcr.io/carverauto/serviceradar-web:sha-6e4b814a0356 and confirmed rollout.","created_at":"2025-10-29T15:31:49Z"},{"id":213,"issue_id":"serviceradar-55","author":"mfreeman","text":"Rebuilt web image (sha256:c475512118b13b3b22f0a3140e27fce3557df6b959c8c8095c27d3491d0c9564) and rolled demo deployment to tag sha-65aff1ef6819.","created_at":"2025-10-29T15:49:08Z"},{"id":214,"issue_id":"serviceradar-55","author":"mfreeman","text":"Implemented revoked-package deletion end-to-end: DB can drop package + events, core service/API expose DELETE /api/admin/edge-packages/{id}, and the admin UI now offers a Delete action for revoked entries (plus a back-to-list control and better refresh handling). Tests: go test ./pkg/db/... ./pkg/core/...","created_at":"2025-10-29T15:58:32Z"},{"id":215,"issue_id":"serviceradar-55","author":"mfreeman","text":"Added Next.js app route to proxy DELETE /api/admin/edge-packages/:id to core so the UI can remove revoked packages without 404s. Bazel rebuilt and pushed ghcr.io/carverauto/serviceradar-web:sha-d26752741b3b (digest sha256:36280c7dd18fdc6f32107ac4b26c6e793c2ee54bd2326b6f39d5f6943803680a) and rolled serviceradar-web in demo to pick up the change; deletion now succeeds in the admin UI.","created_at":"2025-10-29T18:03:13Z"},{"id":216,"issue_id":"serviceradar-55","author":"mfreeman","text":"Rebuilt and pushed core image ghcr.io/carverauto/serviceradar-core:sha-53c1d7abe00759ab640b6c710b444b139a491209 (digest sha256:19467752073ded0a2991c73f3b8a03c063a6746fa2f3e2b969932abf8ee53230) so the DELETE /api/admin/edge-packages/:id handler is live. Rolled deployment/serviceradar-core in demo; deletion calls should now hit the new API instead of 404ing.","created_at":"2025-10-29T18:25:33Z"},{"id":217,"issue_id":"serviceradar-55","author":"mfreeman","text":"Relaxed DeletePackage guard: allow deleting records that are already expired or have a revoked timestamp even if the legacy status column wasn’t updated; logs when we repair the status in-process (pkg/core/edge_onboarding.go#L787). Rebuilt/pushed core image (digest sha256:5a523b5f36fef063da1ddd6964f90898f9f5f5d0944122a121662729678e74ef) and rolled the demo core deployment so the change is live.","created_at":"2025-10-29T18:35:29Z"},{"id":218,"issue_id":"serviceradar-55","author":"mfreeman","text":"Normalised edge onboarding package statuses/types to lowercase during DB scans so legacy rows stored as 'Revoked' now map to the canonical enums (pkg/db/edge_onboarding.go#L431). This keeps DeletePackage from rejecting already-revoked entries. Rebuilt/pushed core image (digest sha256:219266a8bde1c3a78e7b7a4b864e1cfd782fc12d0a2524cc7e9100c71705a9e7) and rolled the serviceradar-core deployment at 13:46 UTC so the change is active.","created_at":"2025-10-29T18:47:21Z"},{"id":219,"issue_id":"serviceradar-55","author":"mfreeman","text":"Adjusted Get/List queries to always surface the latest edge package row for each package_id. We dedupe on the Go side and order by updated_at so API responses don’t return stale 'issued' versions alongside revoked ones. Rebuilt core, pushed digest sha256:a114b145edd411ba4fa89edaedd8057715b207f6734128b1c8063afa3e450901, and rolled the demo core deployment (pod now serviceradar-core-c5bfc957-579qt).","created_at":"2025-10-29T18:54:58Z"},{"id":220,"issue_id":"serviceradar-55","author":"mfreeman","text":"Added error logging on API delete failures so we capture root causes (pkg/core/api/edge_onboarding.go) and redeployed a fresh core build (digest sha256:c762c6f148da02eba508296bef5df5543f781d547da50a040afd26d1ad00faf9) at 14:09 UTC. If the UI delete still 502s, logs will now show the underlying error.","created_at":"2025-10-29T19:10:04Z"},{"id":221,"issue_id":"serviceradar-55","author":"mfreeman","text":"Swapped edge deletion SQL to use ALTER STREAM for both packages and events (Proton rejects ALTER TABLE on Stream engines), rebuilt core (digest sha256:fef36fdbf6445d4b464a7b27f469442792e7cd35dc8ae487731826606b54a36a), and rolled the demo deployment (pod serviceradar-core-9b866b48-spzks).","created_at":"2025-10-29T19:13:25Z"},{"id":222,"issue_id":"serviceradar-55","author":"mfreeman","text":"Progress update:\n- Admin API/UI now exposes DELETE via app router () and the core service accepts the call. Adjusted the proxy helper so 204 responses from core no longer throw in Next ( skips bodies for no-content cases). Pushed web image  (digest ) and rolled the demo deployment.\n- Core delete path tolerates legacy casing, emits structured errors, and uses  so packages/events are actually removed (Proton rejects ). Latest core build is  (digest ), deployed to .\n- Proton shows each package has two rows (issued + revoked); ListPackages now collapses to the newest revision so the UI doesn’t double-render issued state.\n\nOpen items / next steps:\n1. Replace the hard  deletes with a softer tombstone strategy (e.g. status→ + TTL or background vacuum) so operators aren’t issuing heavy deletes.\n2. Fix  502 by parsing UUIDs ahead of the Proton query.\n3. Populate missing  metadata before archive builds so downloads succeed.\n4. Revisit the UI list/detail UX once the above are stable (back navigation, list refresh jitter).","created_at":"2025-10-29T19:19:11Z"},{"id":223,"issue_id":"serviceradar-55","author":"mfreeman","text":"Progress update (2025-10-29):\n- Added the dynamic app route and hardened the proxy helper so DELETE calls flush through without 404s or Next 204 errors (`web/src/app/api/admin/edge-packages/[id]/route.ts`, `helpers.ts`). Pushed web image `ghcr.io/carverauto/serviceradar-web:sha-93df912ec1ae` (digest `sha256:bcb46fa3d723cfed72f2080a7bb65627445d4568676b64c2ef45f71234cb4031`) and rolled the demo deployment.\n- Core deletion now accepts legacy casing, logs failures, and runs on Proton via `ALTER STREAM` (`pkg/core/edge_onboarding.go`, `pkg/core/api/edge_onboarding.go`, `pkg/db/edge_onboarding.go`). Latest build: `ghcr.io/carverauto/serviceradar-core:sha-53c1d7abe00759ab640b6c710b444b139a491209` (digest `sha256:fef36fdbf6445d4b464a7b27f469442792e7cd35dc8ae487731826606b54a36a`) rolling on pod `serviceradar-core-9b866b48-spzks`.\n- ListPackages now collapses multi-version rows so the UI only shows the newest revision for each package.\n\nNext steps:\n1. Replace the heavy `ALTER STREAM … DELETE` approach with a tombstone/TTL strategy so operators can \"delete\" without rewriting the stream.\n2. Fix `/api/admin/edge-packages/:id/events` by parsing UUIDs before the Proton query to stop the 502 spinner.\n3. Populate `core_address` metadata prior to archive builds so downloads succeed.\n4. Clean up UI polish once the backend is stable (back navigation, refresh jitter).\n","created_at":"2025-10-29T19:19:37Z"},{"id":224,"issue_id":"serviceradar-55","author":"mfreeman","text":"Bumped the delete flow so tombstone revisions always carry a strictly newer updated_at. That should let Proton’s changelog surface the deleted status instead of re-surfacing the prior revoked row. Added a regression test to cover the monotonic timestamp bump and ran go test ./pkg/core/... . Next up: verify the tombstone row persists in Proton (edge_onboarding_packages without FINAL) and confirm the admin list no longer resurrects deleted packages after a refresh.","created_at":"2025-10-29T23:48:42Z"},{"id":225,"issue_id":"serviceradar-55","author":"mfreeman","text":"Rebuilt and pushed ghcr.io/carverauto/serviceradar-core@sha256:4de554ecc1d83d00a410e6c5e2f61524a962ffecea399f5ba9daa30f53952a8d via Bazel, then pointed the demo deployment at the new digest (serviceradar-core-66b98b8459-st7tc).","created_at":"2025-10-29T23:52:12Z"},{"id":226,"issue_id":"serviceradar-55","author":"mfreeman","text":"Implemented soft-delete tombstone handling: Core now writes to a dedicated  stream, ListPackages/GetPackage rehydrate deletion markers, and core deletion emits both the Upsert + tombstone event. Added migration 00000000000008, updated mocks/interfaces/tests, and rebuilt/pushed core image (ghcr.io/carverauto/serviceradar-core@sha256:c60b81b695339f7dc5d5bcbb782b97bf5339d5b93071a152dd1f129b1cbba114) with the new schema. Need to run through UI delete flow once we can auth against core to confirm the tombstoned packages stay hidden.","created_at":"2025-10-30T00:51:52Z"},{"id":227,"issue_id":"serviceradar-55","author":"mfreeman","text":"Soft-delete is now live end-to-end: Core writes package tombstones into the new  stream (migration 00000000000008) and / hydrate the deletion metadata so default queries hide deleted rows. Updated mocks, tests, and rebuilt the demo core pod to digest sha256:c60b81b695339f7dc5d5bcbb782b97bf5339d5b93071a152dd1f129b1cbba114. Next up: 1) authenticate to the demo core API (bearer token flow is currently blocked) and run the admin UI delete flow to confirm the tombstoned packages stay hidden; 2) verify the tombstone stream records actor/reason data via proton-sql after a real delete; 3) once auth is sorted, issue/revoke/delete a fresh package to ensure the event timeline renders correctly in the UI.","created_at":"2025-10-30T02:18:00Z"},{"id":228,"issue_id":"serviceradar-55","author":"mfreeman","text":"Reworked the delete flow to keep soft deletes inside edge_onboarding_packages: migration 0008 now just adds deleted_by/deleted_reason columns and drops the extra tombstone stream, the DB/service/API no longer query edge_onboarding_package_tombstones, and deletes write status=deleted plus actor metadata on the package row. Updated Go tests (go test ./pkg/db/... ./pkg/core/...) to cover the inline tombstone path. Next up: verify the demo UI still hides deleted packages after refresh, spot-check the stream in Proton to confirm deleted_by/reason land, and keep chipping away at the remaining TODOs (events UUID fix, archive metadata, UI polish).","created_at":"2025-10-30T02:27:26Z"},{"id":229,"issue_id":"serviceradar-55","author":"mfreeman","text":"Tightened the core/db queries so we always pull the latest revision per package: both GetEdgeOnboardingPackage and ListEdgeOnboardingPackages now window over edge_onboarding_packages and take row_number=1. That fixed the delete check returning the initial issued row (and blocking revocation/deletion). Built/pushed ghcr.io/carverauto/serviceradar-core@sha256:9be841384063a0ce11d592316db4708ca92541d43da451c3d016006c5a204a25 and rolled the demo deployment. Next up: re-test deleting a revoked package via the admin UI and, assuming that sticks, move on to the events UUID + archive metadata cleanup.","created_at":"2025-10-30T02:52:10Z"},{"id":230,"issue_id":"serviceradar-55","author":"mfreeman","text":"Rebuilt the edge onboarding DB insert path so it writes columns in Proton’s expected order (includes deleted_* and downstream_entry_id). Cut fresh core + db-event-writer images (core@sha256:a705cf352d82…, db-event-writer@sha256:db9c8f49ed6f…) and rolled demo. Proton was still noisy from earlier failures, so I ran scripts/reset-proton.sh demo to wipe/recreate the datastore and let the new core reapply migrations. Logs are quiet again; next check is to issue/revoke/delete a package in the UI while watching edge_onboarding_packages to confirm status=deleted rows persist without errors.","created_at":"2025-10-30T04:50:47Z"},{"id":231,"issue_id":"serviceradar-55","author":"mfreeman","text":"Instrumented DB.UpsertEdgeOnboardingPackage to log column counts and rebuilt core/db-event-writer (core@sha256:0f1dc42c822d…, db-event-writer@sha256:05524c4377a5…). Rolled both deployments and reset Proton. Issued a poller package via the core API using admin credentials; instrumentation shows the new core image now sends 33-column inserts. Proton still spams '35 columns, 24 given' errors for package 2058c587-c957-444a-aef5-3add56b3f47c and /api/admin/edge-packages returns an empty list, so some other writer (likely a replaying consumer) is still emitting the legacy payload. Need to trace that source next.","created_at":"2025-10-30T20:19:19Z"},{"id":232,"issue_id":"serviceradar-55","author":"mfreeman","text":"Proton reset + demo restart cleared the column mismatch. Rebuilt/pushed core and db-event-writer with Bazel, rolled demo, and verified edge package create→revoke→delete hits Proton without errors. Cleaned up edge onboarding lint issues (exhaustive switches, %w errors, statusUnknown reuse, serviceAgent const) and make lint/test now pass.","created_at":"2025-10-30T23:32:06Z"}]}
{"id":"serviceradar-56","title":"Edge onboarding e2e validation for docker poller and agent","description":"We need to exercise the new edge onboarding path end-to-end using the demo cluster APIs and a local docker-compose stack. Steps: 1) Pull API key/bearer token from demo namespace secrets, issue poller package via /api/admin/edge-packages, and bootstrap a docker poller locally. 2) Repeat for agent, ensuring it binds to the docker poller. 3) Verify core/device registry surfaces the new docker poller/agent devices in inventory; investigate merge/clobber behavior where multiple services from the same IP are being collapsed incorrectly. Expect follow-up changes in core registry if inventory does not retain both devices.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-10-30T18:36:29.179795581-05:00","updated_at":"2025-10-30T18:36:29.179795581-05:00","comments":[{"id":233,"issue_id":"serviceradar-56","author":"mfreeman","text":"Issued fresh poller/agent packages off the demo cluster API using the new component_type payloads and pulled the artifacts locally. Brought up the docker poller/agent stack with the packaged env file (skip-refresh) and copied the join token + bundle into docker/compose/spire/. Poller container reaches the nested SPIRE bootstrap but is stuck rotating upstream SVIDs (repeated 'SVIDStore not found' / 'join token not provided' loops), so the package stays in delivered state and never registers with core yet.","created_at":"2025-10-31T00:18:37Z"},{"id":234,"issue_id":"serviceradar-56","author":"mfreeman","text":"Refreshed SPIRE downstream credentials via docker/compose/refresh-upstream-credentials.sh and reran edge-poller-restart to pick up a new template tweak that sets POLLERS_POLLER_ID and disables KV. Poller stack now comes up cleanly and registers with core as docker-poller-e2e-01 (status reports visible in serviceradar-core logs), but the edge package still shows delivered in the admin API. Agent gRPC handshakes are still bouncing because it presents the poller SVID; need a follow-up tweak to either adjust the agent server SVID or teach the TLS authorizer to accept it so the package can flip to activated.","created_at":"2025-10-31T00:59:45Z"},{"id":235,"issue_id":"serviceradar-56","author":"mfreeman","text":"Current state: docker poller/agent stack now boots cleanly with the refreshed upstream join token and reports as docker-poller-e2e-01 into core (status batches visible in logs), but the edge packages remain in delivered. The activation likely stalls because the agent SVID in the package metadata still points to spiffe://carverauto.dev/services/agent while the stack is presenting the poller SVID to make auth succeed. Need to reconcile the SPIFFE IDs (either regenerate the package with the poller ID for the agent server handshake or adjust the TLS authorizer) and then re-check device inventory.","created_at":"2025-10-31T01:44:14Z"},{"id":236,"issue_id":"serviceradar-56","author":"mfreeman","text":"Investigated why edge packages fc348035-9092-4903-aaa6-c3e9105ce719 and bce4363a-da28-43bd-afce-9aa68f365b04 are stuck in \"delivered\" even though the docker poller is streaming status updates. Core never persisted an activation transition – we only ever wrote \"issued\"/\"delivered\" rows and events. Added activation tracking inside the core device registration path so the first heartbeat from a poller/agent promotes the package to \"activated\" and records the source IP. Included unit coverage in pkg/core for the new edge onboarding flow. Next step will be to ship a fresh serviceradar-core build to demo and confirm the UI flips those packages to activated once the new binary is live.","created_at":"2025-10-31T02:31:26Z"},{"id":237,"issue_id":"serviceradar-56","author":"mfreeman","text":"Repro root cause \u0026 remediation:\n- Docker compose stack was still emitting the default IDs (, ) baked into the templates, so core never matched the agent package () and its status stayed \"delivered\".\n- The persistent compose volumes also kept the old nested SPIRE SVIDs. Even after refreshing the join token, the cached upstream agent kept reusing the stale credential, causing repeated  failures.\n\nFix:\n1. Torn down the compose stack and purged the relevant volumes (, , ) to get a clean slate.\n2. Re-ran Deleted existing downstream entry 1d0811c9-981f-43db-a9cd-59b1e5ddb892 for spiffe://carverauto.dev/ns/edge/poller-nested-spire.\nCreated downstream entry for spiffe://carverauto.dev/ns/edge/poller-nested-spire with parent spiffe://carverauto.dev/spire/agent/join_token/d849497c-5e6b-4687-bf4b-62ec77cd2475.\nJoin token written to /home/mfreeman/serviceradar/docker/compose/spire/upstream-join-token.\nUpstream bundle written to /home/mfreeman/serviceradar/docker/compose/spire/upstream-bundle.pem.\nUpstream SPIRE credentials refreshed successfully. +  with  (and cleared  so the agent stops trying to dial datasvc).\n3. Restarted the stack; both poller and agent now advertise the package IDs (, ).\n4. Core immediately promoted the poller package to ; the agent package will flip after the first report cycle (~60s) now that the IDs line up.\n\nAction items:\n- Confirm  reaches  once the agent heartbeat lands.\n- Verify device inventory renders both docker devices; this will prove the new activation hook is wiring correctly.","created_at":"2025-10-31T03:14:08Z"},{"id":238,"issue_id":"serviceradar-56","author":"mfreeman","text":"Repro root cause \u0026 remediation:\n- Docker compose stack was still emitting the default IDs (`docker-poller`, `docker-agent`) baked into the templates, so core never matched the agent package (`docker-agent-e2e-01`) and its status stayed \"delivered\".\n- The persistent compose volumes also kept the old nested SPIRE SVIDs. Even after refreshing the join token, the cached upstream agent kept reusing the stale credential, causing repeated `join token does not exist or has already been used` failures.\n\nFix:\n1. Torn down the compose stack and purged the relevant volumes (`compose_poller-spire-runtime`, `compose_poller-data`, `compose_agent-data`) to get a clean slate.\n2. Re-ran `docker/compose/refresh-upstream-credentials.sh` + `setup-edge-poller.sh` with `POLLERS_POLLER_ID=docker-poller-e2e-01` (and cleared `KV_ADDRESS` so the agent stops trying to dial datasvc).\n3. Restarted the stack; both poller and agent now advertise the package IDs (`docker-poller-e2e-01`, `docker-agent-e2e-01`).\n4. Core immediately promoted the poller package to `activated`; the agent package will flip after the first report cycle (~60s) now that the IDs line up.\n\nAction items:\n- Confirm `bce4363a-da28-43bd-afce-9aa68f365b04` reaches `activated` once the agent heartbeat lands.\n- Verify device inventory renders both docker devices; this will prove the new activation hook is wiring correctly.\n","created_at":"2025-10-31T03:14:24Z"},{"id":239,"issue_id":"serviceradar-56","author":"mfreeman","text":"- Re-issued demo edge packages via Core admin API after revoking legacy poller/agent IDs; new package ids: poller `0f61dd1e-bfc7-4448-8461-bb65e9f71509`, agent `5bd01c6d-91a2-4821-8154-bdce639ae894`.\n- Reset compose stack (wiped volumes, refreshed nested SPIRE creds) and replayed `edge-poller-restart.sh` using the new artifacts. Poller successfully enrolls once upstream join token is refreshed.\n- Agent bootstrap was blocked by KV SPIFFE validation (`invalid server SPIFFE ID`). Temporarily cleared `KV_ADDRESS`/`KV_SPIFFE_ID` to get the container to come up; agent now serves gRPC (`serviceradar-agent` broadcasts ready state) but runs without KV linkage.\n- Poller↔agent handshake still fails: downstream TLS reports `transport: authentication handshake failed: unexpected ID \"spiffe://carverauto.dev/services/poller\"` during `/grpc.health.v1.Health/Check`. Package metadata currently stamps the agent server SPIFFE ID as `spiffe://carverauto.dev/ns/edge/docker-agent-e2e-01`; need to reconcile what the agent actually expects versus what the poller presents.\n- Because of the handshake failure both packages remain `status:\"delivered\"`; core never records activation. Device inventory still missing the docker agent entry.\n- Next step: decide whether to adjust package metadata (agent server/parent SPIFFE IDs) or relax the agent-side client allowlist so the poller’s `spiffe://carverauto.dev/services/poller` caller is accepted. Also need a follow-up pass to re-enable KV once the SPIFFE mismatch is sorted.\n","created_at":"2025-10-31T04:19:23Z"},{"id":240,"issue_id":"serviceradar-56","author":"mfreeman","text":"Updated the edge package tooling so new deliveries carry the nested SPIRE SVIDs we expect. The archive now emits NESTED_SPIRE_PARENT_ID/NESTED_SPIRE_DOWNSTREAM_SPIFFE_ID/NESTED_SPIRE_AGENT_SPIFFE_ID, setup-edge-poller.sh rewrites poller-spire/env with those values, and docker compose passes NESTED_SPIRE_AGENT_SPIFFE_ID through to the poller entrypoint. Next step is to reissue the demo packages, rerun setup-edge-poller.sh, re-enable KV in edge-poller.env, and confirm the poller/agent packages flip to activated and show up in device inventory.","created_at":"2025-10-31T04:39:43Z"},{"id":241,"issue_id":"serviceradar-56","author":"mfreeman","text":"Re-ran the edge onboarding flow end-to-end with fresh packages after adding unique SPIFFE selectors. Poller package ce492405-a4ed-404d-bece-7044a0bb7798 and agent package 91bcd977-11da-4fc4-bf01-213d4c9a549b both activated (core /api/admin/edge-packages reports status=activated). Docker compose stack (serviceradar-poller + serviceradar-agent) is up with agent_id=docker-agent-e2e-02; poller-to-agent gRPC handshakes now succeed: poller logs show \"Service check completed successfully\" and activations escalate out of delivered. Core device inventory includes default:172.19.0.2 with poller_id docker-poller-e2e-02, confirming telemetry flows into core. Left KV disabled in edge-poller.env because re-enabling still crashes the agent (invalid server SPIFFE ID); need a separate follow-up to realign KV SPIFFE expectations before re-enabling.","created_at":"2025-10-31T06:01:18Z"}]}
{"id":"serviceradar-6","title":"Identity publisher churns KVService and floods otel logs","description":"Otel perf telemetry is logging proto.KVService/Get and Update spans constantly for serviceradar-core even when device records are unchanged. Registry's identity publisher issues a Get on every DeviceUpdate, causing duplicate RPCs and otel spam. We need to cache canonical identity writes locally so unchanged updates short-circuit and CAS revisions are reused.","acceptance_criteria":"Core identity publishes skip redundant KV RPCs and otel perf logs no longer emit every few milliseconds for proto.KVService/Get+Update.","notes":"Trimmed OTEL perf metrics to export only when spans exceed the slow threshold, cutting batch sizes from 512 to 2 spans per export. Rebuilt/pushed otel collector (sha256:dc3b9a9da7089b1bb3c1259531e5806e637413895ecccb8b70a213ed60517dbf) and rolled the demo deployment; collector logs now show 2-span batches instead of 512.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T16:27:45.671978374-05:00","updated_at":"2025-10-27T01:24:07.084423425-05:00","closed_at":"2025-10-14T16:28:15.408673518-05:00"}
{"id":"serviceradar-7","title":"Investigate OTEL trace surge and dashboard mismatch","description":"Observability widget shows ~791k traces while observability dashboard reports ~29.8k; Proton telemetry DB already back above 578k traces shortly after reset. Need to identify which services are emitting high-volume KV spans, shut off noisy OTEL instrumentation around KV calls if appropriate, and reconcile dashboard counts.","notes":"Hooked analytics widget and /observability cards into the shared trace-count hook so both read the same 24h aggregates. Bazel rebuilt \u0026 pushed ghcr.io/carverauto/serviceradar-web@sha256:3e1e90760afda36a545526f756ce414a6f7741960d4e14411b091bfdca0d0a9a and I rolled the demo serviceradar-web deployment to that digest; the new pod is healthy. Dashboards now show matching trace totals after cache refresh.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T18:18:43.577346219-05:00","updated_at":"2025-10-27T01:24:07.357433178-05:00","closed_at":"2025-10-14T20:56:07.477820626-05:00"}
{"id":"serviceradar-8","title":"Prevent JWKS rotation after core restarts","description":"Core init container overwrote /var/lib/serviceradar/core.json with the configmap every restart, dropping the persisted RS256 keypair. A fresh serviceradar-cli generate-jwt-keys run produced a new kid, so Kong’s DB-less config still referenced the old key and rejected tokens with 401 No credentials found for given kid.","notes":"Updated k8s demo core init script to preserve any existing auth.jwt_private_key_pem / jwt_key_id when copying the templated core.json, then rerun generate-jwt-keys to backfill only if missing. Applied new Deployment spec (with app.kubernetes.io/part-of label alignment) and rolled the core pod; existing kid key-9b8643c5be64350b survived the restart. Restarted serviceradar-kong so it re-rendered kong.yml from the current JWKS (key now matches) and verified /auth/jwks.json plus kong.yml both advertise the same kid.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T19:36:35.202849445-05:00","updated_at":"2025-10-27T01:24:07.574872612-05:00","closed_at":"2025-10-14T19:36:52.527921534-05:00"}
{"id":"serviceradar-9","title":"Investigate identity publisher revision caching","description":"Another agent flagged that identity publisher caches the revision from the pre-update Get response instead of the Update response. Need to reproduce to confirm whether cache entries wind up stale after KV revisions roll, before applying any fix.","notes":"Fixed the cache to record revisions exclusively from Update responses; if the RPC omits a value we now fall back to zero so the next publish performs a fresh Get. Added an always-on regression test that failed before the change; 'go test ./pkg/registry' now passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T21:14:17.003578027-05:00","updated_at":"2025-10-27T01:24:07.632571594-05:00","closed_at":"2025-10-14T21:21:08.797102353-05:00"}
