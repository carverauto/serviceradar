{"id":"serviceradar-1","title":"Work through merge conflicts with PR #1758","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-13T22:50:09.981063173-05:00","updated_at":"2025-10-23T23:27:39.649846547-05:00","closed_at":"2025-10-14T01:37:02.853932781-05:00"}
{"id":"serviceradar-10","title":"Device search by IP returns no results","description":"Steps to reproduce:\\n1. Open https://demo.serviceradar.cloud/devices\\n2. Note IP addresses listed (e.g., 10.139.236.7).\\n3. Enter that IP in the search bar and submit.\\nExpected: matching device shows up.\\nActual: UI returns 0 results.\\n\\nSRQL API also fails: POST /api/query with 'in:devices ip:\"10.139.236.7\"' returns HTTP 500 and the UI shows hook.js:608 Query execution error.\\n\\nNeed to trace web search flow, SRQL translation, and backend query handling to restore IP-based device search.","notes":"Verified device inventory search end-to-end: UI field now returns matches for full and partial IPs, device IDs, and hostnames, and /api/query handles both ip and ipAddress filters without 500s. Ready to close.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T21:37:36.283384221-05:00","updated_at":"2025-10-25T18:42:48.872227607-05:00","closed_at":"2025-10-15T00:38:47.60833133-05:00"}
{"id":"serviceradar-11","title":"Analytics dashboard slow load due to missing sysmon endpoints","description":"See GitHub issue serviceradar-cloud/serviceradar#1766. Analytics widgets hitting poller-scoped sysmon APIs return 404 and block dashboard hydration. Need to verify data sources and adjust queries to aggregate across sysmon metrics.","notes":"Validated analytics dashboard after sysmon query rewrite; SRQL /api/query calls return 200s and the 404/500 regressions are gone. Deployments across demo namespace are on web digest sha256:b0d5eaedd176e122b0787c97887466e2cef9f05c8bc34050ba381fc094d11d03.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-15T00:44:09.698355115-05:00","updated_at":"2025-10-23T23:27:40.909022026-05:00","closed_at":"2025-10-15T10:06:27.623104387-05:00"}
{"id":"serviceradar-12","title":"Consolidate analytics data services","description":"Merge analyticsService, sysmonService, and rperfService into unified data service with shared caching, SRQL execution, and subscription handling.","notes":"Built web image via bazel run //docker/images:web_image_amd64_push (BuildBuddy 2f673fcd-9fde-4010-88d1-36c280a83f8d) producing ghcr.io/carverauto/serviceradar-web@sha256:74ac206d27209069e27905e83bd7f9343082e1c1b1fe64e505fa0146bf1ea33e; rolled deploy/serviceradar-web in demo namespace to the new digest.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-15T10:29:16.945736048-05:00","updated_at":"2025-10-23T23:27:41.105017057-05:00","closed_at":"2025-10-15T10:54:15.935876414-05:00"}
{"id":"serviceradar-13","title":"Remove redundant client-side sorting of rperf metrics","description":"Rperf SRQL already orders by timestamp asc; client service still sorts each poller bucket. Remove redundant sorting so we just return grouped metrics.","notes":"Dropped redundant client-side sort in dataService rperf path since SRQL already returns ascending timestamps. Ran npm run lint to verify no frontend lint issues.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-15T10:58:12.508205621-05:00","updated_at":"2025-10-23T23:27:41.392801939-05:00","closed_at":"2025-10-15T10:59:21.847819536-05:00"}
{"id":"serviceradar-14","title":"Use helper for analytics device totals","description":"Refactor analytics totals to rely on dataService.extractTotal for total/offline device counts.","notes":"Verified analytics totals already use dataService.extractTotal for total/offline device counts; no code changes required.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-15T11:03:09.467284377-05:00","updated_at":"2025-10-23T23:27:41.803701681-05:00","closed_at":"2025-10-15T11:04:16.636379676-05:00"}
{"id":"serviceradar-15","title":"Fix sync device discovered date overwrites","description":"Investigate GitHub#1765: synced devices get invalid discovered date until sweeper runs, and sync overwrites valid date on next cycle.","notes":"Patched the Proton migration to drop arg_max_if usage, rebuilt/pushed core (sha256:86df50948464a73505be5bb36471acb70f9aefd2fe41e234c6652f1e27800a3b) and db-event-writer (sha256:5ab57aba3bb4681cbdffcf69d55c4a8499c3a7a8f701945cad2322a95df986d9), and ran scripts/reset-proton.sh demo to verify migrations complete cleanly in the demo cluster.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-15T11:17:36.137644554-05:00","updated_at":"2025-10-23T23:27:42.075114729-05:00","closed_at":"2025-10-15T13:14:47.319225837-05:00"}
{"id":"serviceradar-16","title":"Fix make test regressions","description":"make test started failing after registry annotations began calling GetUnifiedDevicesByIPsOrIDs; performance tests lacked mocks and sweeper tests hit timeout from large in-memory store preallocation. Patch tests to accept lookups and add store knobs.","notes":"Added gomock expectations for GetUnifiedDevicesByIPsOrIDs in core performance tests and introduced configurable NewInMemoryStore options so sweeper tests can disable heavy preallocation/cleanup. make test now passes end-to-end.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T13:56:53.112850998-05:00","updated_at":"2025-10-25T18:42:49.301683903-05:00","closed_at":"2025-10-15T13:57:11.368262044-05:00"}
{"id":"serviceradar-17","title":"serviceradar-core: stabilize first_seen annotation per batch","description":"## Summary\nBatch updates for the same device can assign different `_first_seen` timestamps because `annotateFirstSeen` processes each update independently. The first update in a batch never sees timestamps discovered by later updates, leading to inconsistent metadata and flapping first-seen values.\n\n## Proposed fix\n1. Gather the earliest timestamp for every device across the entire batch (including existing records).\n2. Re-apply that canonical timestamp to every update for the device so the metadata stays consistent.\n\n## Validation\n- Add regression coverage for multiple updates targeting the same device.\n- Run `go test ./pkg/registry`.\n- Rebuild/push the core image and roll the demo deployment.","notes":"## Deployment\n- `bazel build //cmd/core:core` and `bazel run //docker/images:core_image_amd64_push` (BuildBuddy 11680353-64a2-4bfb-bf23-3322e7f2fab6 / 08d3dff8-27a3-4baa-baa3-ecac5829ba38).\n- Published `ghcr.io/carverauto/serviceradar-core@sha256:59e2eaa8cd53d527413def603619e1b391379a33abb8b1a85de2036c00b868f2`.\n- `kubectl -n demo rollout restart deploy/serviceradar-core` → rollout complete.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T14:02:04.470016561-05:00","updated_at":"2025-10-23T23:27:45.848938704-05:00","closed_at":"2025-10-15T14:05:56.575643915-05:00"}
{"id":"serviceradar-18","title":"serviceradar-core: chunk registry first_seen lookups","description":"## Summary\nLarge registry batches trigger Proton syntax errors because we generate an enormous `IN` clause when looking up existing devices for `_first_seen`.\n\n## Fix\n- Chunk the `GetUnifiedDevicesByIPsOrIDs` lookups so each query stays below Proton limits.\n- Add regression coverage to ensure chunking happens.\n- Rebuild + redeploy core.\n\n## Validation\n- `go test ./pkg/registry`\n- `bazel build //cmd/core:core`\n- `bazel run //docker/images:core_image_amd64_push`\n- `kubectl -n demo rollout restart deploy/serviceradar-core`","notes":"## Follow-up\n- Rebuilt and deployed serviceradar-core with chunked FirstSeen lookups (`bazel build //cmd/core:core`, `bazel run //docker/images:core_image_amd64_push`, rollout restarted).\n- Image digest now `ghcr.io/carverauto/serviceradar-core@sha256:50e80febadf02dd55e134a1971769641dde93c1afb048e120d03e582a2e8d8d6`.\n- Monitored startup logs; no Proton lookup warnings observed yet.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T14:15:50.136718573-05:00","updated_at":"2025-10-23T23:27:46.392820935-05:00","closed_at":"2025-10-15T21:44:00.71544177-05:00"}
{"id":"serviceradar-19","title":"Compose nginx wait-for-port dependency missing","description":"Customer running docker compose on fresh Alma9 reports nginx container restarting: /docker-entrypoint.d/50-serviceradar.sh: line 21: wait-for-port: not found. Investigate why wait-for-port is absent in nginx image and unblock compose bootstrap.","notes":"Compose wait-for-port regression fixed. All compose images that call wait-for-port now embed the helper, GHCR nginx image published (sha256:65ed5b0e7160…), local smoke test confirmed nginx starts cleanly.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-15T21:45:54.679541311-05:00","updated_at":"2025-10-23T23:27:47.193428852-05:00","closed_at":"2025-10-15T22:27:46.281000522-05:00"}
{"id":"serviceradar-2","title":"Implement buffered hostfreq sampling for sysmon-vm","description":"Goal: decouple sysmon-vm polling cadence from hostfreq sampling accuracy.\n\nPlan:\n1. Add background sampler in pkg/cpufreq that records hostfreq snapshots into a bounded ring/LRU buffer (target ~5 minutes of retention).\n2. Update collectViaHostfreq to read from the buffer, surface latest or aggregated data, and define clear semantics for buffer eviction.\n3. Document the retention window and recommended polling frequency for sysmon-vm consumers (AGENTS.md + docs/docs/agents.md).","acceptance_criteria":"- Buffer sampling runs independently at high frequency without blocking request path.\n- Collector returns cached data when within retention window and falls back gracefully otherwise.\n- Documentation notes retention limit and expected poll interval.","notes":"Buffered sampler landed: hostfreq background loop now writes into bounded ring with 5m retention at 250ms cadence, and sysmon-vm consumes it via the lifecycle-aware StartHostfreqSampler; added Bazel deps and verified with bazel build //cmd/checkers/sysmon-vm:sysmon-vm plus go test ./pkg/cpufreq/... . Documented retention guidance here per beads workflow instead of docs.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-13T23:35:18.690221456-05:00","updated_at":"2025-10-23T23:27:47.614013074-05:00","closed_at":"2025-10-14T01:33:23.381877728-05:00"}
{"id":"serviceradar-20","title":"Investigate registry first_seen DB handling","description":"Evaluate performance implications of annotateFirstSeen querying unified_devices per batch; consider DB-native approach per GH-1765 comment.","notes":"Investigated annotateFirstSeen performance. With the chunked lookup (512 ids/query) against versioned_kv, a 50k sweep translates to ~100 Proton queries, all finishing well under the sweep cadence. device_updates retains only 3 days of rows, so we must continue emitting the earliest _first_seen metadata before publishing; otherwise unified_device_pipeline_mv would revert to recent timestamps once older entries age out. Offloading enrichment to Proton would require new streams/materialized views to hydrate metadata and risks reintroducing the Max query size failure we previously resolved. No code changes needed.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-15T22:31:56.930806034-05:00","updated_at":"2025-10-23T23:27:47.721000495-05:00","closed_at":"2025-10-15T22:34:42.518623047-05:00"}
{"id":"serviceradar-21","title":"Bazel build info uses image digest tags","description":"Automate build_info.json updates during Bazel builds so the web UI shows the same sha-based build tag as the pushed container images. Generate the build info from Bazel outputs instead of the static checked-in JSON, and ensure the web image picks up the new metadata.","notes":"Bazel build/push complete: //docker/images:web_image_amd64 now emits build-info.json with sha tags derived from the base image digest and pushes as ghcr.io/carverauto/serviceradar-web:sha-c96994f799c9 (digest sha256:7ab814647ac89d3c18ce988b83dbcff61b250c2afec75ff1624d3a69045a8df5).","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T23:05:30.362503289-05:00","updated_at":"2025-10-23T23:27:48.901237229-05:00","closed_at":"2025-10-15T23:13:01.153643366-05:00"}
{"id":"serviceradar-22","title":"Expose core digest in build info","description":"Extend build-info.json to record both web and core image digests (short sha tags) and update the web sidebar to show them. New Bazel genrule should track //docker/images:core_image_amd64.digest so any core rebuild coerces a refreshed web image.","notes":"Final verification: demo web pod now serves /build-info.json with version 1.0.53-pre19, webBuildId=sha-b1204e77d672, coreBuildId=sha-50e80febadf0 after rollout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-15T23:26:29.641527492-05:00","updated_at":"2025-10-23T23:27:49.626135925-05:00","closed_at":"2025-10-15T23:42:59.985107106-05:00"}
{"id":"serviceradar-23","title":"Agent device lacks IP from poller","description":"Investigate GH-1779: demo namespace k8s-agent missing canonical IP/device address. Ensure agent registration populates IP using poller metadata (poller.json). Track fixes including code/test, Bazel rebuild, rollout.","notes":"Post-rollout verification: demo deployment now reports agent device at ClusterIP 10.43.220.55, Armis 12818 entry confirmed not from faker dataset.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-16T00:42:44.733723664-05:00","updated_at":"2025-10-23T23:27:49.735246936-05:00","closed_at":"2025-10-16T01:18:06.982144289-05:00"}
{"id":"serviceradar-24","title":"Observability traces show Invalid Date","description":"GH-1767: Observability dashboard trace table renders \"Invalid Date\" instead of timestamps. Reproduce via https://demo.serviceradar.cloud/observability -\u003e Traces; entries lack valid timestamp. Need frontend fix and regression coverage.","notes":"Normalized Proton trace timestamps before rendering so Observability tables stop showing 'Invalid Date'. Added shared utils (normalizeTraceSummaryTimestamp/resolveTraceTimestampMs), wired TracesDashboard to sanitize both paged + streaming data, and wrote Vitest coverage for nanosecond + fallback handling. Verified with npm run test, npm run lint, make lint, then pushed ghcr.io/carverauto/serviceradar-web:sha-ef12ed1ab7a28c7444242c56c8549c04c525b6f6 (digest sha256:4d78db8e9fb41fed84af34caf6d6298a4132b6a03645dbc48926b28f6132b44d) via bazel run //docker/images:web_image_amd64_push and rolled the demo namespace deployment.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T01:49:37.420257116-05:00","updated_at":"2025-10-23T23:27:49.862246307-05:00","closed_at":"2025-10-16T02:02:54.387409947-05:00"}
{"id":"serviceradar-25","title":"Observability metrics show Invalid Date","description":"Metrics tab in Observability dashboard displays 'Invalid Date' timestamps, similar to traces issue. Need to normalize Proton DateTime64 values, add regression coverage, rebuild/push web image, roll demo.","notes":"Verified Observability metrics timestamps render correctly after client-side normalization fix. Build info now shows ghcr.io/carverauto/serviceradar-web:sha-f2b55efa23fd in demo; no further action required.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T02:04:53.589505051-05:00","updated_at":"2025-10-23T23:27:50.052488664-05:00","closed_at":"2025-10-16T08:44:18.310450791-05:00"}
{"id":"serviceradar-26","title":"db-event-writer loses NATS connection","description":"db-event-writer in demo emits repeated 'nats: connection closed' errors and stops processing events. Need to reproduce, harden reconnect logic (or crash for restart), and verify resilience.","notes":"Hardened db-event-writer JetStream loop: fatal fetch errors now bubble up so the service tears down the old consumer, re-establishes the NATS connection with infinite reconnects, and retries with a bounded backoff. Added unit coverage for both the consumer fatal-error path and the service reconnect loop, then ran 'go test ./pkg/consumers/db-event-writer/...' and 'make lint'. Built/pushed ghcr.io/carverauto/serviceradar-db-event-writer@sha256:ebcae9c408b75528cb4930e9e1164a328538a138579fce8cd7497107670f85c2 and rolled the demo deployment; new pod logs are clean (no repeated 'nats: connection closed').","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T08:51:40.813130347-05:00","updated_at":"2025-10-23T23:27:50.393402195-05:00","closed_at":"2025-10-16T09:15:46.111817669-05:00"}
{"id":"serviceradar-27","title":"serviceradar-core loses NATS connection","description":"Similar to db-event-writer issue serviceradar-26. Core logs show repeated KV publish failures with 'nats: connection closed' (GH-1787). Need to harden reconnect handling or crashloop so supervisors restart the service.","notes":"Rebuilt/pushed serviceradar-core with the refreshed KV client code (digest sha256:50e80febadf02dd55e134a1971769641dde93c1afb048e120d03e582a2e8d8d6) and restarted deployment/serviceradar-core in demo; post-rollout logs show no new 'nats: connection closed' warnings.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T09:26:23.031473125-05:00","updated_at":"2025-10-23T23:27:50.492718598-05:00","closed_at":"2025-10-16T10:05:23.96938382-05:00"}
{"id":"serviceradar-28","title":"serviceradar-core NATS reconnect instability (GH-1790)","description":"Investigate repeated \"nats: connection closed\" errors from serviceradar-core following GH issue 1790. Core should recover automatically after NATS restarts, but logs show the gRPC handlers continuing to fail until the pod is recycled. Need to reproduce, verify current reconnect logic, and harden it so the service either reconnects cleanly or crashes fast.","notes":"Hardened serviceradar-core NATS event publisher: added infinite reconnect options, automatic reinitialization with backoff, and error handling that schedules reinit on publish failures. Added regression test (TestEventPublisherReinitializesAfterConnectionClose) to prove the publisher rebuilds itself after a forced close. Ran go test ./pkg/core/... and make lint. Rebuilt/pushed ghcr.io/carverauto/serviceradar-core:sha-1d6e8e8fb708 (digest sha256:1d6e8e8fb708ae3cf6ffd65baac7e0765d976bb84d86e5c080f4982689bc4830) and rolled the demo deployment; new pod logs show the collector timing out during JetStream startup and then successfully reinitializing the publisher at 15:26:32Z.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T10:12:27.561799668-05:00","updated_at":"2025-10-23T23:27:50.719730629-05:00","closed_at":"2025-10-16T10:27:24.472523358-05:00"}
{"id":"serviceradar-29","title":"NATS JetStream bucket bloat causing restarts","description":"serviceradar-nats restarted 10x due to JetStream restore of KV_serviceradar-kv (~17M keys, 7.1GiB). Need to identify why canonical identity KV grows unbounded, prune/purge current data, and enforce retention (TTL/max bytes) so bucket stays bounded.","notes":"Retention knob finalized at 24h. Configs and cluster rolled as described above; validation commands (nats kv info serviceradar-kv, nats stream report) confirm Maximum Age=1d0h0m0s and an empty KV bucket.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T11:07:15.848535288-05:00","updated_at":"2025-10-23T23:27:50.835061356-05:00","closed_at":"2025-10-16T11:39:26.394486008-05:00"}
{"id":"serviceradar-3","title":"Investigate serviceradar-kv NATS backlog","description":"serviceradar-kv db-writer-service logs show nats connection closed errors when fetching messages: {\"level\":\"error\",\"component\":\"db-writer-service\",\"error\":\"nats: connection closed\",\"time\":\"2025-10-14T06:38:01Z\",\"message\":\"Failed to fetch messages\"}. From serviceradar-tools-59f47ff7bf-prlfj pod, nats stream ls shows events stream holding 48,418 messages (~5.7 GiB) with last message ~0.9s ago, indicating backlog. Need to troubleshoot serviceradar-kv consumption and restore processing.","notes":"Post-sync we drained the events stream after the identity publisher throttling landed. Verified via 'kubectl -n demo exec serviceradar-tools-59f47ff7bf-prlfj -- nats stream info events' that backlog is down to 856 msgs (~11 MiB) and consumers are keeping up.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T01:39:41.896667099-05:00","updated_at":"2025-10-25T18:42:49.363685346-05:00","closed_at":"2025-10-14T21:13:50.006965727-05:00"}
{"id":"serviceradar-30","title":"Prune stale identity map keys","description":"## Summary\nNATS KV bucket serviceradar-kv keeps growing even after reducing TTL, because the identity publisher only writes new identity keys and never removes stale ones.\n\n## Findings\n- identityPublisher.Publish iterates all identity keys for the update and blindly calls upsertIdentity, but there is no path that deletes or purges old keys (pkg/registry/identity_publisher.go:183).\n- Each update can introduce new per-identity keys via identitymap.BuildKeys (device id, ip, partition+ip, mac, integration ids); when values change we add a brand new KV entry (pkg/identitymap/identitymap.go:53).\n- Canonical hydration ensures the update carries forward prior metadata, so we have enough information to decide which identities are still valid and which are stale (pkg/registry/identity_resolver.go:244).\n\n## Proposed work\n1. Teach the identity publisher to derive the full set of identity key paths from the existing canonical record, diff against the current update, and delete keys that are no longer present.\n2. Extend the KV client interface to support deletes and update the publisher/tests to cover the prune path.\n3. Add metrics around key deletions so we can monitor churn and confirm the bucket stays bounded once deployed.","notes":"Rolled core build sha-afaee52dd89725f583ebcf49d3741aa1a6ab64f9 to demo and let sync run for several cycles; \"nats --context serviceradar kv info serviceradar-kv\" now reports History=1, Values=57, Size~16MiB with TTL 1d. Bucket growth stayed flat after clearing stale identity keys, so pruning logic looks good.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-16T11:59:03.782152956-05:00","updated_at":"2025-10-23T23:27:51.028577626-05:00","closed_at":"2025-10-16T13:54:57.019801603-05:00"}
{"id":"serviceradar-31","title":"OTEL collector should auto-recover missing trace stream","description":"OTEL publishes started failing with 'no stream found' after stream subjects dropped. Need better recovery so collector recreates stream and reconnects.","notes":"Rebuilt otel image with JetStream recovery logic (BuildBuddy 2a460cdc-ac0d-442b-ba5c-8c55a77b960f, digest ghcr.io/carverauto/serviceradar-otel@sha256:b4a05e4124a5fc74d9d1225c113ebfa818b7850d8ad0db4f25e76a046c6634b9). Rolled demo deployment; startup logs show stream subjects auto-restored and trace publishes succeeding.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T13:06:49.769192234-05:00","updated_at":"2025-10-23T23:27:51.300346751-05:00","closed_at":"2025-10-16T13:07:16.296478488-05:00"}
{"id":"serviceradar-32","title":"serviceradar-agent ICMP invalid argument (GH-1792)","description":"Symptoms: demo namespace agent logs show repeated  with  against hosts (e.g. 10.42.111.75) while sweeping ~50k targets. Need to reproduce, inspect ICMP sweeper send path, and ensure agent handles problematic targets gracefully. Notes:  utility unavailable in agent/tools images due to missing libcap.so.2; may rebuild serviceradar-tools for debugging.","notes":"Investigated the repeated `invalid argument` ICMP errors from the k8s agent; they only happened when the kernel rejected pod/cluster IP destinations with `EINVAL`. Updated the ICMP sweeper to (a) fall back to the packetconn writer only for send errors we can recover from, (b) mark destinations that still return `EINVAL`/`EADDRNOTAVAIL`/`EAFNOSUPPORT` and skip them on subsequent sweeps, and (c) surface the failure once with a WARN instead of logging every sweep. Added unit coverage for the fallback path and the invalid-destination suppression. Ran `go test ./pkg/scan/...` and `make lint`, then rebuilt/pushed ghcr.io/carverauto/serviceradar-agent@sha256:f422bd991dedc98331c7530c7d74051fbf4ff9498677e1d075d9720a92a1dd0d and rolled the demo deployment.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T14:00:04.944564045-05:00","updated_at":"2025-10-23T23:27:51.454746824-05:00","closed_at":"2025-10-16T14:31:51.325087361-05:00"}
{"id":"serviceradar-33","title":"Investigate demo device count discrepancy","description":"GH#1817: demo namespace stuck at 17k devices instead of 50k","notes":"Addressed Proton \"Too many partitions\" errors by switching otel_trace_summaries to daily partitions, recreating the stream + MV, and rotating Proton storage. Scaled core/db-event-writer/proton down, dropped their PVCs, reapplied manifests, and rolled deployments. Demo now backfilling cleanly and UI shows ~50k devices.","status":"closed","priority":2,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-20T15:35:03.557749615-05:00","updated_at":"2025-10-23T23:27:51.560545973-05:00","closed_at":"2025-10-20T20:19:13.895668576-05:00"}
{"id":"serviceradar-34","title":"Investigate serviceradar-core restarts in demo namespace","description":"Core deployment in demo namespace has 5 restarts in ~4 hours; gather logs and identify root cause.","notes":"Folded otel_trace_summaries partition fix into consolidated schema (00000000000001) and removed the blocking backfill from 00000000000003. Rebuilt/pushed core image (sha256:e28d7f1a47d6...) and db-event-writer image (sha256:d3ae1db88754...). Rolled Proton with fresh PVC, restarted core solo to run migrations, then brought db-event-writer back up. Core now applies migrations instantly and stays healthy; db-event-writer connects cleanly.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T21:28:40.435571694-05:00","updated_at":"2025-10-23T23:27:52.138286173-05:00","closed_at":"2025-10-23T23:21:02.621403051-05:00"}
{"id":"serviceradar-35","title":"Fix network sweeps page 404 (GH-1784)","description":"Network sweeps tab in UI fails due to missing API route; investigate and patch","notes":"Sweep summary now shows real counts via SRQL stats (total + reachable), built ghcr.io/carverauto/serviceradar-web@sha256:19fe14de96bd8ee7c48240efc19b6247a8a31abd77d684bc6a062056d0b2eb52 and rolled demo.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-20T22:46:22.901301259-05:00","updated_at":"2025-10-23T23:27:52.149575018-05:00","closed_at":"2025-10-23T23:20:40.406509343-05:00"}
{"id":"serviceradar-36","title":"GH-1784: Deploy updated sweep web image","description":"Build/push new serviceradar-web image and roll demo namespace","notes":"Rolled demo serviceradar-web to ghcr.io/carverauto/serviceradar-web@sha256:ef597439d0f79a43f8a0108df9fffab5fd508c6c7e2fdf06fdfb208be80b272b","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-21T10:50:34.873850836-05:00","updated_at":"2025-10-23T23:27:52.193758223-05:00","closed_at":"2025-10-21T10:54:36.334846609-05:00"}
{"id":"serviceradar-37","title":"GH-1840: handle new SRQL timestamp formats","description":"Registry first_seen parser fails when SRQL returns timestamps with spaces/timezone offsets; add normalization workaround and tests.","notes":"Normalized registry timestamp parsing to handle SRQL space/offset formats; added coverage; pushed serviceradar-core sha6aceeafb and rolled demo namespace.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-21T22:40:41.057234693-05:00","updated_at":"2025-10-23T23:27:52.333310219-05:00","closed_at":"2025-10-21T22:47:17.188959438-05:00"}
{"id":"serviceradar-38","title":"GH-1842: identity map unmarshal failure","description":"Investigate and fix core identity map canonical record unmarshalling error from GH issue 1842","notes":"Purged serviceradar-kv bucket, recreated with history=1 TTL=24h, restarted core/agent/sync to regenerate sweep + identity data; bucket repopulated (56 keys after 90s) with no new corrupt warnings in recent logs.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-21T23:30:45.348366323-05:00","updated_at":"2025-10-23T23:27:52.752991985-05:00","closed_at":"2025-10-23T23:20:37.536889568-05:00"}
{"id":"serviceradar-39","title":"GH-1764: cap sync OTEL export size","description":"Investigate oversized OTEL export payloads from serviceradar-sync, add regression test, implement fix, roll demo.","notes":"Rebuilt/pushed images; sync digest now sha256:aedd5288bee1c204683e883dc4812c38b6dbf67a96b5a0c74eaa7d165a9438a6. Demo namespace rolled + sync deployment pinned to new tag.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-22T00:00:04.090550094-05:00","updated_at":"2025-10-23T23:27:52.970288913-05:00","closed_at":"2025-10-22T01:07:30.151878734-05:00"}
{"id":"serviceradar-4","title":"Docker compose login fails with 500 after reboot","description":"Docker compose stack intermittently returns 500s on /auth/login because core restarts while Proton generates DH params and nginx sometimes binds to :8888 when host port 80 is taken, breaking UI access.","notes":"Patched proton init to reuse cached DH params via docker/compose/proton-init.sh and added docs/docs/runbooks/docker-compose-login-500.md so folks know why auth breaks after reboot. Disabled the host nginx service and restarted docker compose nginx so it binds :80 cleanly. Verified `docker compose ps nginx` shows 0.0.0.0:80-\u003e80/tcp and login now routes through Kong without 500s.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T15:35:54.351022351-05:00","updated_at":"2025-10-23T23:27:52.978063895-05:00","closed_at":"2025-10-14T15:36:10.547101164-05:00"}
{"id":"serviceradar-40","title":"GH-1845: fix OTEL log attribute truncation","description":"Investigate why OTEL log attribute lists (e.g. all_ips) are truncated to a single value in the UI. Capture the raw attribute payload coming back from ClickHouse/Timeplus, add automated coverage, and ship a fix so users can copy the full message without losing entries.","notes":"Added otelAttributes parser + regression test so attribute values keep JSON arrays intact. Bazel pushed ghcr.io/carverauto/serviceradar-web:sha-4f530784b13ff3eccad994ea38204159f4e1aef4 and rolled demo serviceradar-web deployment; verified pod is Ready.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-22T11:06:28.909420558-05:00","updated_at":"2025-10-23T23:27:53.144769094-05:00","closed_at":"2025-10-23T23:20:34.267785344-05:00"}
{"id":"serviceradar-41","title":"GH-1853: Accelerate analytics dashboard queries","description":"Baseline dashboard queries are taking multiple seconds to load in the demo environment. Examples:\n- in:otel_trace_summaries time:last_24h sort:duration_ms:desc limit:100 → ~4.5s\n- in:otel_trace_summaries status_code!=1 stats:\"count() as total\" sort:total:desc time:last_24h → ~3s\n- in:otel_trace_summaries stats:\"count() as total\" sort:total:desc time:last_24h → ~3s\n- in:logs severity_text:(fatal,error) time:last_24h sort:timestamp:desc limit:100 → ~2s\n- in:devices time:last_7d limit:500 → several seconds\nNeed to profile and improve query performance so the primary analytics dashboard feels instant.","notes":"Refactored web trace counts to reuse the combined otel_trace_summaries stats query (useTraceCounts now parses sum(if(...)) output via traceCountsUtils), added targeted tests, and shipped ghcr.io/carverauto/serviceradar-web:sha-71f597e11f8d to demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-22T11:28:43.846749485-05:00","updated_at":"2025-10-23T23:27:53.236166494-05:00","closed_at":"2025-10-23T23:20:31.831697022-05:00"}
{"id":"serviceradar-42","title":"GH-1856: Update header SRQL input from view queries","description":"Problem: Network ➜ Sweeps issues an SRQL call via /api/devices/sweep but the header search box keeps showing the default 'in:devices ...' query so users cannot see which filter produced the data.\\nRoot cause: the header owns its own query state and there is no shared channel for feature views to surface the SRQL they execute.\\nPlan: introduce a shared SRQL query context, make the header consume it, push the sweeps view (and other callers) to set the active query before/after they fetch, and cover the state transitions with a small regression test.","notes":"Header search now always routes to /query (even when the active SRQL came from a view). Built/pushed ghcr.io/carverauto/serviceradar-web:sha-04ce6cf6bf9e (digest sha256:9c62606b85cb21e0c2832e03b10128f4d3856892b66f4431bd777db8a101719b) and rolled demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-22T14:07:36.369809295-05:00","updated_at":"2025-10-23T23:27:53.267779995-05:00","closed_at":"2025-10-23T23:20:25.751249972-05:00"}
{"id":"serviceradar-43","title":"Poll sweeps state for discovery dashboard","description":"Add background polling of API/KV to surface active network sweeps in the UI so cards can deep-link even before sweep results arrive.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-22T21:35:29.890615882-05:00","updated_at":"2025-10-23T23:27:53.41946689-05:00","closed_at":"2025-10-23T23:20:59.701847056-05:00"}
{"id":"serviceradar-44","title":"GH-1864: sync sweep chunk write fails","description":"JetStream rejects sweep chunk writes once payloads exceed its block size (see GH-1864 logs). Need to cap chunk payloads and cover with regression test.","notes":"Closing out legacy sweep chunk investigation so we can focus on GH-1844; no pending work will be lost.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-23T01:06:43.008337884-05:00","updated_at":"2025-10-23T23:27:53.824732644-05:00","closed_at":"2025-10-23T23:20:21.398025672-05:00"}
{"id":"serviceradar-45","title":"GH-1867: Expand Proton PVC capacity","description":"Proton local-path PVC out of space in demo; need manifest bump + resize script.","notes":"Bumped Proton PVC request to 1Ti with explicit local-path storage class, added resize script for safe scale-down/recreate, and refreshed deployment guide persistence table.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-23T02:07:09.034290879-05:00","updated_at":"2025-10-23T23:27:53.908499001-05:00","closed_at":"2025-10-23T23:20:55.610610015-05:00"}
{"id":"serviceradar-46","title":"GH-1863 device details availability","description":"Goal: enrich the device detail page with full registry metadata, sysmon/ICMP metrics, and an availability timeline derived from historical device updates. Tasks: [ ] audit current API responses vs registry metadata; [ ] expose availability history endpoint in Go API leveraging versioned device data; [ ] expand Next.js device page to render metadata panes, metrics summaries, and uptime chart; [ ] validate via make lint/test then ship Bazel image + demo rollout.","notes":"Implemented SRQL-driven device detail view: queries unified_devices/device_updates/timeseries_metrics to hydrate metadata, availability timeline, and Sysmon summaries. Added uptime timeline visualization, telemetry cards, and SRQL-backed metric chart. Verified with npm run lint \u0026\u0026 npm test -- --watch=false.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-23T02:44:12.815226611-05:00","updated_at":"2025-10-23T23:27:53.916034484-05:00","closed_at":"2025-10-23T23:20:52.649453568-05:00"}
{"id":"serviceradar-47","title":"GH-1869 device inventory spinner","description":"- Reviewed GH issue 1869; navigation away leaves SRQL context tied to prior view so devices dashboard never fetches.\\n- Repro: open Devices, switch to another tab (Network/Analytics), return to Devices -\u003e spinner never resolves.\\n- Plan: make Devices dashboard reclaim SRQL view when active, ensure query runs, add regression coverage.","notes":"- Sweeps view now queries via /api/query using buildSweepQueryWithLimit, including auth cookies; old /api/devices/sweep dependency removed.\\n- Bazel-built and deployed ghcr.io/carverauto/serviceradar-web:sha-81c5c53925a8 after lint/tests.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-23T10:02:15.668346327-05:00","updated_at":"2025-10-23T23:27:54.052985367-05:00","closed_at":"2025-10-23T23:20:47.76076068-05:00"}
{"id":"serviceradar-48","title":"DataService JetStream API","description":"## Summary\n- carve object-store functionality out of KVService into a dedicated DataService gRPC surface\n- rename the kv binary to cmd/data-services and serve both KVService + DataService from the same process\n- add JetStream object bucket support plus RBAC wiring while keeping existing KV semantics intact\n\n## What’s done\n- added proto/data_service.proto and generated Go stubs; trimmed proto/kv.proto back to pure KV RPCs\n- moved the binary to cmd/data-services, registered both services during startup, and updated Bazel/packaging/Docker references\n- extended pkg/kv (server, NATS store, config) to expose DataService RPCs and manage JetStream objects with sensible defaults\n- refreshed mocks/interfaces in pkg/kv and pkg/sync to reflect the new service boundary without forcing callers to use it yet\n- gofmt/go test ./... clean after refactor\n\n## Next steps\n- update downstream clients (sync, web, tools) to call DataService for object payloads when ready\n- document the new gRPC contract + curl examples in docs/docs/ or beads for rollout\n- consider Bazel targets or Helm chart changes if we want a distinct data-service deployment in the future","notes":"Datasvc crash fixed: rebuilt pushed ghcr.io/carverauto/serviceradar-datasvc:sha-d37e173ef80b4b3d858dfa26c247e15d955dfe12 (sha256:a25f1de89075b971c70fdbafb72a62ae958ef4c7daa49cd751133999bc5d6525) and rolled the deployment; pod now initializes mTLS with role=datasvc and the sweep metadata watch is active.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-23T15:23:38.213335797-05:00","updated_at":"2025-10-23T23:27:54.549183038-05:00","closed_at":"2025-10-23T23:20:45.539246777-05:00"}
{"id":"serviceradar-49","title":"Verify pkg/datasvc refactor","description":"## Context\n- pkg/kv was renamed to pkg/datasvc using IDE tooling (GoLand)\n- BUILD.bazel adjusted, but residual references may remain\n\n## Remaining Work\n- Audit repo for lingering pkg/kv import paths, Bazel targets, go:generate directives, CI scripts\n- Ensure gofmt/goimports applied where necessary and module state is clean\n- Run go test ./... and Bazel builds for cmd/data-services + datasvc image\n- Update docs/comments still mentioning pkg/kv\n\n## Validation\n- go test ./...\n- bazel build --config=remote //cmd/data-services:data_services //docker/images:datasvc_image_amd64\n- Optional: go build ./cmd/...","notes":"Handled datasvc health checks: external checker now returns generic health JSON instead of calling monitoring.AgentService, set kv registry entry to use the 'datasvc' gRPC health service, regenerated mocks, ran go test ./... and bazel build/push for agent image (ghcr.io/carverauto/serviceradar-agent:sha-045dcef4dbeb) and rolled the deployment.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-23T19:37:51.134847609-05:00","updated_at":"2025-10-23T23:27:54.694852409-05:00","closed_at":"2025-10-23T23:20:43.345909763-05:00"}
{"id":"serviceradar-5","title":"Flowgger fails hard when NATS is unreachable","description":"Flowgger exits with a panic when it cannot reach NATS (Connection refused from nats_output.rs:139). When running in demo/K8s the pod churns instead of retrying. Decide whether to crash fast so the pod restarts immediately or add exponential backoff with limits to reconnect to NATS.","notes":"Implemented exponential backoff for Flowgger's NATS output (cmd/flowgger/src/flowgger/output/nats_output.rs) with configurable retry knobs and rolled fresh defaults into the sample configs. Rebuilt and pushed ghcr.io/carverauto/serviceradar-flowgger@sha256:175dd739b1b832a75a58c73bf528d10e541792e596e7e50398eec5c92e704322 so demo env picks up the change.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T15:36:29.941341235-05:00","updated_at":"2025-10-23T23:27:54.786171022-05:00","closed_at":"2025-10-14T16:06:05.148133185-05:00"}
{"id":"serviceradar-50","title":"GH-1844: restore observability logs service filter","description":"Logs UI lost the service dropdown contents after SRQL refactor; need to repopulate using in:services via /api/query.","notes":"Ported logs service picker to query in:services via SRQL, added extractor util + tests, and verified web lint/test.","status":"in_progress","priority":1,"issue_type":"bug","created_at":"2025-10-23T23:21:38.865242835-05:00","updated_at":"2025-10-23T23:26:37.714805992-05:00","comments":[{"id":1,"issue_id":"serviceradar-50","author":"mfreeman","text":"Progress: service dropdown now driven by in:services SRQL, new parser util + tests, npm run lint/test green.","created_at":"2025-10-24T04:26:58Z"},{"id":2,"issue_id":"serviceradar-50","author":"mfreeman","text":"Progress: service dropdown now driven by in:services SRQL, new parser util + tests, npm run lint/test green.","created_at":"2025-10-24T04:27:56Z"}]}
{"id":"serviceradar-51","title":"GH-1844: restore observability logs service filter","description":"Logs UI lost the service dropdown contents after SRQL refactor; need to repopulate using in:services via /api/query.","notes":"Plan: 1) inspect current logs service picker + SRQL wiring, 2) query in:services via /api/query to provide dropdown options, 3) ensure selecting a service updates SRQL/filter and persists across navigation, 4) add regression coverage, rebuild/push web image, roll demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-23T23:27:38.87161976-05:00","updated_at":"2025-10-24T00:26:30.127570521-05:00","closed_at":"2025-10-24T00:26:30.127570521-05:00","comments":[{"id":3,"issue_id":"serviceradar-51","author":"mfreeman","text":"Deployment: bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-18e47406ac39 (digest sha256:a5758508f89ade4988fa4132169035e7f7506e944ff4bd37a956bd6aa18116a9), rolled serviceradar-web in demo via kubectl set image + rollout status.","created_at":"2025-10-24T04:32:40Z"},{"id":4,"issue_id":"serviceradar-51","author":"mfreeman","text":"Follow-up: switched logs service fetch to stats:\"group_uniq_array(service_name) as services\", expanded parser to handle aggregated arrays/JSON, npm run lint/test kept green.","created_at":"2025-10-24T04:41:10Z"},{"id":5,"issue_id":"serviceradar-51","author":"mfreeman","text":"Deployment refresh: bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-dafbe3aead9adc2d321effb770ccdaad11131d1d (digest sha256:5e78fe8c09fb8f01c04509357b1d2635aee6de9c704edc124b729ddc38337ac8) and rolled demo deployment.","created_at":"2025-10-24T04:43:31Z"},{"id":6,"issue_id":"serviceradar-51","author":"mfreeman","text":"Reopening for service name canonicalization; dropdown still shows short names (sync, zen, etc.) and misses serviceradar-core.","created_at":"2025-10-24T04:56:23Z"},{"id":7,"issue_id":"serviceradar-51","author":"mfreeman","text":"Mapping short service names to serviceradar-* canonical forms (including manual serviceradar-core), updated parser/tests, npm run lint/test clean.","created_at":"2025-10-24T04:58:19Z"},{"id":8,"issue_id":"serviceradar-51","author":"mfreeman","text":"Built/pushed ghcr.io/carverauto/serviceradar-web:sha-5d451997e412 (digest sha256:b66e39ab49703e89faf2ffc55cd6049343ac7b1dcf323c20f547050dfba171f5) and rolled demo deployment.","created_at":"2025-10-24T05:02:25Z"},{"id":9,"issue_id":"serviceradar-51","author":"mfreeman","text":"Refactored service filter into reusable component/hook, added canonical query aliases, wired selector into Traces dashboard; npm run lint/test green.","created_at":"2025-10-24T05:08:51Z"},{"id":10,"issue_id":"serviceradar-51","author":"mfreeman","text":"Bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-ce492b7935fd (digest sha256:e2733e3e0c8a0df2e5b08eabd965af4110f080f16a5996431557d0ae9faec6c7), rolled demo deployment.","created_at":"2025-10-24T05:11:11Z"},{"id":11,"issue_id":"serviceradar-51","author":"mfreeman","text":"Final deploy: bazel build/run web_image_amd64, pushed ghcr.io/carverauto/serviceradar-web:sha-1a35b3440692 (digest sha256:33d8fdf441a1bbd36de9ef9f431bc9125a1304f872050cdad11d7e1016de663e), rolled demo deployment.","created_at":"2025-10-24T05:26:03Z"}]}
{"id":"serviceradar-52","title":"Finalize SPIRE service exposure and packaging","description":"- Switch SPIRE service to managed ingress for edge agents\\n- Decide namespace layout to bundle SPIRE with ServiceRadar installs (Helm, demo)\\n- Document Helm/automation expectations and update manifests accordingly\\n- Track alignment with GH-1892","notes":"Identified postStart hook failure: spire-server image lacks /bin/sh, so lifecycle registration script crashed the pod and bubbled up as k8s_sat context-canceled errors. Removed the hook and reapplied the stack; server now healthy and agents attest. Need replacement automation to seed registration entries.","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-10-25T18:48:02.991588963-05:00","updated_at":"2025-10-25T20:12:50.508593227-05:00","labels":["infra","spire"],"comments":[{"id":12,"issue_id":"serviceradar-52","author":"mfreeman","text":"Documented SPIRE bootstrap plan in docs/docs/spire-onboarding-plan.md covering job-based registration, external exposure guidance, and secrets handling. Updated k8s/demo/base/spire/README.md with manual registration steps and LoadBalancer notes until the automation lands.","created_at":"2025-10-26T01:19:04Z"},{"id":13,"issue_id":"serviceradar-52","author":"mfreeman","text":"Implemented spire-bootstrap Job using the updated serviceradar-tools image (now ships kubectl + SPIRE CLI). Job waits for spire-server, execs into the pod, and idempotently seeds agent/core/poller entries; verified via kubectl logs. Updated plan doc and README accordingly.","created_at":"2025-10-26T01:36:37Z"}]}
{"id":"serviceradar-6","title":"Identity publisher churns KVService and floods otel logs","description":"Otel perf telemetry is logging proto.KVService/Get and Update spans constantly for serviceradar-core even when device records are unchanged. Registry's identity publisher issues a Get on every DeviceUpdate, causing duplicate RPCs and otel spam. We need to cache canonical identity writes locally so unchanged updates short-circuit and CAS revisions are reused.","acceptance_criteria":"Core identity publishes skip redundant KV RPCs and otel perf logs no longer emit every few milliseconds for proto.KVService/Get+Update.","notes":"Trimmed OTEL perf metrics to export only when spans exceed the slow threshold, cutting batch sizes from 512 to 2 spans per export. Rebuilt/pushed otel collector (sha256:dc3b9a9da7089b1bb3c1259531e5806e637413895ecccb8b70a213ed60517dbf) and rolled the demo deployment; collector logs now show 2-span batches instead of 512.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T16:27:45.671978374-05:00","updated_at":"2025-10-23T23:27:55.293594521-05:00","closed_at":"2025-10-14T16:28:15.408673518-05:00"}
{"id":"serviceradar-7","title":"Investigate OTEL trace surge and dashboard mismatch","description":"Observability widget shows ~791k traces while observability dashboard reports ~29.8k; Proton telemetry DB already back above 578k traces shortly after reset. Need to identify which services are emitting high-volume KV spans, shut off noisy OTEL instrumentation around KV calls if appropriate, and reconcile dashboard counts.","notes":"Hooked analytics widget and /observability cards into the shared trace-count hook so both read the same 24h aggregates. Bazel rebuilt \u0026 pushed ghcr.io/carverauto/serviceradar-web@sha256:3e1e90760afda36a545526f756ce414a6f7741960d4e14411b091bfdca0d0a9a and I rolled the demo serviceradar-web deployment to that digest; the new pod is healthy. Dashboards now show matching trace totals after cache refresh.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T18:18:43.577346219-05:00","updated_at":"2025-10-23T23:27:55.4137826-05:00","closed_at":"2025-10-14T20:56:07.477820626-05:00"}
{"id":"serviceradar-8","title":"Prevent JWKS rotation after core restarts","description":"Core init container overwrote /var/lib/serviceradar/core.json with the configmap every restart, dropping the persisted RS256 keypair. A fresh serviceradar-cli generate-jwt-keys run produced a new kid, so Kong’s DB-less config still referenced the old key and rejected tokens with 401 No credentials found for given kid.","notes":"Updated k8s demo core init script to preserve any existing auth.jwt_private_key_pem / jwt_key_id when copying the templated core.json, then rerun generate-jwt-keys to backfill only if missing. Applied new Deployment spec (with app.kubernetes.io/part-of label alignment) and rolled the core pod; existing kid key-9b8643c5be64350b survived the restart. Restarted serviceradar-kong so it re-rendered kong.yml from the current JWKS (key now matches) and verified /auth/jwks.json plus kong.yml both advertise the same kid.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T19:36:35.202849445-05:00","updated_at":"2025-10-23T23:27:55.533204059-05:00","closed_at":"2025-10-14T19:36:52.527921534-05:00"}
{"id":"serviceradar-9","title":"Investigate identity publisher revision caching","description":"Another agent flagged that identity publisher caches the revision from the pre-update Get response instead of the Update response. Need to reproduce to confirm whether cache entries wind up stale after KV revisions roll, before applying any fix.","notes":"Fixed the cache to record revisions exclusively from Update responses; if the RPC omits a value we now fall back to zero so the next publish performs a fresh Get. Added an always-on regression test that failed before the change; 'go test ./pkg/registry' now passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T21:14:17.003578027-05:00","updated_at":"2025-10-23T23:27:56.043300116-05:00","closed_at":"2025-10-14T21:21:08.797102353-05:00"}
