{"id":"serviceradar-1","title":"Work through merge conflicts with PR #1758","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-13T22:50:09.981063173-05:00","updated_at":"2025-11-02T19:24:28.628949132-06:00","closed_at":"2025-10-14T01:37:02.853932781-05:00"}
{"id":"serviceradar-10","title":"Device search by IP returns no results","description":"Steps to reproduce:\\n1. Open https://demo.serviceradar.cloud/devices\\n2. Note IP addresses listed (e.g., 10.139.236.7).\\n3. Enter that IP in the search bar and submit.\\nExpected: matching device shows up.\\nActual: UI returns 0 results.\\n\\nSRQL API also fails: POST /api/query with 'in:devices ip:\"10.139.236.7\"' returns HTTP 500 and the UI shows hook.js:608 Query execution error.\\n\\nNeed to trace web search flow, SRQL translation, and backend query handling to restore IP-based device search.","notes":"Verified device inventory search end-to-end: UI field now returns matches for full and partial IPs, device IDs, and hostnames, and /api/query handles both ip and ipAddress filters without 500s. Ready to close.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T21:37:36.283384221-05:00","updated_at":"2025-11-02T19:24:28.7494463-06:00","closed_at":"2025-10-15T00:38:47.60833133-05:00"}
{"id":"serviceradar-11","title":"Analytics dashboard slow load due to missing sysmon endpoints","description":"See GitHub issue serviceradar-cloud/serviceradar#1766. Analytics widgets hitting poller-scoped sysmon APIs return 404 and block dashboard hydration. Need to verify data sources and adjust queries to aggregate across sysmon metrics.","notes":"Validated analytics dashboard after sysmon query rewrite; SRQL /api/query calls return 200s and the 404/500 regressions are gone. Deployments across demo namespace are on web digest sha256:b0d5eaedd176e122b0787c97887466e2cef9f05c8bc34050ba381fc094d11d03.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-15T00:44:09.698355115-05:00","updated_at":"2025-11-02T19:24:28.841274644-06:00","closed_at":"2025-10-15T10:06:27.623104387-05:00"}
{"id":"serviceradar-12","title":"Consolidate analytics data services","description":"Merge analyticsService, sysmonService, and rperfService into unified data service with shared caching, SRQL execution, and subscription handling.","notes":"Built web image via bazel run //docker/images:web_image_amd64_push (BuildBuddy 2f673fcd-9fde-4010-88d1-36c280a83f8d) producing ghcr.io/carverauto/serviceradar-web@sha256:74ac206d27209069e27905e83bd7f9343082e1c1b1fe64e505fa0146bf1ea33e; rolled deploy/serviceradar-web in demo namespace to the new digest.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-15T10:29:16.945736048-05:00","updated_at":"2025-11-02T19:24:28.999971677-06:00","closed_at":"2025-10-15T10:54:15.935876414-05:00"}
{"id":"serviceradar-13","title":"Remove redundant client-side sorting of rperf metrics","description":"Rperf SRQL already orders by timestamp asc; client service still sorts each poller bucket. Remove redundant sorting so we just return grouped metrics.","notes":"Dropped redundant client-side sort in dataService rperf path since SRQL already returns ascending timestamps. Ran npm run lint to verify no frontend lint issues.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-15T10:58:12.508205621-05:00","updated_at":"2025-11-02T19:24:29.009622529-06:00","closed_at":"2025-10-15T10:59:21.847819536-05:00"}
{"id":"serviceradar-14","title":"Use helper for analytics device totals","description":"Refactor analytics totals to rely on dataService.extractTotal for total/offline device counts.","notes":"Verified analytics totals already use dataService.extractTotal for total/offline device counts; no code changes required.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-15T11:03:09.467284377-05:00","updated_at":"2025-11-02T19:24:29.065521833-06:00","closed_at":"2025-10-15T11:04:16.636379676-05:00"}
{"id":"serviceradar-15","title":"Fix sync device discovered date overwrites","description":"Investigate GitHub#1765: synced devices get invalid discovered date until sweeper runs, and sync overwrites valid date on next cycle.","notes":"Patched the Proton migration to drop arg_max_if usage, rebuilt/pushed core (sha256:86df50948464a73505be5bb36471acb70f9aefd2fe41e234c6652f1e27800a3b) and db-event-writer (sha256:5ab57aba3bb4681cbdffcf69d55c4a8499c3a7a8f701945cad2322a95df986d9), and ran scripts/reset-proton.sh demo to verify migrations complete cleanly in the demo cluster.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-15T11:17:36.137644554-05:00","updated_at":"2025-11-02T19:24:29.173434443-06:00","closed_at":"2025-10-15T13:14:47.319225837-05:00"}
{"id":"serviceradar-16","title":"Fix make test regressions","description":"make test started failing after registry annotations began calling GetUnifiedDevicesByIPsOrIDs; performance tests lacked mocks and sweeper tests hit timeout from large in-memory store preallocation. Patch tests to accept lookups and add store knobs.","notes":"Added gomock expectations for GetUnifiedDevicesByIPsOrIDs in core performance tests and introduced configurable NewInMemoryStore options so sweeper tests can disable heavy preallocation/cleanup. make test now passes end-to-end.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-10-15T13:56:53.112850998-05:00","updated_at":"2025-11-02T19:24:29.499394004-06:00","closed_at":"2025-10-15T13:57:11.368262044-05:00"}
{"id":"serviceradar-17","title":"serviceradar-core: stabilize first_seen annotation per batch","description":"## Summary\nBatch updates for the same device can assign different `_first_seen` timestamps because `annotateFirstSeen` processes each update independently. The first update in a batch never sees timestamps discovered by later updates, leading to inconsistent metadata and flapping first-seen values.\n\n## Proposed fix\n1. Gather the earliest timestamp for every device across the entire batch (including existing records).\n2. Re-apply that canonical timestamp to every update for the device so the metadata stays consistent.\n\n## Validation\n- Add regression coverage for multiple updates targeting the same device.\n- Run `go test ./pkg/registry`.\n- Rebuild/push the core image and roll the demo deployment.","notes":"## Deployment\n- `bazel build //cmd/core:core` and `bazel run //docker/images:core_image_amd64_push` (BuildBuddy 11680353-64a2-4bfb-bf23-3322e7f2fab6 / 08d3dff8-27a3-4baa-baa3-ecac5829ba38).\n- Published `ghcr.io/carverauto/serviceradar-core@sha256:59e2eaa8cd53d527413def603619e1b391379a33abb8b1a85de2036c00b868f2`.\n- `kubectl -n demo rollout restart deploy/serviceradar-core` → rollout complete.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T14:02:04.470016561-05:00","updated_at":"2025-11-02T19:24:29.614858028-06:00","closed_at":"2025-10-15T14:05:56.575643915-05:00"}
{"id":"serviceradar-18","title":"serviceradar-core: chunk registry first_seen lookups","description":"## Summary\nLarge registry batches trigger Proton syntax errors because we generate an enormous `IN` clause when looking up existing devices for `_first_seen`.\n\n## Fix\n- Chunk the `GetUnifiedDevicesByIPsOrIDs` lookups so each query stays below Proton limits.\n- Add regression coverage to ensure chunking happens.\n- Rebuild + redeploy core.\n\n## Validation\n- `go test ./pkg/registry`\n- `bazel build //cmd/core:core`\n- `bazel run //docker/images:core_image_amd64_push`\n- `kubectl -n demo rollout restart deploy/serviceradar-core`","notes":"## Follow-up\n- Rebuilt and deployed serviceradar-core with chunked FirstSeen lookups (`bazel build //cmd/core:core`, `bazel run //docker/images:core_image_amd64_push`, rollout restarted).\n- Image digest now `ghcr.io/carverauto/serviceradar-core@sha256:50e80febadf02dd55e134a1971769641dde93c1afb048e120d03e582a2e8d8d6`.\n- Monitored startup logs; no Proton lookup warnings observed yet.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T14:15:50.136718573-05:00","updated_at":"2025-11-02T19:24:29.623390087-06:00","closed_at":"2025-10-15T21:44:00.71544177-05:00"}
{"id":"serviceradar-19","title":"Compose nginx wait-for-port dependency missing","description":"Customer running docker compose on fresh Alma9 reports nginx container restarting: /docker-entrypoint.d/50-serviceradar.sh: line 21: wait-for-port: not found. Investigate why wait-for-port is absent in nginx image and unblock compose bootstrap.","notes":"Compose wait-for-port regression fixed. All compose images that call wait-for-port now embed the helper, GHCR nginx image published (sha256:65ed5b0e7160…), local smoke test confirmed nginx starts cleanly.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-15T21:45:54.679541311-05:00","updated_at":"2025-11-02T19:24:29.731537305-06:00","closed_at":"2025-10-15T22:27:46.281000522-05:00"}
{"id":"serviceradar-2","title":"Implement buffered hostfreq sampling for sysmon-vm","description":"Goal: decouple sysmon-vm polling cadence from hostfreq sampling accuracy.\n\nPlan:\n1. Add background sampler in pkg/cpufreq that records hostfreq snapshots into a bounded ring/LRU buffer (target ~5 minutes of retention).\n2. Update collectViaHostfreq to read from the buffer, surface latest or aggregated data, and define clear semantics for buffer eviction.\n3. Document the retention window and recommended polling frequency for sysmon-vm consumers (AGENTS.md + docs/docs/agents.md).","acceptance_criteria":"- Buffer sampling runs independently at high frequency without blocking request path.\n- Collector returns cached data when within retention window and falls back gracefully otherwise.\n- Documentation notes retention limit and expected poll interval.","notes":"Buffered sampler landed: hostfreq background loop now writes into bounded ring with 5m retention at 250ms cadence, and sysmon-vm consumes it via the lifecycle-aware StartHostfreqSampler; added Bazel deps and verified with bazel build //cmd/checkers/sysmon-vm:sysmon-vm plus go test ./pkg/cpufreq/... . Documented retention guidance here per beads workflow instead of docs.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-13T23:35:18.690221456-05:00","updated_at":"2025-11-02T19:24:29.879810916-06:00","closed_at":"2025-10-14T01:33:23.381877728-05:00"}
{"id":"serviceradar-20","title":"Investigate registry first_seen DB handling","description":"Evaluate performance implications of annotateFirstSeen querying unified_devices per batch; consider DB-native approach per GH-1765 comment.","notes":"Investigated annotateFirstSeen performance. With the chunked lookup (512 ids/query) against versioned_kv, a 50k sweep translates to ~100 Proton queries, all finishing well under the sweep cadence. device_updates retains only 3 days of rows, so we must continue emitting the earliest _first_seen metadata before publishing; otherwise unified_device_pipeline_mv would revert to recent timestamps once older entries age out. Offloading enrichment to Proton would require new streams/materialized views to hydrate metadata and risks reintroducing the Max query size failure we previously resolved. No code changes needed.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-15T22:31:56.930806034-05:00","updated_at":"2025-11-02T19:24:29.922193179-06:00","closed_at":"2025-10-15T22:34:42.518623047-05:00"}
{"id":"serviceradar-21","title":"Bazel build info uses image digest tags","description":"Automate build_info.json updates during Bazel builds so the web UI shows the same sha-based build tag as the pushed container images. Generate the build info from Bazel outputs instead of the static checked-in JSON, and ensure the web image picks up the new metadata.","notes":"Bazel build/push complete: //docker/images:web_image_amd64 now emits build-info.json with sha tags derived from the base image digest and pushes as ghcr.io/carverauto/serviceradar-web:sha-c96994f799c9 (digest sha256:7ab814647ac89d3c18ce988b83dbcff61b250c2afec75ff1624d3a69045a8df5).","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-15T23:05:30.362503289-05:00","updated_at":"2025-11-02T19:24:29.928541536-06:00","closed_at":"2025-10-15T23:13:01.153643366-05:00"}
{"id":"serviceradar-22","title":"Expose core digest in build info","description":"Extend build-info.json to record both web and core image digests (short sha tags) and update the web sidebar to show them. New Bazel genrule should track //docker/images:core_image_amd64.digest so any core rebuild coerces a refreshed web image.","notes":"Final verification: demo web pod now serves /build-info.json with version 1.0.53-pre19, webBuildId=sha-b1204e77d672, coreBuildId=sha-50e80febadf0 after rollout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-15T23:26:29.641527492-05:00","updated_at":"2025-11-02T19:24:30.023144963-06:00","closed_at":"2025-10-15T23:42:59.985107106-05:00"}
{"id":"serviceradar-23","title":"Agent device lacks IP from poller","description":"Investigate GH-1779: demo namespace k8s-agent missing canonical IP/device address. Ensure agent registration populates IP using poller metadata (poller.json). Track fixes including code/test, Bazel rebuild, rollout.","notes":"Post-rollout verification: demo deployment now reports agent device at ClusterIP 10.43.220.55, Armis 12818 entry confirmed not from faker dataset.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-16T00:42:44.733723664-05:00","updated_at":"2025-11-02T19:24:30.18220967-06:00","closed_at":"2025-10-16T01:18:06.982144289-05:00"}
{"id":"serviceradar-24","title":"Observability traces show Invalid Date","description":"GH-1767: Observability dashboard trace table renders \"Invalid Date\" instead of timestamps. Reproduce via https://demo.serviceradar.cloud/observability -\u003e Traces; entries lack valid timestamp. Need frontend fix and regression coverage.","notes":"Normalized Proton trace timestamps before rendering so Observability tables stop showing 'Invalid Date'. Added shared utils (normalizeTraceSummaryTimestamp/resolveTraceTimestampMs), wired TracesDashboard to sanitize both paged + streaming data, and wrote Vitest coverage for nanosecond + fallback handling. Verified with npm run test, npm run lint, make lint, then pushed ghcr.io/carverauto/serviceradar-web:sha-ef12ed1ab7a28c7444242c56c8549c04c525b6f6 (digest sha256:4d78db8e9fb41fed84af34caf6d6298a4132b6a03645dbc48926b28f6132b44d) via bazel run //docker/images:web_image_amd64_push and rolled the demo namespace deployment.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T01:49:37.420257116-05:00","updated_at":"2025-11-02T19:24:30.554148237-06:00","closed_at":"2025-10-16T02:02:54.387409947-05:00"}
{"id":"serviceradar-25","title":"Observability metrics show Invalid Date","description":"Metrics tab in Observability dashboard displays 'Invalid Date' timestamps, similar to traces issue. Need to normalize Proton DateTime64 values, add regression coverage, rebuild/push web image, roll demo.","notes":"Verified Observability metrics timestamps render correctly after client-side normalization fix. Build info now shows ghcr.io/carverauto/serviceradar-web:sha-f2b55efa23fd in demo; no further action required.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T02:04:53.589505051-05:00","updated_at":"2025-11-02T19:24:30.55885471-06:00","closed_at":"2025-10-16T08:44:18.310450791-05:00"}
{"id":"serviceradar-26","title":"db-event-writer loses NATS connection","description":"db-event-writer in demo emits repeated 'nats: connection closed' errors and stops processing events. Need to reproduce, harden reconnect logic (or crash for restart), and verify resilience.","notes":"Hardened db-event-writer JetStream loop: fatal fetch errors now bubble up so the service tears down the old consumer, re-establishes the NATS connection with infinite reconnects, and retries with a bounded backoff. Added unit coverage for both the consumer fatal-error path and the service reconnect loop, then ran 'go test ./pkg/consumers/db-event-writer/...' and 'make lint'. Built/pushed ghcr.io/carverauto/serviceradar-db-event-writer@sha256:ebcae9c408b75528cb4930e9e1164a328538a138579fce8cd7497107670f85c2 and rolled the demo deployment; new pod logs are clean (no repeated 'nats: connection closed').","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T08:51:40.813130347-05:00","updated_at":"2025-11-02T19:24:30.661371245-06:00","closed_at":"2025-10-16T09:15:46.111817669-05:00"}
{"id":"serviceradar-27","title":"serviceradar-core loses NATS connection","description":"Similar to db-event-writer issue serviceradar-26. Core logs show repeated KV publish failures with 'nats: connection closed' (GH-1787). Need to harden reconnect handling or crashloop so supervisors restart the service.","notes":"Rebuilt/pushed serviceradar-core with the refreshed KV client code (digest sha256:50e80febadf02dd55e134a1971769641dde93c1afb048e120d03e582a2e8d8d6) and restarted deployment/serviceradar-core in demo; post-rollout logs show no new 'nats: connection closed' warnings.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T09:26:23.031473125-05:00","updated_at":"2025-11-02T19:24:30.774602362-06:00","closed_at":"2025-10-16T10:05:23.96938382-05:00"}
{"id":"serviceradar-28","title":"serviceradar-core NATS reconnect instability (GH-1790)","description":"Investigate repeated \"nats: connection closed\" errors from serviceradar-core following GH issue 1790. Core should recover automatically after NATS restarts, but logs show the gRPC handlers continuing to fail until the pod is recycled. Need to reproduce, verify current reconnect logic, and harden it so the service either reconnects cleanly or crashes fast.","notes":"Hardened serviceradar-core NATS event publisher: added infinite reconnect options, automatic reinitialization with backoff, and error handling that schedules reinit on publish failures. Added regression test (TestEventPublisherReinitializesAfterConnectionClose) to prove the publisher rebuilds itself after a forced close. Ran go test ./pkg/core/... and make lint. Rebuilt/pushed ghcr.io/carverauto/serviceradar-core:sha-1d6e8e8fb708 (digest sha256:1d6e8e8fb708ae3cf6ffd65baac7e0765d976bb84d86e5c080f4982689bc4830) and rolled the demo deployment; new pod logs show the collector timing out during JetStream startup and then successfully reinitializing the publisher at 15:26:32Z.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T10:12:27.561799668-05:00","updated_at":"2025-11-02T19:24:30.909227765-06:00","closed_at":"2025-10-16T10:27:24.472523358-05:00"}
{"id":"serviceradar-29","title":"NATS JetStream bucket bloat causing restarts","description":"serviceradar-nats restarted 10x due to JetStream restore of KV_serviceradar-kv (~17M keys, 7.1GiB). Need to identify why canonical identity KV grows unbounded, prune/purge current data, and enforce retention (TTL/max bytes) so bucket stays bounded.","notes":"Retention knob finalized at 24h. Configs and cluster rolled as described above; validation commands (nats kv info serviceradar-kv, nats stream report) confirm Maximum Age=1d0h0m0s and an empty KV bucket.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T11:07:15.848535288-05:00","updated_at":"2025-11-02T19:24:30.962567122-06:00","closed_at":"2025-10-16T11:39:26.394486008-05:00"}
{"id":"serviceradar-3","title":"Investigate serviceradar-kv NATS backlog","description":"serviceradar-kv db-writer-service logs show nats connection closed errors when fetching messages: {\"level\":\"error\",\"component\":\"db-writer-service\",\"error\":\"nats: connection closed\",\"time\":\"2025-10-14T06:38:01Z\",\"message\":\"Failed to fetch messages\"}. From serviceradar-tools-59f47ff7bf-prlfj pod, nats stream ls shows events stream holding 48,418 messages (~5.7 GiB) with last message ~0.9s ago, indicating backlog. Need to troubleshoot serviceradar-kv consumption and restore processing.","notes":"Post-sync we drained the events stream after the identity publisher throttling landed. Verified via 'kubectl -n demo exec serviceradar-tools-59f47ff7bf-prlfj -- nats stream info events' that backlog is down to 856 msgs (~11 MiB) and consumers are keeping up.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-14T01:39:41.896667099-05:00","updated_at":"2025-11-02T19:24:31.013681066-06:00","closed_at":"2025-10-14T21:13:50.006965727-05:00"}
{"id":"serviceradar-30","title":"Prune stale identity map keys","description":"## Summary\nNATS KV bucket serviceradar-kv keeps growing even after reducing TTL, because the identity publisher only writes new identity keys and never removes stale ones.\n\n## Findings\n- identityPublisher.Publish iterates all identity keys for the update and blindly calls upsertIdentity, but there is no path that deletes or purges old keys (pkg/registry/identity_publisher.go:183).\n- Each update can introduce new per-identity keys via identitymap.BuildKeys (device id, ip, partition+ip, mac, integration ids); when values change we add a brand new KV entry (pkg/identitymap/identitymap.go:53).\n- Canonical hydration ensures the update carries forward prior metadata, so we have enough information to decide which identities are still valid and which are stale (pkg/registry/identity_resolver.go:244).\n\n## Proposed work\n1. Teach the identity publisher to derive the full set of identity key paths from the existing canonical record, diff against the current update, and delete keys that are no longer present.\n2. Extend the KV client interface to support deletes and update the publisher/tests to cover the prune path.\n3. Add metrics around key deletions so we can monitor churn and confirm the bucket stays bounded once deployed.","notes":"Rolled core build sha-afaee52dd89725f583ebcf49d3741aa1a6ab64f9 to demo and let sync run for several cycles; \"nats --context serviceradar kv info serviceradar-kv\" now reports History=1, Values=57, Size~16MiB with TTL 1d. Bucket growth stayed flat after clearing stale identity keys, so pruning logic looks good.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-16T11:59:03.782152956-05:00","updated_at":"2025-11-02T19:24:31.12994566-06:00","closed_at":"2025-10-16T13:54:57.019801603-05:00"}
{"id":"serviceradar-31","title":"OTEL collector should auto-recover missing trace stream","description":"OTEL publishes started failing with 'no stream found' after stream subjects dropped. Need better recovery so collector recreates stream and reconnects.","notes":"Rebuilt otel image with JetStream recovery logic (BuildBuddy 2a460cdc-ac0d-442b-ba5c-8c55a77b960f, digest ghcr.io/carverauto/serviceradar-otel@sha256:b4a05e4124a5fc74d9d1225c113ebfa818b7850d8ad0db4f25e76a046c6634b9). Rolled demo deployment; startup logs show stream subjects auto-restored and trace publishes succeeding.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T13:06:49.769192234-05:00","updated_at":"2025-11-02T19:24:31.222741451-06:00","closed_at":"2025-10-16T13:07:16.296478488-05:00"}
{"id":"serviceradar-32","title":"serviceradar-agent ICMP invalid argument (GH-1792)","description":"Symptoms: demo namespace agent logs show repeated  with  against hosts (e.g. 10.42.111.75) while sweeping ~50k targets. Need to reproduce, inspect ICMP sweeper send path, and ensure agent handles problematic targets gracefully. Notes:  utility unavailable in agent/tools images due to missing libcap.so.2; may rebuild serviceradar-tools for debugging.","notes":"Investigated the repeated `invalid argument` ICMP errors from the k8s agent; they only happened when the kernel rejected pod/cluster IP destinations with `EINVAL`. Updated the ICMP sweeper to (a) fall back to the packetconn writer only for send errors we can recover from, (b) mark destinations that still return `EINVAL`/`EADDRNOTAVAIL`/`EAFNOSUPPORT` and skip them on subsequent sweeps, and (c) surface the failure once with a WARN instead of logging every sweep. Added unit coverage for the fallback path and the invalid-destination suppression. Ran `go test ./pkg/scan/...` and `make lint`, then rebuilt/pushed ghcr.io/carverauto/serviceradar-agent@sha256:f422bd991dedc98331c7530c7d74051fbf4ff9498677e1d075d9720a92a1dd0d and rolled the demo deployment.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-16T14:00:04.944564045-05:00","updated_at":"2025-11-02T19:24:31.281002065-06:00","closed_at":"2025-10-16T14:31:51.325087361-05:00"}
{"id":"serviceradar-33","title":"Investigate demo device count discrepancy","description":"GH#1817: demo namespace stuck at 17k devices instead of 50k","notes":"Addressed Proton \"Too many partitions\" errors by switching otel_trace_summaries to daily partitions, recreating the stream + MV, and rotating Proton storage. Scaled core/db-event-writer/proton down, dropped their PVCs, reapplied manifests, and rolled deployments. Demo now backfilling cleanly and UI shows ~50k devices.","status":"closed","priority":2,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-20T15:35:03.557749615-05:00","updated_at":"2025-11-02T19:24:31.286702784-06:00","closed_at":"2025-10-20T20:19:13.895668576-05:00"}
{"id":"serviceradar-34","title":"Investigate serviceradar-core restarts in demo namespace","description":"Core deployment in demo namespace has 5 restarts in ~4 hours; gather logs and identify root cause.","notes":"Folded otel_trace_summaries partition fix into consolidated schema (00000000000001) and removed the blocking backfill from 00000000000003. Rebuilt/pushed core image (sha256:e28d7f1a47d6...) and db-event-writer image (sha256:d3ae1db88754...). Rolled Proton with fresh PVC, restarted core solo to run migrations, then brought db-event-writer back up. Core now applies migrations instantly and stays healthy; db-event-writer connects cleanly.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T21:28:40.435571694-05:00","updated_at":"2025-11-02T19:24:31.435247592-06:00","closed_at":"2025-10-23T23:21:02.621403051-05:00"}
{"id":"serviceradar-35","title":"Fix network sweeps page 404 (GH-1784)","description":"Network sweeps tab in UI fails due to missing API route; investigate and patch","notes":"Sweep summary now shows real counts via SRQL stats (total + reachable), built ghcr.io/carverauto/serviceradar-web@sha256:19fe14de96bd8ee7c48240efc19b6247a8a31abd77d684bc6a062056d0b2eb52 and rolled demo.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-20T22:46:22.901301259-05:00","updated_at":"2025-11-02T19:24:31.580453571-06:00","closed_at":"2025-10-23T23:20:40.406509343-05:00"}
{"id":"serviceradar-36","title":"GH-1784: Deploy updated sweep web image","description":"Build/push new serviceradar-web image and roll demo namespace","notes":"Rolled demo serviceradar-web to ghcr.io/carverauto/serviceradar-web@sha256:ef597439d0f79a43f8a0108df9fffab5fd508c6c7e2fdf06fdfb208be80b272b","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-21T10:50:34.873850836-05:00","updated_at":"2025-11-02T19:24:32.266450247-06:00","closed_at":"2025-10-21T10:54:36.334846609-05:00"}
{"id":"serviceradar-37","title":"GH-1840: handle new SRQL timestamp formats","description":"Registry first_seen parser fails when SRQL returns timestamps with spaces/timezone offsets; add normalization workaround and tests.","notes":"Normalized registry timestamp parsing to handle SRQL space/offset formats; added coverage; pushed serviceradar-core sha6aceeafb and rolled demo namespace.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-21T22:40:41.057234693-05:00","updated_at":"2025-11-02T19:24:32.393289842-06:00","closed_at":"2025-10-21T22:47:17.188959438-05:00"}
{"id":"serviceradar-38","title":"GH-1842: identity map unmarshal failure","description":"Investigate and fix core identity map canonical record unmarshalling error from GH issue 1842","notes":"Purged serviceradar-kv bucket, recreated with history=1 TTL=24h, restarted core/agent/sync to regenerate sweep + identity data; bucket repopulated (56 keys after 90s) with no new corrupt warnings in recent logs.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-21T23:30:45.348366323-05:00","updated_at":"2025-11-02T19:24:32.507556867-06:00","closed_at":"2025-10-23T23:20:37.536889568-05:00"}
{"id":"serviceradar-39","title":"GH-1764: cap sync OTEL export size","description":"Investigate oversized OTEL export payloads from serviceradar-sync, add regression test, implement fix, roll demo.","notes":"Rebuilt/pushed images; sync digest now sha256:aedd5288bee1c204683e883dc4812c38b6dbf67a96b5a0c74eaa7d165a9438a6. Demo namespace rolled + sync deployment pinned to new tag.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-22T00:00:04.090550094-05:00","updated_at":"2025-11-02T19:24:32.990786817-06:00","closed_at":"2025-10-22T01:07:30.151878734-05:00"}
{"id":"serviceradar-4","title":"Docker compose login fails with 500 after reboot","description":"Docker compose stack intermittently returns 500s on /auth/login because core restarts while Proton generates DH params and nginx sometimes binds to :8888 when host port 80 is taken, breaking UI access.","notes":"Patched proton init to reuse cached DH params via docker/compose/proton-init.sh and added docs/docs/runbooks/docker-compose-login-500.md so folks know why auth breaks after reboot. Disabled the host nginx service and restarted docker compose nginx so it binds :80 cleanly. Verified `docker compose ps nginx` shows 0.0.0.0:80-\u003e80/tcp and login now routes through Kong without 500s.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T15:35:54.351022351-05:00","updated_at":"2025-11-02T19:24:33.00160238-06:00","closed_at":"2025-10-14T15:36:10.547101164-05:00"}
{"id":"serviceradar-40","title":"GH-1845: fix OTEL log attribute truncation","description":"Investigate why OTEL log attribute lists (e.g. all_ips) are truncated to a single value in the UI. Capture the raw attribute payload coming back from ClickHouse/Timeplus, add automated coverage, and ship a fix so users can copy the full message without losing entries.","notes":"Added otelAttributes parser + regression test so attribute values keep JSON arrays intact. Bazel pushed ghcr.io/carverauto/serviceradar-web:sha-4f530784b13ff3eccad994ea38204159f4e1aef4 and rolled demo serviceradar-web deployment; verified pod is Ready.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-22T11:06:28.909420558-05:00","updated_at":"2025-11-02T19:24:33.065315469-06:00","closed_at":"2025-10-23T23:20:34.267785344-05:00"}
{"id":"serviceradar-41","title":"GH-1853: Accelerate analytics dashboard queries","description":"Baseline dashboard queries are taking multiple seconds to load in the demo environment. Examples:\n- in:otel_trace_summaries time:last_24h sort:duration_ms:desc limit:100 → ~4.5s\n- in:otel_trace_summaries status_code!=1 stats:\"count() as total\" sort:total:desc time:last_24h → ~3s\n- in:otel_trace_summaries stats:\"count() as total\" sort:total:desc time:last_24h → ~3s\n- in:logs severity_text:(fatal,error) time:last_24h sort:timestamp:desc limit:100 → ~2s\n- in:devices time:last_7d limit:500 → several seconds\nNeed to profile and improve query performance so the primary analytics dashboard feels instant.","notes":"Refactored web trace counts to reuse the combined otel_trace_summaries stats query (useTraceCounts now parses sum(if(...)) output via traceCountsUtils), added targeted tests, and shipped ghcr.io/carverauto/serviceradar-web:sha-71f597e11f8d to demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-22T11:28:43.846749485-05:00","updated_at":"2025-11-02T19:24:33.214887002-06:00","closed_at":"2025-10-23T23:20:31.831697022-05:00"}
{"id":"serviceradar-42","title":"GH-1856: Update header SRQL input from view queries","description":"Problem: Network ➜ Sweeps issues an SRQL call via /api/devices/sweep but the header search box keeps showing the default 'in:devices ...' query so users cannot see which filter produced the data.\\nRoot cause: the header owns its own query state and there is no shared channel for feature views to surface the SRQL they execute.\\nPlan: introduce a shared SRQL query context, make the header consume it, push the sweeps view (and other callers) to set the active query before/after they fetch, and cover the state transitions with a small regression test.","notes":"Header search now always routes to /query (even when the active SRQL came from a view). Built/pushed ghcr.io/carverauto/serviceradar-web:sha-04ce6cf6bf9e (digest sha256:9c62606b85cb21e0c2832e03b10128f4d3856892b66f4431bd777db8a101719b) and rolled demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-22T14:07:36.369809295-05:00","updated_at":"2025-11-02T19:24:33.874697278-06:00","closed_at":"2025-10-23T23:20:25.751249972-05:00"}
{"id":"serviceradar-43","title":"Poll sweeps state for discovery dashboard","description":"Add background polling of API/KV to surface active network sweeps in the UI so cards can deep-link even before sweep results arrive.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-22T21:35:29.890615882-05:00","updated_at":"2025-11-02T19:24:33.940864481-06:00","closed_at":"2025-10-23T23:20:59.701847056-05:00"}
{"id":"serviceradar-44","title":"GH-1864: sync sweep chunk write fails","description":"JetStream rejects sweep chunk writes once payloads exceed its block size (see GH-1864 logs). Need to cap chunk payloads and cover with regression test.","notes":"Closing out legacy sweep chunk investigation so we can focus on GH-1844; no pending work will be lost.","status":"closed","priority":1,"issue_type":"bug","assignee":"mfreeman451","created_at":"2025-10-23T01:06:43.008337884-05:00","updated_at":"2025-11-02T19:24:34.003430524-06:00","closed_at":"2025-10-23T23:20:21.398025672-05:00"}
{"id":"serviceradar-45","title":"GH-1867: Expand Proton PVC capacity","description":"Proton local-path PVC out of space in demo; need manifest bump + resize script.","notes":"Bumped Proton PVC request to 1Ti with explicit local-path storage class, added resize script for safe scale-down/recreate, and refreshed deployment guide persistence table.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-10-23T02:07:09.034290879-05:00","updated_at":"2025-11-02T19:24:34.153083552-06:00","closed_at":"2025-10-23T23:20:55.610610015-05:00"}
{"id":"serviceradar-46","title":"GH-1863 device details availability","description":"Goal: enrich the device detail page with full registry metadata, sysmon/ICMP metrics, and an availability timeline derived from historical device updates. Tasks: [ ] audit current API responses vs registry metadata; [ ] expose availability history endpoint in Go API leveraging versioned device data; [ ] expand Next.js device page to render metadata panes, metrics summaries, and uptime chart; [ ] validate via make lint/test then ship Bazel image + demo rollout.","notes":"Implemented SRQL-driven device detail view: queries unified_devices/device_updates/timeseries_metrics to hydrate metadata, availability timeline, and Sysmon summaries. Added uptime timeline visualization, telemetry cards, and SRQL-backed metric chart. Verified with npm run lint \u0026\u0026 npm test -- --watch=false.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-23T02:44:12.815226611-05:00","updated_at":"2025-11-02T19:24:34.876881582-06:00","closed_at":"2025-10-23T23:20:52.649453568-05:00"}
{"id":"serviceradar-47","title":"GH-1869 device inventory spinner","description":"- Reviewed GH issue 1869; navigation away leaves SRQL context tied to prior view so devices dashboard never fetches.\\n- Repro: open Devices, switch to another tab (Network/Analytics), return to Devices -\u003e spinner never resolves.\\n- Plan: make Devices dashboard reclaim SRQL view when active, ensure query runs, add regression coverage.","notes":"- Sweeps view now queries via /api/query using buildSweepQueryWithLimit, including auth cookies; old /api/devices/sweep dependency removed.\\n- Bazel-built and deployed ghcr.io/carverauto/serviceradar-web:sha-81c5c53925a8 after lint/tests.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-23T10:02:15.668346327-05:00","updated_at":"2025-11-02T19:24:34.990481926-06:00","closed_at":"2025-10-23T23:20:47.76076068-05:00"}
{"id":"serviceradar-48","title":"DataService JetStream API","description":"## Summary\n- carve object-store functionality out of KVService into a dedicated DataService gRPC surface\n- rename the kv binary to cmd/data-services and serve both KVService + DataService from the same process\n- add JetStream object bucket support plus RBAC wiring while keeping existing KV semantics intact\n\n## What’s done\n- added proto/data_service.proto and generated Go stubs; trimmed proto/kv.proto back to pure KV RPCs\n- moved the binary to cmd/data-services, registered both services during startup, and updated Bazel/packaging/Docker references\n- extended pkg/kv (server, NATS store, config) to expose DataService RPCs and manage JetStream objects with sensible defaults\n- refreshed mocks/interfaces in pkg/kv and pkg/sync to reflect the new service boundary without forcing callers to use it yet\n- gofmt/go test ./... clean after refactor\n\n## Next steps\n- update downstream clients (sync, web, tools) to call DataService for object payloads when ready\n- document the new gRPC contract + curl examples in docs/docs/ or beads for rollout\n- consider Bazel targets or Helm chart changes if we want a distinct data-service deployment in the future","notes":"Datasvc crash fixed: rebuilt pushed ghcr.io/carverauto/serviceradar-datasvc:sha-d37e173ef80b4b3d858dfa26c247e15d955dfe12 (sha256:a25f1de89075b971c70fdbafb72a62ae958ef4c7daa49cd751133999bc5d6525) and rolled the deployment; pod now initializes mTLS with role=datasvc and the sweep metadata watch is active.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-23T15:23:38.213335797-05:00","updated_at":"2025-11-02T19:24:34.998014913-06:00","closed_at":"2025-10-23T23:20:45.539246777-05:00"}
{"id":"serviceradar-49","title":"Verify pkg/datasvc refactor","description":"## Context\n- pkg/kv was renamed to pkg/datasvc using IDE tooling (GoLand)\n- BUILD.bazel adjusted, but residual references may remain\n\n## Remaining Work\n- Audit repo for lingering pkg/kv import paths, Bazel targets, go:generate directives, CI scripts\n- Ensure gofmt/goimports applied where necessary and module state is clean\n- Run go test ./... and Bazel builds for cmd/data-services + datasvc image\n- Update docs/comments still mentioning pkg/kv\n\n## Validation\n- go test ./...\n- bazel build --config=remote //cmd/data-services:data_services //docker/images:datasvc_image_amd64\n- Optional: go build ./cmd/...","notes":"Handled datasvc health checks: external checker now returns generic health JSON instead of calling monitoring.AgentService, set kv registry entry to use the 'datasvc' gRPC health service, regenerated mocks, ran go test ./... and bazel build/push for agent image (ghcr.io/carverauto/serviceradar-agent:sha-045dcef4dbeb) and rolled the deployment.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-23T19:37:51.134847609-05:00","updated_at":"2025-11-02T19:24:35.15522976-06:00","closed_at":"2025-10-23T23:20:43.345909763-05:00"}
{"id":"serviceradar-5","title":"Flowgger fails hard when NATS is unreachable","description":"Flowgger exits with a panic when it cannot reach NATS (Connection refused from nats_output.rs:139). When running in demo/K8s the pod churns instead of retrying. Decide whether to crash fast so the pod restarts immediately or add exponential backoff with limits to reconnect to NATS.","notes":"Implemented exponential backoff for Flowgger's NATS output (cmd/flowgger/src/flowgger/output/nats_output.rs) with configurable retry knobs and rolled fresh defaults into the sample configs. Rebuilt and pushed ghcr.io/carverauto/serviceradar-flowgger@sha256:175dd739b1b832a75a58c73bf528d10e541792e596e7e50398eec5c92e704322 so demo env picks up the change.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T15:36:29.941341235-05:00","updated_at":"2025-11-02T19:24:35.835202692-06:00","closed_at":"2025-10-14T16:06:05.148133185-05:00"}
{"id":"serviceradar-50","title":"GH-1844: restore observability logs service filter","description":"Logs UI lost the service dropdown contents after SRQL refactor; need to repopulate using in:services via /api/query.","notes":"Ported logs service picker to query in:services via SRQL, added extractor util + tests, and verified web lint/test.","status":"in_progress","priority":1,"issue_type":"bug","created_at":"2025-10-23T23:21:38.865242835-05:00","updated_at":"2025-11-02T19:24:35.88415527-06:00"}
{"id":"serviceradar-51","title":"GH-1844: restore observability logs service filter","description":"Logs UI lost the service dropdown contents after SRQL refactor; need to repopulate using in:services via /api/query.","notes":"Plan: 1) inspect current logs service picker + SRQL wiring, 2) query in:services via /api/query to provide dropdown options, 3) ensure selecting a service updates SRQL/filter and persists across navigation, 4) add regression coverage, rebuild/push web image, roll demo.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-23T23:27:38.87161976-05:00","updated_at":"2025-11-02T19:24:35.891155831-06:00","closed_at":"2025-10-24T00:26:30.127570521-05:00"}
{"id":"serviceradar-52","title":"Finalize SPIRE service exposure and packaging","description":"- Switch SPIRE service to managed ingress for edge agents\\n- Decide namespace layout to bundle SPIRE with ServiceRadar installs (Helm, demo)\\n- Document Helm/automation expectations and update manifests accordingly\\n- Track alignment with GH-1892","notes":"Identified postStart hook failure: spire-server image lacks /bin/sh, so lifecycle registration script crashed the pod and bubbled up as k8s_sat context-canceled errors. Removed the hook and reapplied the stack; server now healthy and agents attest. Need replacement automation to seed registration entries.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-25T18:48:02.991588963-05:00","updated_at":"2025-11-02T19:24:36.046846312-06:00","closed_at":"2025-10-27T02:16:35.959815385-05:00","labels":["infra","spire"]}
{"id":"serviceradar-53","title":"Integrate nested SPIRE server into poller","description":"Goals:\\n- Embed a downstream SPIRE server/agent pair inside serviceradar-poller for edge deployments.\\n- Wire poller lifecycle to bootstrap the nested server and expose a workload socket for local services.\\n- Ensure documentation and packaging cover the nested topology and fallback modes.\\n\\nInitial tasks:\\n1. Inventory current poller SPIFFE usage and identify bootstrap requirements.\\n2. Prototype nested SPIRE server configuration (upstream auth, socket sharing) and determine runtime packaging changes.\\n3. Add observability/tests to validate certificate issuance when the poller is disconnected from the core.\\n4. Update demo/Helm manifests to optionally enable nested mode.","notes":"Opened external endpoints and rewired the edge workflow: datasvc and SPIRE now have LoadBalancer services (23.138.124.23:50057, 23.138.124.18:18081) and the compose helper updates poller/agent configs to use SPIFFE sockets. The controller config/Helm now set entryIDPrefix=k8s. so it ignores external entries, letting us mint a join-token-based downstream entry for the nested server. Local Docker stack hits Core/KV and shares /run/spire/nested with the agent, but the upstream SPIRE handshake still fails with PermissionDenied: no identity issued—manual entry survives, yet the join-token attestation never returns an SVID, so the poller falls back to mTLS. Need to debug the upstream Workload API path or automate token/entry creation so the nested agent can complete attestation and expose the workload socket.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-27T02:16:59.216991572-05:00","updated_at":"2025-11-02T19:24:36.162146197-06:00","closed_at":"2025-10-28T15:18:01.301541473-05:00"}
{"id":"serviceradar-54","title":"Secure edge poller onboarding flow","description":"Track design/implementation for Core-managed edge poller enrollment. See https://github.com/carverauto/serviceradar/issues/1903 for the PRD and task list.","design":"## Proposed Architecture\n- Persist onboarding artifacts in new Proton tables so Core can track issuance, download, activation, and revocation:\n  - `edge_onboarding_packages` (ReplacingMergeTree, no TTL) keyed by `package_id` (UUID). Columns include human label/site, `poller_id`, derived downstream SPIFFE ID, encrypted join token blob, join token expiry, downstream entry id, package status enum, `download_token_hash`, download expiry, metadata JSON, `created_by`, timestamps (`created_at`, `delivered_at`, `activated_at`, `revoked_at`), and optional activation context (source IP, last seen SVID).\n  - `edge_onboarding_events` (MergeTree) for audit trail with columns: `package_id`, `event_type` (issued/downloaded/activated/revoked/expired), actor, source IP, message JSON, timestamp. No TTL so security reviewers can query history.\n\n- Extend `models.CoreServiceConfig` with `edge_onboarding` block: `enabled` flag, default downstream selectors, SPIFFE path template (e.g. `spiffe://{trust_domain}/ns/edge/{poller_id}`), default token/join TTLs, download TTL, optional default poller id prefix, and `encryption_key_base64` used to wrap join tokens + bundles. Validation ensures key decodes to 32 bytes (AES-256-GCM).\n\n- Introduce `pkg/crypto/secrets` helper that wraps AES-GCM with per-record random nonce + HMAC for integrity. Provide `Encrypt([]byte)` / `Decrypt([]byte)` returning base64 strings so we can persist ciphertext in Proton. Key supplied from config and cached in Core.\n\n- Build a dedicated onboarding service inside Core (`pkg/core/edgeonboarding`) that owns package lifecycle:\n  - `CreatePackage(ctx, req)` uses `spireadmin.Client` to mint join token (`agentv1.CreateJoinToken`), create downstream entry with selectors, and fetch the upstream x509 bundle via `bundle/v1.BundleClient` (X.509 Roots). Token + bundle stored encrypted.\n  - Generate a random `download_token` (24 bytes) saved as SHA-256 hash; plaintext returned once in the POST response.\n  - Persist package + `issued` event in DB. Service returns metadata plus download URL template.\n  - `RecordDownload` flips status to `delivered`, timestamps the event, and logs actor/IP.\n  - `MarkActivated` marks package active when Core sees a status report from the poller (matching poller ID or downstream SPIFFE ID). Activation event stores poller source IP + SVID (if presented).\n  - `RevokePackage` deletes downstream entry via `entryv1.BatchDeleteEntry` and marks status revoked. If token still valid, we simply invalidate future downloads and update status to `revoked`.\n  - `ExpirePackages` cron: background goroutine sweeps for expired tokens/download tokens and updates status to `expired`, revoking downstream entries when appropriate.\n\n- Modify `db.Service` with CRUD helpers: create/list/update packages, append audit events, query active poller IDs, mark activation. Implementations live in new files under `pkg/db/edge_onboarding.go` using Proton SQL with `INSERT`/`ALTER TABLE ... UPDATE` semantics.\n\n- Update Core startup to hydrate a runtime cache of active pollers from DB and hand it to the API layer and `Server.isKnownPoller`. Replace the hard-coded list check with `config.KnownPollers ∪ activePackages`. Provide watch method (`EdgeOnboardingService.Subscribe`) so new activations push into the cache without restart.\n\n- API surface:\n  - `POST /api/admin/edge-packages` (admin role) accepts JSON `{label, poller_id?, site?, ttl_seconds?, download_ttl_seconds?, selectors?, metadata}`. Validates uniqueness, derives poller ID if absent (e.g. slugified label with prefix), and returns `201` with metadata + `download_token` (shown once) and poller config preview (core address, kv address, etc.).\n  - `GET /api/admin/edge-packages` lists packages with filters (status, created_by, date range, poller id) and aggregates event history for UI chips.\n  - `GET /api/admin/edge-packages/{id}` provides detail including audit log (without revealing token).\n  - `GET /api/admin/edge-packages/{id}/download?token=...` streams a tar.gz assembled on-demand: `README.txt`, `metadata.json`, `edge-poller.env` prefilled with Core/KV/LB info + poller id, plus `spire/upstream-join-token` and `spire/upstream-bundle.pem`. Valid token is required; on success we emit a `downloaded` event and mark delivered.\n  - `POST /api/admin/edge-packages/{id}/revoke` (idempotent) revokes the downstream entry, updates status, and records audit event. Revoked packages cannot be downloaded again.\n\n- Extend gRPC ReportStatus path: when an unknown poller reports, check pending onboarding packages. If the poller_id matches an issued/delivered package (and join token still valid), flip status to `activated`, append audit event, and add poller id to active set so future reports pass `isKnownPoller`. Optionally capture SPIFFE principal from the TLS peer via `peer.FromContext` when using SPIRE security provider.\n\n- CLI + scripts:\n  - Add `serviceradar-cli edge package create/list/download/revoke` commands that wrap the new HTTP APIs for operators who prefer terminal workflows.\n  - New installer helper `docker/compose/edge-poller-install.sh` that accepts the package archive, verifies expiry timestamps, unpacks artifacts into `docker/compose/spire/` + `edge-poller.env`, and invokes `edge-poller-restart.sh --skip-refresh --env-file ...`. Script aborts if join token expired or package status ≠ activated.\n\n- Web UI:\n  - Add \"Edge Onboarding\" admin section (`/admin/edge`) showing table of packages (status badges, expiry, site, poller ID) with actions for create/download/revoke and audit timeline drawer.\n  - Creation modal collects label/site/TTL/selectors, surfaces generated poller ID + SPIFFE ID, and provides copy/download button for the package archive + instructions for installers.\n  - Integrate with existing auth/role guard (admin only) and reuse toast/confirmation components.\n\n- Observability \u0026 security:\n  - Emit structured logs + metrics (`edge_onboarding_packages_total`, `edge_onboarding_package_state`) so we can alert on high pending counts or failed revocations.\n  - Require `config:write` scope (or admin role) for issuance endpoints; download endpoint checks session + token. Ensure responses set `Cache-Control: no-store` and mask token in logs.\n  - Document operational playbooks under `docs/docs/edge-onboarding.md` covering issuance flow, revocation, and installer steps.","notes":"Documented the end-to-end edge onboarding flow in docs/docs/edge-onboarding.md (poller issuance, metadata requirements, Docker restart) and linked it from the Docker setup guide. Captured the upcoming agent/checker onboarding expectations so we can wire KV updates + package types in the next sprint.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-28T15:18:46.365672928-05:00","updated_at":"2025-11-02T19:24:36.195349282-06:00","closed_at":"2025-10-29T01:01:01.519318022-05:00"}
{"id":"serviceradar-55","title":"serviceradar edge onboarding multi-component automation","description":"Implement the agent/checker onboarding enhancements described in GH-1909: extend the edge onboarding API/UI to handle poller/agent/checker types, enforce parent associations, and apply KV updates when packages are issued. Include CLI updates and activation telemetry.","acceptance_criteria":"Operators can issue agent and checker packages linked to parents and observe automatic KV updates once components activate.","notes":"Rebuilt db-event-writer via Bazel and rolled demo to ghcr.io/carverauto/serviceradar-db-event-writer:sha-53c1d7abe00759ab640b6c710b444b139a491209. API smoke test still hits Proton 49 errors (Cannot clone block with columns because block has 35 columns, but 24 columns given) when POST /api/admin/edge-packages runs, so rows disappear immediately and downloads 404. Need to instrument the core/db writer path to confirm which insert is still emitting the legacy 24-column shape.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-29T01:01:10.444582861-05:00","updated_at":"2025-11-02T19:24:36.202198046-06:00","closed_at":"2025-10-30T18:32:12.464392249-05:00"}
{"id":"serviceradar-56","title":"Edge onboarding e2e validation for docker poller and agent","description":"We need to exercise the new edge onboarding path end-to-end using the demo cluster APIs and a local docker-compose stack. Steps: 1) Pull API key/bearer token from demo namespace secrets, issue poller package via /api/admin/edge-packages, and bootstrap a docker poller locally. 2) Repeat for agent, ensuring it binds to the docker poller. 3) Verify core/device registry surfaces the new docker poller/agent devices in inventory; investigate merge/clobber behavior where multiple services from the same IP are being collapsed incorrectly. Expect follow-up changes in core registry if inventory does not retain both devices.","notes":"Final Summary: Successfully debugged edge onboarding e2e. Issues resolved: 1) Poller config fixed to use localhost:50051 for agent in shared network namespace. 2) SPIRE upstream uses LoadBalancer IP 23.138.124.18:18081 instead of k8s DNS. 3) K8s agent deployed with sha-657e49c0, connects to all services. Known limitation: Downstream nested SPIRE agent needs manual join token (documented for future automation). Updated docs/docs/edge-onboarding.md with troubleshooting guide, bootstrap instructions, and required env modifications. All core acceptance criteria met.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-30T18:36:29.179795581-05:00","updated_at":"2025-11-02T19:24:36.276587365-06:00","closed_at":"2025-10-31T11:54:45.785863931-05:00"}
{"id":"serviceradar-57","title":"Create common onboarding library to eliminate edge deployment friction","description":"## Problem Statement\n\nThe current edge onboarding process requires extensive manual intervention through shell scripts, kubectl commands, and configuration manipulation. This creates significant friction for edge deployments and doesn't scale for production use.\n\n**Current Pain Points:**\n- Multiple shell scripts required (setup-edge-e2e.sh, edge-poller-restart.sh, setup-edge-poller.sh, refresh-upstream-credentials.sh)\n- Manual poller registration via kubectl and KV updates\n- Manual DNS to IP address conversion for Docker deployments\n- No simple API/CLI for package management (authentication issues)\n- Join token expiration handling is manual\n- Complex multi-step process prone to errors\n\nSee detailed analysis in: docker/compose/edge-e2e/FRICTION_POINTS.md\n\n**IMPORTANT SCOPE**: This is ONLY for edge service onboarding. k8s and main docker-compose stack already have working SPIFFE enrollment via SPIFFE controller and CRDs - those are NOT being changed.\n\n## Vision\n\nCreate a **common onboarding library** (pkg/edgeonboarding) that edge ServiceRadar components (agent, poller, checkers) can use to automatically onboard using a simple token-based flow.\n\n### Desired User Experience\n\n**For Edge Poller/Agent:**\n```bash\n# Create package in UI or via CLI\nserviceradar-cli edge create-package --name \"Remote Site A\" --type poller\n\n# Deploy with just the token - everything else is automatic\ndocker run -e ONBOARDING_TOKEN=\u003ctoken\u003e ghcr.io/carverauto/serviceradar-poller:latest\n```\n\n**For Edge Checkers:**\n```bash\n# Create checker package\nserviceradar-cli edge create-package --name \"Remote Checker\" --type checker\n\n# Deploy checker with token\ndocker run -e ONBOARDING_TOKEN=\u003ctoken\u003e ghcr.io/carverauto/serviceradar-checker:latest\n```\n\nThe library handles everything:\n- Downloads package from Core\n- Extracts and configures SPIRE credentials (nested SPIRE for edge, not k8s SPIRE)\n- Auto-registers with Core via KV (datasvc)\n- Generates service configuration\n- Handles credential rotation\n- Starts service\n\n## Proposed Architecture\n\n### 1. Common Onboarding Library\n\n**Package**: pkg/edgeonboarding\n\n**Core Interface**:\n```go\ntype OnboardingConfig struct {\n    Token              string   // Onboarding token from package\n    CoreEndpoint       string   // Optional: auto-discovered from package\n    KVEndpoint         string   // Bootstrap KV address (sticky config)\n    ServiceType        string   // \"agent\", \"poller\", \"checker\"\n    ServiceID          string   // Optional: readable name override\n    StoragePath        string   // Where to persist config/credentials\n    DeploymentType     string   // \"docker\", \"bare-metal\" (auto-detect if not provided)\n}\n\nfunc Bootstrap(ctx context.Context, cfg OnboardingConfig) error {\n    // 1. Download package from Core using token\n    // 2. Validate and extract package contents\n    // 3. Configure nested SPIRE for edge (NOT k8s SPIRE)\n    // 4. Auto-register with Core via KV/database\n    // 5. Generate service config based on deployment type\n    // 6. Set up credential rotation\n    // 7. Return ready-to-use config\n}\n\nfunc Rotate(ctx context.Context) error {\n    // Handle SPIRE credential rotation\n}\n```\n\n**Deployment Type Detection:**\n- Automatically detect if running in Docker or bare metal\n- Use appropriate addresses (LoadBalancer IPs for Docker accessing k8s services)\n- Configure nested SPIRE appropriately for edge environment\n\n**Sticky vs Dynamic Config:**\n- **Sticky (static config file)**: KV address, Core address (chicken/egg - need these to bootstrap)\n- **Dynamic (from KV)**: Everything else - checker configs, known pollers list, etc.\n\n### 2. Automatic Poller Registration via KV\n\n**KV/Database-based approach** (NOT ConfigMaps):\n\n- Modify isKnownPoller() in Core to check edge_packages table via KV\n- Add allowed_poller_id column to edge_packages table\n- Package creation sets: allowed_poller_id = poller_id_override OR component_id\n- Packages with status in [Issued, Delivered, Activated] are automatically allowed\n- Services read allowed poller list from KV, not ConfigMaps\n- No restarts needed\n\n**Key Point**: ConfigMaps are legacy. Everything dynamic should be in KV (datasvc).\n\n### 3. Separate Docker Compose Stacks\n\n**Strategy**: Keep main stack clean, create dedicated edge stacks\n\n**edge-poller-stack.compose.yml** (already exists as poller-stack.compose.yml):\n- Poller with nested SPIRE server (edge deployment)\n- Agent sharing network namespace with poller\n- Optional checkers\n- Uses onboarding library\n- For edge sites needing full agent + poller deployment\n- Uses nested SPIRE, NOT k8s SPIRE\n\n**edge-checker-stack.compose.yml** (new):\n- Just checkers, no poller/agent\n- Lighter weight authentication\n- For edge sites that only need checker deployment\n- Can run in container or bare metal\n- Uses onboarding library\n\n**main docker-compose.yml**:\n- UNCHANGED - no edge onboarding logic\n- Uses SPIFFE controller and CRDs for automatic enrollment\n- Used for local development and trusted environments\n- NOT related to this work\n\n**k8s deployments**:\n- UNCHANGED - SPIFFE controller and CRDs handle enrollment automatically\n- NOT related to this work\n\n### 4. Simplified Package Management\n\n**Add CLI commands** (extends existing serviceradar-cli):\n```bash\nserviceradar-cli edge create-package --name \"Remote Site A\" --type poller --spiffe-id \"spiffe://carverauto.dev/ns/edge/site-a-poller\"\nserviceradar-cli edge list-packages [--status issued|delivered|activated|revoked]\nserviceradar-cli edge revoke-package \u003cid\u003e\nserviceradar-cli edge delete-package \u003cid\u003e\nserviceradar-cli edge download-package \u003cid\u003e --output ./package.tar.gz\n```\n\n**Fix API authentication**:\n- Document default admin credentials\n- Add token-based API access for automation\n- Consider service account tokens for CI/CD\n\n## Implementation Plan\n\n### Phase 1: Core Library (Week 1-2)\n- Create pkg/edgeonboarding package\n- Implement package download and validation\n- Implement nested SPIRE credential handling for edge\n- Implement deployment type detection (docker/bare-metal)\n- Add configuration generation (sticky bootstrap + dynamic from KV)\n- Unit tests for all components\n\n### Phase 2: KV-backed Registration (Week 2)\n- Add allowed_poller_id column to edge_packages table\n- Update package creation to populate allowed_poller_id\n- Modify isKnownPoller() to check KV/database\n- Ensure Core reads known pollers from KV, not ConfigMaps\n- Add migration path for existing pollers\n- Integration tests\n\n### Phase 3: Service Integration (Week 3)\n- Update edge poller to use onboarding library\n- Update edge agent to use onboarding library\n- Update edge checkers to use onboarding library\n- Add ONBOARDING_TOKEN environment variable support\n- E2E tests for each service type\n- DO NOT modify k8s or main docker-compose deployments\n\n### Phase 4: Stack Reorganization (Week 3-4)\n- Create edge-checker-stack.compose.yml\n- Clean up edge-poller-stack.compose.yml to use library\n- Ensure main docker-compose.yml has no edge logic\n- Document differences between edge stacks and main stack\n- Update all edge deployment guides\n\n### Phase 5: CLI and API Improvements (Week 4)\n- Add serviceradar-cli edge commands\n- Fix API authentication issues\n- Document API endpoints with Swagger\n- Add service account token support\n- Update Web UI for edge package management\n\n### Phase 6: Cleanup and Documentation (Week 4-5)\n- Remove old shell scripts (setup-edge-e2e.sh, etc.)\n- Update all edge documentation\n- Create migration guide for existing edge deployments\n- Add troubleshooting guide\n- Record demo video\n\n## Success Criteria\n\n**Deployment Simplicity:**\n- Edge deployment requires only onboarding token (no shell scripts)\n- No manual kubectl commands required\n- No manual ConfigMap or KV updates\n- Works across Docker and bare metal edge deployments\n\n**Automation:**\n- Automatic poller registration via KV\n- Automatic deployment type detection\n- Automatic configuration generation (bootstrap sticky + dynamic from KV)\n- Automatic credential rotation\n\n**Scalability:**\n- Can deploy hundreds of edge sites without manual intervention\n- Package management via CLI and API\n- No Core restarts required for new edge pollers\n\n**Code Quality:**\n- Main docker-compose stack has zero edge onboarding code\n- k8s deployments unchanged (SPIFFE controller still works)\n- Clear separation between trusted (k8s/main) and untrusted (edge) environments\n- Comprehensive tests (unit, integration, e2e)\n- Well-documented with examples\n\n**Architecture:**\n- All dynamic config in KV (datasvc), NOT ConfigMaps\n- Only bootstrap/sticky config in static files (KV address, Core address)\n- Nested SPIRE for edge, separate from k8s SPIRE\n\n## Out of Scope (Future Work)\n\n- Automatic SPIRE join token rotation (currently requires manual regeneration after 15 minutes)\n- Multi-region package distribution\n- Edge deployment health monitoring dashboard\n- Automatic version upgrades for edge deployments\n- Advanced networking (VPN, NAT traversal, etc.)\n- Changes to k8s SPIFFE enrollment (already works)\n- Changes to main docker-compose SPIFFE enrollment (already works)\n\n## Related Issues\n\n- Closes: serviceradar-56 (E2E validation - completed)\n- Addresses all friction points documented in docker/compose/edge-e2e/FRICTION_POINTS.md\n\n## Files Impacted\n\n**New:**\n- pkg/edgeonboarding/ - Core library\n- docker/compose/edge-checker-stack.compose.yml - New checker-only stack for edge\n\n**Modified:**\n- pkg/db/edge_packages.go - Add allowed_poller_id column\n- pkg/core/pollers.go - Update isKnownPoller() to check KV/database\n- docker/compose/edge-poller-stack.compose.yml - Use onboarding library (edge only)\n- Edge services (poller, agent, checkers) - Integrate onboarding library\n\n**NOT Modified:**\n- k8s deployments - SPIFFE controller continues to work\n- docker-compose.yml (main) - No edge logic added\n- k8s SPIRE configuration - Not touched\n\n**Removed (after migration):**\n- docker/compose/setup-edge-e2e.sh\n- docker/compose/edge-poller-restart.sh\n- docker/compose/setup-edge-poller.sh\n- docker/compose/refresh-upstream-credentials.sh\n- docker/compose/edge-e2e/ directory (documentation moves to main docs)","status":"open","priority":1,"issue_type":"feature","created_at":"2025-10-31T15:51:59.163532394-05:00","updated_at":"2025-11-02T19:24:36.424351017-06:00"}
{"id":"serviceradar-58","title":"Device detail page 404 errors on Next.js API routes","description":"## Problem\n\nDevice detail pages and device deletion functionality returning 404 errors from nginx ingress despite Next.js routes being correctly implemented and compiled.\n\n## Symptoms\n\n1. **Device detail page**: GET `/api/devices/core%3A0.0.0.0/registry` returns 404\n2. **Device deletion**: DELETE `/api/devices/core%3A0.0.0.0` returns 404\n3. No requests reach the serviceradar-web pod (no logs)\n4. Browser shows 404 errors consistently\n\n## Investigation Results\n\n### ✅ Confirmed Working\n\n1. **Routes exist in Next.js build**:\n   - `/app/.next/server/app/api/devices/[id]/route.js` (GET and DELETE handlers)\n   - `/app/.next/server/app/api/devices/[id]/registry/route.js` (GET handler)\n\n2. **Routes work from inside pod**:\n   ```bash\n   # GET device - returns 401 (route exists, needs auth)\n   kubectl exec -n demo deployment/serviceradar-web -- curl -s -o /dev/null -w \"%{http_code}\" http://127.0.0.1:3000/api/devices/core:0.0.0.0\n   \n   # DELETE device - returns 401 (DELETE handler exists)\n   kubectl exec -n demo deployment/serviceradar-web -- curl -X DELETE -s -o /dev/null -w \"%{http_code}\" 'http://127.0.0.1:3000/api/devices/core%3A0.0.0.0'\n   \n   # GET registry - returns 401 (registry route exists)\n   kubectl exec -n demo deployment/serviceradar-web -- curl -s -o /dev/null -w \"%{http_code}\" http://127.0.0.1:3000/api/devices/core:0.0.0.0/registry\n   ```\n\n3. **Ingress routing appears correct**:\n   - `/api/devices` → `serviceradar-web:3000` (pathType: Prefix)\n   - Longer prefix should win over generic `/api` route\n\n### ❌ Issues Found\n\n1. **Requests don't reach web pod**: No logs in serviceradar-web for these requests\n2. **404 from browser**: Consistently returning 404, not 401\n3. **Nginx ingress blocking**: Requests appear to be blocked/rejected at nginx layer\n\n## Attempted Fixes\n\n1. ✅ Added DELETE handler to web/src/app/api/devices/[id]/route.ts\n2. ✅ Fixed authentication token passing in DeviceDetail.tsx\n3. ✅ Rebuilt and deployed web image: sha256:d1738c5c78e7e062af9ef9e04a96350fffa0ad92500b70ec0ea135c33c6a34da\n4. ✅ Restarted serviceradar-web deployment\n5. ✅ Restarted nginx ingress daemonset (12 controllers)\n6. ❌ Still getting 404 errors from browser\n\n## Files Modified\n\n1. web/src/app/api/devices/[id]/route.ts - Added DELETE handler (lines 85-142)\n2. web/src/app/api/devices/[id]/registry/route.ts - Removed incorrect DELETE handler\n3. web/src/components/Devices/DeviceDetail.tsx - Fixed auth token in SRQL queries\n\n## Current Status\n\nBLOCKED: Routes work internally but nginx ingress is returning 404 before requests reach the pod. Root cause unclear.\n\n## Possible Causes\n\n1. Nginx ingress routing issue - Path matching not working as expected\n2. WAF/Security policy - Blocking certain request patterns or methods\n3. Nginx cache - Despite restart, some cache layer might persist\n4. CDN layer - Unknown proxy/CDN in front of demo.serviceradar.cloud\n5. Ingress annotation issue - Some annotation blocking these specific paths","status":"open","priority":1,"issue_type":"bug","created_at":"2025-11-02T01:49:59.501160735-06:00","updated_at":"2025-11-02T01:49:59.501160735-06:00"}
{"id":"serviceradar-6","title":"Identity publisher churns KVService and floods otel logs","description":"Otel perf telemetry is logging proto.KVService/Get and Update spans constantly for serviceradar-core even when device records are unchanged. Registry's identity publisher issues a Get on every DeviceUpdate, causing duplicate RPCs and otel spam. We need to cache canonical identity writes locally so unchanged updates short-circuit and CAS revisions are reused.","acceptance_criteria":"Core identity publishes skip redundant KV RPCs and otel perf logs no longer emit every few milliseconds for proto.KVService/Get+Update.","notes":"Trimmed OTEL perf metrics to export only when spans exceed the slow threshold, cutting batch sizes from 512 to 2 spans per export. Rebuilt/pushed otel collector (sha256:dc3b9a9da7089b1bb3c1259531e5806e637413895ecccb8b70a213ed60517dbf) and rolled the demo deployment; collector logs now show 2-span batches instead of 512.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T16:27:45.671978374-05:00","updated_at":"2025-11-02T19:24:36.56299779-06:00","closed_at":"2025-10-14T16:28:15.408673518-05:00"}
{"id":"serviceradar-7","title":"Investigate OTEL trace surge and dashboard mismatch","description":"Observability widget shows ~791k traces while observability dashboard reports ~29.8k; Proton telemetry DB already back above 578k traces shortly after reset. Need to identify which services are emitting high-volume KV spans, shut off noisy OTEL instrumentation around KV calls if appropriate, and reconcile dashboard counts.","notes":"Hooked analytics widget and /observability cards into the shared trace-count hook so both read the same 24h aggregates. Bazel rebuilt \u0026 pushed ghcr.io/carverauto/serviceradar-web@sha256:3e1e90760afda36a545526f756ce414a6f7741960d4e14411b091bfdca0d0a9a and I rolled the demo serviceradar-web deployment to that digest; the new pod is healthy. Dashboards now show matching trace totals after cache refresh.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T18:18:43.577346219-05:00","updated_at":"2025-11-02T19:24:36.573531286-06:00","closed_at":"2025-10-14T20:56:07.477820626-05:00"}
{"id":"serviceradar-8","title":"Prevent JWKS rotation after core restarts","description":"Core init container overwrote /var/lib/serviceradar/core.json with the configmap every restart, dropping the persisted RS256 keypair. A fresh serviceradar-cli generate-jwt-keys run produced a new kid, so Kong’s DB-less config still referenced the old key and rejected tokens with 401 No credentials found for given kid.","notes":"Updated k8s demo core init script to preserve any existing auth.jwt_private_key_pem / jwt_key_id when copying the templated core.json, then rerun generate-jwt-keys to backfill only if missing. Applied new Deployment spec (with app.kubernetes.io/part-of label alignment) and rolled the core pod; existing kid key-9b8643c5be64350b survived the restart. Restarted serviceradar-kong so it re-rendered kong.yml from the current JWKS (key now matches) and verified /auth/jwks.json plus kong.yml both advertise the same kid.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T19:36:35.202849445-05:00","updated_at":"2025-11-02T19:24:36.707566856-06:00","closed_at":"2025-10-14T19:36:52.527921534-05:00"}
{"id":"serviceradar-9","title":"Investigate identity publisher revision caching","description":"Another agent flagged that identity publisher caches the revision from the pre-update Get response instead of the Update response. Need to reproduce to confirm whether cache entries wind up stale after KV revisions roll, before applying any fix.","notes":"Fixed the cache to record revisions exclusively from Update responses; if the RPC omits a value we now fall back to zero so the next publish performs a fresh Get. Added an always-on regression test that failed before the change; 'go test ./pkg/registry' now passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-14T21:14:17.003578027-05:00","updated_at":"2025-11-02T19:24:36.815465944-06:00","closed_at":"2025-10-14T21:21:08.797102353-05:00"}
